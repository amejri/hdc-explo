{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# explore_bis_v2\n",
        "\n",
        "Notebook simplifié montrant comment utiliser les blocs **ENC** et **MEM**\n",
        "exposés par la librairie `hdc_project.encoder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4471c8d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using src path: /Users/aymenmejri/Desktop/MyCode/experiments/hdc_v2/hdc_project/src\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "ROOT = Path.cwd().parent\n",
        "SRC = ROOT / \"src\"\n",
        "if str(SRC) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC))\n",
        "print(f'Using src path: {SRC}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5984d662",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from hdc_project.encoder import m4, pipeline as enc_pipeline\n",
        "from hdc_project.encoder.mem import pipeline as mem_pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e1f6e16",
      "metadata": {},
      "source": [
        "\n",
        "## Chargement du sous-corpus OPUS\n",
        "\n",
        "On réutilise `opus_load_subset` depuis la librairie pour récupérer un petit\n",
        "sous-échantillon bilingue (EN/FR). En environnement hors-ligne, un jeu de\n",
        "repli est utilisé pour que le notebook reste exécutable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a58ced34",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPUS subset loaded: 10000 pairs\n",
            "ENC sample size: 10000\n",
            "MEM sample size: 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 10000/10000 [00:26<00:00, 381.24it/s]\n",
            "Processing: 100%|██████████| 10000/10000 [00:26<00:00, 381.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded 10000 sentences; signature shape = (8192,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [07:08<00:00, 23.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "intra=0.0009, inter(abs)=0.0253, inter segments=0.0089\n",
            "majority curve (eta=0): [(1, 0.0), (2, 0.0)]\n",
            "Pairs available for MEM training: 37968\n",
            "Training complete; few bucket counts: [249 229 310 312 265 289 325 291 326 361 249 251 224 314 335 299 287 308\n",
            " 255 302 318 347 296 319 249 297 259 312 271 270 345 281 287 257 305 318\n",
            " 301 303 321 308 326 307 280 261 338 296 279 330 259 224 269 284 252 293\n",
            " 394 297 332 343 256 333 336 246 293 266]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 2307.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-1 mean similarity over 200 span-probes: 0.2727\n",
            "Top-1 median similarity: 0.2773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from hdc_project.encoder import m4, pipeline as enc_pipeline\n",
        "from hdc_project.encoder.mem import pipeline as mem_pipeline\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Chargement données OPUS\n",
        "# ----------------------------\n",
        "try:\n",
        "    ens_raw, frs_raw = enc_pipeline.opus_load_subset(\n",
        "        name=\"opus_books\",\n",
        "        config=\"en-fr\",\n",
        "        split=\"train\",\n",
        "        N=10_000,\n",
        "        seed=2025,\n",
        "    )\n",
        "    print(f\"OPUS subset loaded: {len(ens_raw)} pairs\")\n",
        "except Exception as exc:\n",
        "    print(\"Warning: OPUS download failed, falling back to local toy corpus.\")\n",
        "    print(f\"Original error: {exc}\")\n",
        "    ens_raw = [\n",
        "        \"hyperdimensional computing is fun\",\n",
        "        \"vector symbolic architectures are powerful\",\n",
        "        \"encoding words into hyperspace\",\n",
        "        \"memory augmented networks love clean data\",\n",
        "    ]\n",
        "    frs_raw = [\n",
        "        \"le calcul hyperdimensionnel est amusant\",\n",
        "        \"les architectures symboliques vectorielles sont puissantes\",\n",
        "        \"encoder des mots dans l'hyperspace\",\n",
        "        \"les réseaux augmentés de mémoire aiment les données propres\",\n",
        "    ]\n",
        "\n",
        "enc_sample_size = min(10_000, len(ens_raw))\n",
        "mem_sample_size = min(10_000, len(ens_raw))\n",
        "ens_sample = ens_raw[:enc_sample_size]\n",
        "frs_sample = frs_raw[:enc_sample_size]\n",
        "print(f\"ENC sample size: {enc_sample_size}\")\n",
        "print(f\"MEM sample size: {mem_sample_size}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Encodage ENC (M5–M7)\n",
        "# ----------------------------\n",
        "D = 8192\n",
        "n = 5\n",
        "rng = np.random.default_rng(123)\n",
        "\n",
        "Lex_en = m4.M4_LexEN_new(seed=1, D=D)\n",
        "Lex_fr = m4.M4_LexEN_new(seed=2, D=D)\n",
        "pi = rng.permutation(D).astype(np.int64)\n",
        "\n",
        "encoded_en = enc_pipeline.encode_corpus_ENC(ens_sample, Lex_en, pi, D, n, seg_seed0=999)\n",
        "encoded_fr = enc_pipeline.encode_corpus_ENC(frs_sample, Lex_fr, pi, D, n, seg_seed0=1999)\n",
        "\n",
        "E_list_en = [segment[\"E_seq\"] for segment in encoded_en]\n",
        "H_list_en = [segment[\"H\"] for segment in encoded_en]\n",
        "print(f\"Encoded {len(encoded_en)} sentences; signature shape = {H_list_en[0].shape}\")\n",
        "\n",
        "# Quelques stats ENC\n",
        "s_intra, s_inter = enc_pipeline.intra_inter_ngram_sims(E_list_en, D)\n",
        "inter_seg = enc_pipeline.inter_segment_similarity(H_list_en)\n",
        "maj_curves = enc_pipeline.majority_error_curve(E_list_en, pi, D, eta_list=(0.0, 0.05))\n",
        "print(f\"intra={s_intra:.4f}, inter(abs)={s_inter:.4f}, inter segments={inter_seg:.4f}\")\n",
        "print(\"majority curve (eta=0):\", maj_curves[0.0][:2])\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2) Helpers de \"contenu\" (sans K_s) pour fabriquer les paires\n",
        "#    -> on somme des X_t (déjà alignés par Pi^Δ), puis on seuillle\n",
        "# -------------------------------------------------------------\n",
        "def content_signature_from_Xseq(X_seq, majority: str = \"strict\"):\n",
        "    if not X_seq:\n",
        "        raise ValueError(\"X_seq vide\")\n",
        "    S = np.zeros((X_seq[0].shape[0],), dtype=np.int32)\n",
        "    for x in X_seq:\n",
        "        S += x.astype(np.int32, copy=False)\n",
        "    if majority == \"strict\":\n",
        "        return np.where(S >= 0, 1, -1).astype(np.int8, copy=False)\n",
        "    elif majority == \"unbiased\":\n",
        "        return np.where(S >= 0, 1, -1).astype(np.int8, copy=False)\n",
        "    else:\n",
        "        raise ValueError(\"majority must be 'strict' or 'unbiased'\")\n",
        "\n",
        "def span_signatures_from_trace(X_seq, win: int = 12, stride: int = 6, majority: str = \"unbiased\"):\n",
        "    if not X_seq:\n",
        "        return []\n",
        "    T = len(X_seq)\n",
        "    out = []\n",
        "    if T <= win:\n",
        "        out.append(content_signature_from_Xseq(X_seq, majority))\n",
        "        return out\n",
        "    for start in range(0, T - win + 1, max(1, stride)):\n",
        "        stop = start + win\n",
        "        out.append(content_signature_from_Xseq(X_seq[start:stop], majority))\n",
        "    return out\n",
        "\n",
        "def build_mem_pairs_from_encoded(encoded_en, encoded_fr, win=8, stride=4, majority=\"strict\", max_pairs=None):\n",
        "    pairs = []\n",
        "    N = min(len(encoded_en), len(encoded_fr))\n",
        "    for i in range(N):\n",
        "        X_en = encoded_en[i][\"X_seq\"]\n",
        "        X_fr = encoded_fr[i][\"X_seq\"]\n",
        "        spans_en = span_signatures_from_trace(X_en, win=win, stride=stride, majority=majority)\n",
        "        spans_fr = span_signatures_from_trace(X_fr, win=win, stride=stride, majority=majority)\n",
        "        L = min(len(spans_en), len(spans_fr))\n",
        "        for t in range(L):\n",
        "            pairs.append((\n",
        "                spans_en[t].astype(np.int8, copy=False),\n",
        "                spans_fr[t].astype(np.int8, copy=False),\n",
        "            ))\n",
        "            if max_pairs is not None and len(pairs) >= max_pairs:\n",
        "                return pairs\n",
        "    return pairs\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3) Paires MEM = spans EN/FR (contenu, sans K_s)\n",
        "# -------------------------------------------------------------\n",
        "pairs_mem = build_mem_pairs_from_encoded(encoded_en, encoded_fr, win=8, stride=4, majority=\"strict\")\n",
        "print(f\"Pairs available for MEM training: {len(pairs_mem)}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4) Instanciation MEM et entraînement one-pass\n",
        "#    (k ≈ log2(B) + marge ; ici B=256, k=24 convient)\n",
        "# -------------------------------------------------------------\n",
        "MEM_K = 16\n",
        "MEM_BUCKETS = 128\n",
        "cfg = mem_pipeline.MemConfig(D=D, B=MEM_BUCKETS, k=MEM_K, seed_lsh=10, seed_gmem=11)\n",
        "comp = mem_pipeline.make_mem_pipeline(cfg)\n",
        "mem_pipeline.train_one_pass_MEM(comp, pairs_mem)\n",
        "print(\"Training complete; few bucket counts:\", comp.mem.n[:64])\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5) Probe correcte : on interroge avec Z_en (span) et on compare\n",
        "#    le prototype choisi à Z_fr (span) correspondant\n",
        "# -------------------------------------------------------------\n",
        "probe_count = min(200, len(pairs_mem))\n",
        "sim_values = []\n",
        "for Z_en_vec, Z_fr_vec in tqdm(pairs_mem[:probe_count]):\n",
        "    bucket_idx, score = mem_pipeline.infer_map_top1(comp, Z_en_vec)  # Z_en (span), pas H_en\n",
        "    prototype = comp.mem.H[bucket_idx].astype(np.int32, copy=False)\n",
        "    sim = float(np.dot(prototype, Z_fr_vec.astype(np.int32, copy=False)) / D)\n",
        "    sim_values.append(sim)\n",
        "\n",
        "print(f\"Top-1 mean similarity over {probe_count} span-probes: {np.mean(sim_values):.4f}\")\n",
        "print(f\"Top-1 median similarity: {np.median(sim_values):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3c82ab18",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pop mean/median/min/max/std: 296.625 293.0 216 477 39.670714954485\n",
            "p90/p99: 345 394\n"
          ]
        }
      ],
      "source": [
        "nb = comp.mem.n\n",
        "print(\"pop mean/median/min/max/std:\",\n",
        "      float(nb.mean()), float(np.median(nb)), int(nb.min()), int(nb.max()), float(nb.std()))\n",
        "print(\"p90/p99:\", int(np.quantile(nb, 0.90)), int(np.quantile(nb, 0.99)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45bad5ba",
      "metadata": {},
      "source": [
        "\n",
        "> ℹ️ **Remarque pratique** : si le téléchargement OPUS échoue (exécution hors-ligne),\n",
        "> le notebook bascule automatiquement sur un mini corpus embarqué afin de\n",
        "> conserver une démonstration reproductible des blocs ENC et MEM.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

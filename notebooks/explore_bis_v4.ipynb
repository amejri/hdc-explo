{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore_bis_v3\n",
    "\n",
    "Notebook simplifié montrant comment utiliser les blocs **ENC** et **MEM**\n",
    "exposés par la librairie `hdc_project.encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4471c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using src path: /Users/aymenmejri/Desktop/MyCode/experiments/hdc_v2/hdc_project/src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "SRC = ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "print(f'Using src path: {SRC}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5984d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from hdc_project.encoder import m4, pipeline as enc_pipeline\n",
    "from hdc_project.encoder.mem import pipeline as mem_pipeline\n",
    "from hdc_project.decoder import (\n",
    "    DD1_ctx,\n",
    "    DD2_query,\n",
    "    DD2_query_bin,\n",
    "    DD3_bindToMem,\n",
    "    DD4_search_topK,\n",
    "    DD5_payload,\n",
    "    DD6_vote,\n",
    "    DD7_updateLM,\n",
    "    DecodeOneStep,\n",
    "    DX2_run,\n",
    "    DX3_run,\n",
    "    DX4_run,\n",
    "    DX5_run,\n",
    "    DX6_run,\n",
    "    DX7_run,\n",
    ")\n",
    "from hdc_project.decoder.dec import (\n",
    "    hd_assert_pm1,\n",
    "    hd_bind,\n",
    "    hd_sim,\n",
    "    hd_sim_dot,\n",
    "    build_perm_inverse,\n",
    "    permute_pow,\n",
    "    permute_pow_signed,\n",
    "    rademacher,\n",
    "    ks_2samp_asymp,\n",
    "    hd_perplexity_from_scores,\n",
    "    flip_to_target,\n",
    "    correlated_pm1,\n",
    "    DEFAULT_ELL_GRID,\n",
    "    CONF_PER_STEP,\n",
    "    TRIALS,\n",
    "    T_STEPS,\n",
    "    SIM_Y_MEM,\n",
    "    SIM_CONF_LM,\n",
    "    DEC_D,\n",
    "    RNG_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilitaires HDC désormais importés depuis hdc_project.decoder.dec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f6e16",
   "metadata": {},
   "source": [
    "\n",
    "## Chargement du sous-corpus OPUS\n",
    "\n",
    "On réutilise `opus_load_subset` depuis la librairie pour récupérer un petit\n",
    "sous-échantillon bilingue (EN/FR). En environnement hors-ligne, un jeu de\n",
    "repli est utilisé pour que le notebook reste exécutable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ced34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hdc_project.encoder import m4, pipeline as enc_pipeline\n",
    "from hdc_project.encoder.mem import pipeline as mem_pipeline\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Chargement données OPUS\n",
    "# ----------------------------\n",
    "try:\n",
    "    ens_raw, frs_raw = enc_pipeline.opus_load_subset(\n",
    "        name=\"opus_books\",\n",
    "        config=\"en-fr\",\n",
    "        split=\"train\",\n",
    "        N=10_000,\n",
    "        seed=2025,\n",
    "    )\n",
    "    print(f\"OPUS subset loaded: {len(ens_raw)} pairs\")\n",
    "except Exception as exc:\n",
    "    print(\"Warning: OPUS download failed, falling back to local toy corpus.\")\n",
    "    print(f\"Original error: {exc}\")\n",
    "    ens_raw = [\n",
    "        \"hyperdimensional computing is fun\",\n",
    "        \"vector symbolic architectures are powerful\",\n",
    "        \"encoding words into hyperspace\",\n",
    "        \"memory augmented networks love clean data\",\n",
    "    ]\n",
    "    frs_raw = [\n",
    "        \"le calcul hyperdimensionnel est amusant\",\n",
    "        \"les architectures symboliques vectorielles sont puissantes\",\n",
    "        \"encoder des mots dans l'hyperspace\",\n",
    "        \"les réseaux augmentés de mémoire aiment les données propres\",\n",
    "    ]\n",
    "\n",
    "enc_sample_size = min(10_000, len(ens_raw))\n",
    "mem_sample_size = min(10_000, len(ens_raw))\n",
    "ens_sample = ens_raw[:enc_sample_size]\n",
    "frs_sample = frs_raw[:enc_sample_size]\n",
    "print(f\"ENC sample size: {enc_sample_size}\")\n",
    "print(f\"MEM sample size: {mem_sample_size}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Encodage ENC (M5–M7)\n",
    "# ----------------------------\n",
    "D = 8192\n",
    "n = 5\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "Lex_en = m4.M4_LexEN_new(seed=1, D=D)\n",
    "Lex_fr = m4.M4_LexEN_new(seed=2, D=D)\n",
    "pi = rng.permutation(D).astype(np.int64)\n",
    "\n",
    "encoded_en = enc_pipeline.encode_corpus_ENC(ens_sample, Lex_en, pi, D, n, seg_seed0=999)\n",
    "encoded_fr = enc_pipeline.encode_corpus_ENC(frs_sample, Lex_fr, pi, D, n, seg_seed0=1999)\n",
    "\n",
    "E_list_en = [segment[\"E_seq\"] for segment in encoded_en]\n",
    "H_list_en = [segment[\"H\"] for segment in encoded_en]\n",
    "print(f\"Encoded {len(encoded_en)} sentences; signature shape = {H_list_en[0].shape}\")\n",
    "\n",
    "# Quelques stats ENC\n",
    "s_intra, s_inter = enc_pipeline.intra_inter_ngram_sims(E_list_en, D)\n",
    "inter_seg = enc_pipeline.inter_segment_similarity(H_list_en)\n",
    "maj_curves = enc_pipeline.majority_error_curve(E_list_en, pi, D, eta_list=(0.0, 0.05))\n",
    "print(f\"intra={s_intra:.4f}, inter(abs)={s_inter:.4f}, inter segments={inter_seg:.4f}\")\n",
    "print(\"majority curve (eta=0):\", maj_curves[0.0][:2])\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2) Helpers de \"contenu\" (sans K_s) pour fabriquer les paires\n",
    "#    -> on somme des X_t (déjà alignés par Pi^Δ), puis on seuillle\n",
    "# -------------------------------------------------------------\n",
    "def content_signature_from_Xseq(X_seq, majority: str = \"strict\"):\n",
    "    if not X_seq:\n",
    "        raise ValueError(\"X_seq vide\")\n",
    "    S = np.zeros((X_seq[0].shape[0],), dtype=np.int32)\n",
    "    for x in X_seq:\n",
    "        S += x.astype(np.int32, copy=False)\n",
    "    if majority == \"strict\":\n",
    "        return np.where(S >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    elif majority == \"unbiased\":\n",
    "        return np.where(S >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    else:\n",
    "        raise ValueError(\"majority must be 'strict' or 'unbiased'\")\n",
    "\n",
    "def span_signatures_from_trace(X_seq, win: int = 12, stride: int = 6, majority: str = \"unbiased\"):\n",
    "    if not X_seq:\n",
    "        return []\n",
    "    T = len(X_seq)\n",
    "    out = []\n",
    "    if T <= win:\n",
    "        out.append(content_signature_from_Xseq(X_seq, majority))\n",
    "        return out\n",
    "    for start in range(0, T - win + 1, max(1, stride)):\n",
    "        stop = start + win\n",
    "        out.append(content_signature_from_Xseq(X_seq[start:stop], majority))\n",
    "    return out\n",
    "\n",
    "def build_mem_pairs_from_encoded(encoded_en, encoded_fr, win=8, stride=4, majority=\"strict\", max_pairs=None):\n",
    "    pairs = []\n",
    "    N = min(len(encoded_en), len(encoded_fr))\n",
    "    for i in range(N):\n",
    "        X_en = encoded_en[i][\"X_seq\"]\n",
    "        X_fr = encoded_fr[i][\"X_seq\"]\n",
    "        spans_en = span_signatures_from_trace(X_en, win=win, stride=stride, majority=majority)\n",
    "        spans_fr = span_signatures_from_trace(X_fr, win=win, stride=stride, majority=majority)\n",
    "        L = min(len(spans_en), len(spans_fr))\n",
    "        for t in range(L):\n",
    "            pairs.append((\n",
    "                spans_en[t].astype(np.int8, copy=False),\n",
    "                spans_fr[t].astype(np.int8, copy=False),\n",
    "            ))\n",
    "            if max_pairs is not None and len(pairs) >= max_pairs:\n",
    "                return pairs\n",
    "    return pairs\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3) Paires MEM = spans EN/FR (contenu, sans K_s)\n",
    "# -------------------------------------------------------------\n",
    "pairs_mem = build_mem_pairs_from_encoded(encoded_en, encoded_fr, win=8, stride=4, majority=\"strict\")\n",
    "print(f\"Pairs available for MEM training: {len(pairs_mem)}\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4) Instanciation MEM et entraînement one-pass\n",
    "#    (k ≈ log2(B) + marge ; ici B=256, k=24 convient)\n",
    "# -------------------------------------------------------------\n",
    "MEM_K = 16\n",
    "MEM_BUCKETS = 128\n",
    "cfg = mem_pipeline.MemConfig(D=D, B=MEM_BUCKETS, k=MEM_K, seed_lsh=10, seed_gmem=11)\n",
    "comp = mem_pipeline.make_mem_pipeline(cfg)\n",
    "mem_pipeline.train_one_pass_MEM(comp, pairs_mem)\n",
    "print(\"Training complete; few bucket counts:\", comp.mem.n[:64])\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5) Probe correcte : on interroge avec Z_en (span) et on compare\n",
    "#    le prototype choisi à Z_fr (span) correspondant\n",
    "# -------------------------------------------------------------\n",
    "probe_count = min(200, len(pairs_mem))\n",
    "sim_values = []\n",
    "for Z_en_vec, Z_fr_vec in tqdm(pairs_mem[:probe_count]):\n",
    "    bucket_idx, score = mem_pipeline.infer_map_top1(comp, Z_en_vec)  # Z_en (span), pas H_en\n",
    "    prototype = comp.mem.H[bucket_idx].astype(np.int32, copy=False)\n",
    "    sim = float(np.dot(prototype, Z_fr_vec.astype(np.int32, copy=False)) / D)\n",
    "    sim_values.append(sim)\n",
    "\n",
    "print(f\"Top-1 mean similarity over {probe_count} span-probes: {np.mean(sim_values):.4f}\")\n",
    "print(f\"Top-1 median similarity: {np.median(sim_values):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82ab18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop mean/median/min/max/std: 296.625 293.0 216 477 39.670714954485\n",
      "p90/p99: 345 394\n"
     ]
    }
   ],
   "source": [
    "nb = comp.mem.n\n",
    "print(\"pop mean/median/min/max/std:\",\n",
    "      float(nb.mean()), float(np.median(nb)), int(nb.min()), int(nb.max()), float(nb.std()))\n",
    "print(\"p90/p99:\", int(np.quantile(nb, 0.90)), int(np.quantile(nb, 0.99)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bad5ba",
   "metadata": {},
   "source": [
    "\n",
    "> ℹ️ **Remarque pratique** : si le téléchargement OPUS échoue (exécution hors-ligne),\n",
    "> le notebook bascule automatiquement sur un mini corpus embarqué afin de\n",
    "> conserver une démonstration reproductible des blocs ENC et MEM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a979536",
   "metadata": {},
   "source": [
    "# DEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd5622",
   "metadata": {},
   "source": [
    "## DEC-0 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52158548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger(\"DEC\")\n",
    "if not log.handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22094d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pm1(D: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"Alias vers rademacher pour générer des vecteurs +/-1 en int8.\"\"\"\n",
    "    return rademacher(D, rng)\n",
    "\n",
    "# ---------------------------\n",
    "# DX0: tests\n",
    "# ---------------------------\n",
    "def dx0_sanity(D: int = 16_384, N_sim: int = 1_000, seed: int = 2024, tol: float = 5e-3) -> None:\n",
    "    \"\"\"\n",
    "    Vérifie:\n",
    "      1) hd_sim(x,x)=1 et hd_sim(x,-x)=-1 (à tol près)\n",
    "      2) Invariance de similarité par binding: sim(x,y)=sim(x⊗k, y⊗k)\n",
    "      3) Préservation de la norme (||x||_2/√D = 1) avant/après binding\n",
    "    Critère d'acceptation (CA): écarts absolus ≤ 5e-3.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    max_err_self = 0.0\n",
    "    max_err_neg = 0.0\n",
    "    max_err_bind = 0.0\n",
    "    max_err_norm = 0.0\n",
    "\n",
    "    for _ in range(N_sim):\n",
    "        x = pm1(D, rng)\n",
    "        y = pm1(D, rng)\n",
    "        k = pm1(D, rng)\n",
    "        hd_assert_pm1(x, D)\n",
    "        hd_assert_pm1(y, D)\n",
    "        hd_assert_pm1(k, D)\n",
    "\n",
    "        # (1) Identités de similarité\n",
    "        s_xx = hd_sim(x, x)\n",
    "        s_xnx = hd_sim(x, (-x).astype(np.int8, copy=False))\n",
    "\n",
    "        max_err_self = max(max_err_self, abs(s_xx - 1.0))\n",
    "        max_err_neg = max(max_err_neg, abs(s_xnx + 1.0))\n",
    "\n",
    "        # (2) Invariance par binding (DEC1)\n",
    "        s_xy = hd_sim(x, y)\n",
    "        xk = hd_bind(x, k)\n",
    "        yk = hd_bind(y, k)\n",
    "        s_xy_bind = hd_sim(xk, yk)\n",
    "        max_err_bind = max(max_err_bind, abs(s_xy - s_xy_bind))\n",
    "\n",
    "        # (3) Normes (avant/après binding)\n",
    "        norm_x = np.linalg.norm(x.astype(np.float64)) / np.sqrt(D)\n",
    "        norm_xk = np.linalg.norm(xk.astype(np.float64)) / np.sqrt(D)\n",
    "        max_err_norm = max(max_err_norm, abs(norm_x - 1.0), abs(norm_xk - 1.0))\n",
    "\n",
    "    # Rapport\n",
    "    print(\"DX0 — Sanity checks (double précision)\")\n",
    "    print(f\"  D={D}, N={N_sim}, tol={tol:.1e}\")\n",
    "    print(f\"  max|sim(x,x)-1|         = {max_err_self:.3e}\")\n",
    "    print(f\"  max|sim(x,-x)+1|        = {max_err_neg:.3e}\")\n",
    "    print(f\"  max|sim(x,y)-sim(x⊗k,y⊗k)| = {max_err_bind:.3e}\")\n",
    "    print(f\"  max| ||x||/√D - 1 | (incl. bind) = {max_err_norm:.3e}\")\n",
    "\n",
    "    # Assertions CA\n",
    "    assert max_err_self <= tol, \"CA non satisfait: sim(x,x) s'écarte de 1\"\n",
    "    assert max_err_neg <= tol, \"CA non satisfait: sim(x,-x) s'écarte de -1\"\n",
    "    assert max_err_bind <= tol, \"CA non satisfait: invariance de similarité après binding\"\n",
    "    assert max_err_norm <= tol, \"CA non satisfait: norme non préservée (relative)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c12ada1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX0 — Sanity checks (double précision)\n",
      "  D=16384, N=1000, tol=5.0e-03\n",
      "  max|sim(x,x)-1|         = 0.000e+00\n",
      "  max|sim(x,-x)+1|        = 0.000e+00\n",
      "  max|sim(x,y)-sim(x⊗k,y⊗k)| = 0.000e+00\n",
      "  max| ||x||/√D - 1 | (incl. bind) = 0.000e+00\n"
     ]
    }
   ],
   "source": [
    "dx0_sanity()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4826d",
   "metadata": {},
   "source": [
    "## DD1 .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ebc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD1_ctx est importé depuis hdc_project.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cc32465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX1 — DD1_ctx (isométrie & contrats)\n",
      "  D=16384, m=64, trials=200, tol=5.0e-03\n",
      "  max|sim_before - sim_after|  = 0.000e+00\n",
      "  max| ||H||/√D - 1 | (incl. bind) = 0.000e+00\n",
      "  max|Gram_before - Gram_after| = 0.000e+00\n"
     ]
    }
   ],
   "source": [
    "# --- DX1: tests détaillés ---\n",
    "def dx1_test_DD1_ctx(D: int = 16_384, m: int = 64, trials: int = 200, seed: int = 1234, tol: float = 5e-3):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # 1) Similarité inchangée et normes préservées (sur 'trials' paires)\n",
    "    max_err_sim = 0.0\n",
    "    max_err_norm = 0.0\n",
    "    for _ in range(trials):\n",
    "        H1, H2, G = pm1(D, rng), pm1(D, rng), pm1(D, rng)\n",
    "        # Copies pour vérifier non-mutation\n",
    "        H1_copy, H2_copy, G_copy = H1.copy(), H2.copy(), G.copy()\n",
    "\n",
    "        Q1, Q2 = DD1_ctx(H1, G), DD1_ctx(H2, G)\n",
    "        # Similarité\n",
    "        s0 = hd_sim(H1, H2)\n",
    "        s1 = hd_sim(Q1, Q2)\n",
    "        max_err_sim = max(max_err_sim, abs(s0 - s1))\n",
    "\n",
    "        # Normes relatives\n",
    "        nH1  = np.linalg.norm(H1.astype(np.float64)) / np.sqrt(D)\n",
    "        nQ1  = np.linalg.norm(Q1.astype(np.float64)) / np.sqrt(D)\n",
    "        nH2  = np.linalg.norm(H2.astype(np.float64)) / np.sqrt(D)\n",
    "        nQ2  = np.linalg.norm(Q2.astype(np.float64)) / np.sqrt(D)\n",
    "        max_err_norm = max(max_err_norm, abs(nH1 - 1.0), abs(nQ1 - 1.0),\n",
    "                                           abs(nH2 - 1.0), abs(nQ2 - 1.0))\n",
    "\n",
    "        # Contrats: dtype & non-mutation\n",
    "        assert Q1.dtype == np.int8 and Q2.dtype == np.int8\n",
    "        assert np.all(H1 == H1_copy) and np.all(H2 == H2_copy) and np.all(G == G_copy), \"mutation détectée\"\n",
    "        assert np.all((Q1 == 1) | (Q1 == -1)) and np.all((Q2 == 1) | (Q2 == -1)), \"sortie hors ±1\"\n",
    "\n",
    "    # 2) Isométrie de Gram (m vecteurs)\n",
    "    H = np.stack([pm1(D, rng) for _ in range(m)], axis=0)  # (m, D) ±1/int8\n",
    "    G = pm1(D, rng)\n",
    "    Q = np.stack([DD1_ctx(H[i], G) for i in range(m)], axis=0)\n",
    "\n",
    "    # Gram avant/après, en double précision\n",
    "    G0 = (H.astype(np.int32) @ H.astype(np.int32).T) / D\n",
    "    G1 = (Q.astype(np.int32) @ Q.astype(np.int32).T) / D\n",
    "    max_err_gram = float(np.max(np.abs(G0.astype(np.float64) - G1.astype(np.float64))))\n",
    "\n",
    "    # --- Rapport ---\n",
    "    print(\"DX1 — DD1_ctx (isométrie & contrats)\")\n",
    "    print(f\"  D={D}, m={m}, trials={trials}, tol={tol:.1e}\")\n",
    "    print(f\"  max|sim_before - sim_after|  = {max_err_sim:.3e}\")\n",
    "    print(f\"  max| ||H||/√D - 1 | (incl. bind) = {max_err_norm:.3e}\")\n",
    "    print(f\"  max|Gram_before - Gram_after| = {max_err_gram:.3e}\")\n",
    "\n",
    "    # --- Critères d'acceptation ---\n",
    "    assert max_err_sim  <= tol, \"Invariance de similarité violée (DEC1)\"\n",
    "    assert max_err_norm <= tol, \"Norme non préservée (relative)\"\n",
    "    assert max_err_gram <= tol, \"Isométrie de Gram violée (DEC1)\"\n",
    "\n",
    "dx1_test_DD1_ctx()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe1921",
   "metadata": {},
   "source": [
    "# DD2 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f4e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "log = logging.getLogger(\"DEC.DX2.v2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a257745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD2_query / DD2_query_bin et la campagne DX2 proviennent de hdc_project.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e4acc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 23:22:58,772 [INFO] DX2 — Norme(R_t)/sqrt(D) par (ell, alpha/beta): min | median | max\n",
      "2025-10-06 23:22:58,772 [INFO]   ell=2, alpha/beta=0.333  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,772 [INFO]   ell=2, alpha/beta=1  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,772 [INFO]   ell=2, alpha/beta=3  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,772 [INFO]   ell=4, alpha/beta=0.333  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,773 [INFO]   ell=4, alpha/beta=1  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,773 [INFO]   ell=4, alpha/beta=3  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,773 [INFO]   ell=8, alpha/beta=0.333  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,773 [INFO]   ell=8, alpha/beta=1  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,773 [INFO]   ell=8, alpha/beta=3  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,773 [INFO] DX2 — CA validés: (i) norme ∈ [0.9,1.1] ; (ii) Gram invariant ; (iii) identité paire-à-paire OK.\n"
     ]
    }
   ],
   "source": [
    "DX2_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d999ef",
   "metadata": {},
   "source": [
    "# DD3 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "871f3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD3_bindToMem et hd_sim_dot sont fournis par hdc_project.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f40787be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_2samp_asymp et DX3_run sont désormais accessibles via hdc_project.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c21c01a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 22:57:03,498 [INFO] DX3 — Invariance (dé)binding mémoire\n",
      "2025-10-06 22:57:03,498 [INFO]   D=16384, C=500, T=200\n",
      "2025-10-06 22:57:03,498 [INFO]   Erreur relative moyenne  = 0.000000\n",
      "2025-10-06 22:57:03,498 [INFO]   KS: D=0.000000, p=1.000\n"
     ]
    }
   ],
   "source": [
    "DX3_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb3d23c",
   "metadata": {},
   "source": [
    "# DD4 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c549139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD4_search_topK est importé depuis hdc_project.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48850b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DX4_run est importé depuis hdc_project.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e3dea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:23<00:00,  1.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{100: 1.0, 500: 1.0, 2000: 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DX4_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b943256",
   "metadata": {},
   "source": [
    "# DD5 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bff951c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD5_payload est importé depuis hdc_project.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e44670a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DX5_run est importé depuis hdc_project.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdb3df09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 1.0, 8: 1.0, 16: 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DX5_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa074d57",
   "metadata": {},
   "source": [
    "# DD6 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7ff4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def DD6_vote(\n",
    "#     Z_hat: np.ndarray,\n",
    "#     H_LM: np.ndarray,\n",
    "#     L_fr,\n",
    "#     cand_vocab: list[str],\n",
    "#     lam: float = 0.0\n",
    "# ) -> tuple[str, np.ndarray]:\n",
    "#     \"\"\"\n",
    "#     Renvoie (token*, scores) sur cand_vocab.\n",
    "#     \"\"\"\n",
    "#     D = Z_hat.shape[0]\n",
    "#     hd_assert_pm1(Z_hat, D); hd_assert_pm1(H_LM, D)\n",
    "#     scores = []\n",
    "#     for v in cand_vocab:\n",
    "#         Lv = L_fr(v).astype(np.int8, copy=False)\n",
    "#         hd_assert_pm1(Lv, D)\n",
    "#         s = (Z_hat.astype(np.int32) @ Lv.astype(np.int32)) \\\n",
    "#             + lam * (H_LM.astype(np.int32) @ Lv.astype(np.int32))\n",
    "#         scores.append(float(s))\n",
    "#     scores = np.asarray(scores, dtype=np.float32)\n",
    "#     best = int(np.argmax(scores))\n",
    "#     return cand_vocab[best], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e56642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyLexFR:\n",
    "    def __init__(self, vocab: list[str], D: int, seed: int = 1234):\n",
    "        self.vocab = vocab\n",
    "        self.D = D\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        # table de vecteurs ±1/int8\n",
    "        self.table = {v: self.rng.choice(np.array([-1, 1], dtype=np.int8), size=D) for v in vocab}\n",
    "\n",
    "    def __call__(self, v: str) -> np.ndarray:\n",
    "        return self.table[v]\n",
    "\n",
    "# # -- Génération contrôlée de corrélations (flip par coordonnée) ----------------\n",
    "# def flip_to_target(vec: np.ndarray, target_sim: float, rng: np.random.Generator) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Retourne une copie de 'vec' dont la similarité attendue vaut 'target_sim'.\n",
    "#     Pour ±1, si p_flip = (1 - target_sim)/2, alors E[sim] = 1 - 2*p_flip = target_sim.\n",
    "#     \"\"\"\n",
    "#     D = vec.shape[0]\n",
    "#     p_flip = max(0.0, min(1.0, (1.0 - float(target_sim)) / 2.0))\n",
    "#     mask = (rng.random(D) < p_flip).astype(np.int8)          # 1 si on flippe\n",
    "#     flips = (1 - 2 * mask).astype(np.int8, copy=False)       # 1 -> -1, 0 -> +1\n",
    "#     out = (vec.astype(np.int8, copy=False) * flips).astype(np.int8, copy=False)\n",
    "#     return out\n",
    "\n",
    "# # -- Module testé (fourni) ------------------------------------------------------\n",
    "\n",
    "# def _batch_lex(cand_vocab, L):\n",
    "#     \"\"\"\n",
    "#     Applique le callable 'L' (v -> ±1/int8 de forme (D,)) sur tout le vocabulaire candidat\n",
    "#     et empile en une matrice (V, D) en int8.\n",
    "#     \"\"\"\n",
    "#     mats = []\n",
    "#     for v in cand_vocab:\n",
    "#         vec = L(v).astype(np.int8, copy=False)\n",
    "#         mats.append(vec)\n",
    "#     M = np.vstack(mats).astype(np.int8, copy=False)\n",
    "#     return M\n",
    "\n",
    "# def DD6_vote(\n",
    "#     Z_hat: np.ndarray,\n",
    "#     H_LM: np.ndarray,\n",
    "#     L_mem,                  # callable: v -> ±1 int8 (D,)\n",
    "#     L_lm,                   # callable: v -> ±1 int8 (D,)\n",
    "#     cand_vocab: list[str],\n",
    "#     lam: float = 0.0,\n",
    "#     *,\n",
    "#     normalize: str = \"sqrtD\",   # {\"none\",\"sqrtD\"} ; \"sqrtD\" conseillé pour perplexité\n",
    "#     return_probs: bool = False, # si True, renvoie aussi les probabilités softmax\n",
    "#     tau: float = 1.0            # température du softmax (si return_probs=True)\n",
    "# ) -> tuple[str, np.ndarray, np.ndarray | None]:\n",
    "#     \"\"\"\n",
    "#     s(v) = <Z_hat, L_mem(v)> + lam * <H_LM, L_lm(v)>\n",
    "#     Retourne: (token*, scores_raw, probs|None)\n",
    "#       - scores_raw: np.float64 de taille V (non normalisés, utiles pour debug/traçage)\n",
    "#       - probs:      np.float64 de taille V si return_probs=True (softmax stable)\n",
    "#     Contrats:\n",
    "#       Z_hat, H_LM: ±1/int8, de longueur D identique.\n",
    "#       L_mem, L_lm: renvoient ±1/int8 (D,) pour tout v de cand_vocab.\n",
    "#     \"\"\"\n",
    "#     # --- Contrats de forme et de type\n",
    "#     D = int(Z_hat.shape[0])\n",
    "#     hd_assert_pm1(Z_hat, D)\n",
    "#     hd_assert_pm1(H_LM, D)\n",
    "#     assert isinstance(cand_vocab, (list, tuple)) and len(cand_vocab) > 0, \"cand_vocab vide\"\n",
    "\n",
    "#     # --- Matrices lexicales (V, D) en int8 (vectorisation)\n",
    "#     M_mem = _batch_lex(cand_vocab, L_mem)   # (V, D)\n",
    "#     M_lm  = _batch_lex(cand_vocab, L_lm)    # (V, D)\n",
    "#     assert M_mem.shape == M_lm.shape == (len(cand_vocab), D), \"Shapes (V,D) incohérents\"\n",
    "\n",
    "#     # --- Produits scalaires vectorisés (int32 pour éviter overflow)\n",
    "#     z32  = Z_hat.astype(np.int32, copy=False)\n",
    "#     h32  = H_LM.astype(np.int32, copy=False)\n",
    "#     mem_scores = (M_mem.astype(np.int32, copy=False) @ z32)              # (V,)\n",
    "#     lm_scores  = (M_lm.astype(np.int32,  copy=False) @ h32)              # (V,)\n",
    "#     scores_raw = mem_scores.astype(np.float64) + float(lam) * lm_scores.astype(np.float64)\n",
    "\n",
    "#     # --- Argmax sur scores bruts (l'échelle n'affecte pas l'argmax)\n",
    "#     best_idx   = int(np.argmax(scores_raw))\n",
    "#     token_star = cand_vocab[best_idx]\n",
    "\n",
    "#     # --- Option: probabilités (softmax stable) avec normalisation choisie\n",
    "#     probs = None\n",
    "#     if return_probs:\n",
    "#         if normalize == \"sqrtD\":\n",
    "#             logits = scores_raw / (np.sqrt(D) * max(1e-6, float(tau)))\n",
    "#         elif normalize == \"none\":\n",
    "#             logits = scores_raw / max(1e-6, float(tau))\n",
    "#         else:\n",
    "#             raise ValueError(\"normalize ∈ {'none','sqrtD'} attendu\")\n",
    "#         logits = logits - np.max(logits)                  # stabilité num.\n",
    "#         exps   = np.exp(logits, dtype=np.float64)\n",
    "#         probs  = exps / np.sum(exps, dtype=np.float64)    # (V,)\n",
    "#         probs  = probs.astype(np.float64, copy=False)\n",
    "\n",
    "#     return token_star, scores_raw, probs\n",
    "\n",
    "# # -- Perplexité HD: softmax sur scores normalisés par D -------------------------\n",
    "# def hd_perplexity(scores: np.ndarray, true_idx: int, D: int, tau: float = 1.0) -> float:\n",
    "#     \"\"\"\n",
    "#     Perplexité = exp( - log p(true) ), avec p ∝ exp( (scores/D)/tau ).\n",
    "#     On divise par D pour éviter des logits trop grands (HD).\n",
    "#     \"\"\"\n",
    "#     logits = scores / (D * max(1e-6, tau))\n",
    "#     logits = logits - np.max(logits)               # stabilité\n",
    "#     exps = np.exp(logits)\n",
    "#     p = exps / np.sum(exps)\n",
    "#     p_true = float(max(p[true_idx], 1e-12))\n",
    "#     return float(np.exp(-np.log(p_true)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "514630c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _softmax_probs(scores: np.ndarray, D: int, tau: float = 1.0) -> np.ndarray:\n",
    "#     # Normalisation par sqrt(D) pour éviter la sur-concentration à grande dimension\n",
    "#     s = scores / (np.sqrt(D) * tau)\n",
    "#     s = s - np.max(s)                       # stabilité numérique\n",
    "#     exps = np.exp(s)\n",
    "#     return exps / np.sum(exps)\n",
    "\n",
    "# def hd_perplexity(scores: np.ndarray, true_index: int, D: int, tau: float = 1.0) -> float:\n",
    "#     p = _softmax_probs(scores, D=D, tau=tau)[true_index]\n",
    "#     # Perplexité = exp(-log p_y) ; bornée inférieurement par 1\n",
    "#     return float(np.exp(-np.log(max(p, 1e-12))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66f1b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def DX6_run_two_spaces(\n",
    "#     D: int = 16384, trials: int = 400,\n",
    "#     lam_grid=(0.0, 0.5, 1.0),\n",
    "#     # corrélations du vrai token:\n",
    "#     sim_payload: float = 0.82,   # corr(Z_hat, L_mem(y))\n",
    "#     sim_lm: float      = 0.65,   # corr(H_LM, L_lm(y))\n",
    "#     # confondeurs:\n",
    "#     n_confounders: int = 6,\n",
    "#     rho_mem_conf: float = 0.72,  # corr(Z_hat, L_mem(conf))\n",
    "#     rho_lm_conf: float  = 0.05,  # corr(H_LM, L_lm(conf))\n",
    "#     tau: float = 1.0,\n",
    "#     rng_seed: int = 7031\n",
    "# ):\n",
    "#     g = np.random.default_rng(rng_seed)\n",
    "\n",
    "#     def rademacher(D):  # ±1/int8\n",
    "#         return g.choice(np.array([-1,1], dtype=np.int8), size=D)\n",
    "\n",
    "#     def correlated_pm1(proto: np.ndarray, rho: float) -> np.ndarray:\n",
    "#         noise = rademacher(proto.shape[0])\n",
    "#         mix = rho * proto.astype(np.int32) + (1-rho) * noise.astype(np.int32)\n",
    "#         return np.where(mix >= 0, 1, -1).astype(np.int8)\n",
    "\n",
    "#     def make_trial():\n",
    "#         Z_true  = rademacher(D)  # payload cible\n",
    "#         H_true  = rademacher(D)  # LM cible\n",
    "#         # Construire DEUX lexiques: L_mem (pour la mémoire) et L_lm (pour le LM)\n",
    "#         V = n_confounders + 1\n",
    "#         L_mem = np.empty((V, D), dtype=np.int8)\n",
    "#         L_lm  = np.empty((V, D), dtype=np.int8)\n",
    "#         # y (indice 0)\n",
    "#         L_mem[0] = correlated_pm1(Z_true, sim_payload)\n",
    "#         L_lm[0]  = correlated_pm1(H_true, sim_lm)\n",
    "#         # confondeurs\n",
    "#         for i in range(1, V):\n",
    "#             L_mem[i] = correlated_pm1(Z_true, rho_mem_conf)\n",
    "#             L_lm[i]  = correlated_pm1(H_true, rho_lm_conf)\n",
    "#         return L_mem, L_lm, 0, Z_true, H_true  # (lexiques, true_id, payload, LM)\n",
    "\n",
    "#     def vote_scores_two_lex(L_mem: np.ndarray, L_lm: np.ndarray,\n",
    "#                             Z_hat: np.ndarray, H_LM: np.ndarray, lam: float) -> np.ndarray:\n",
    "#         # int32 pour éviter overflow ; (V,D) @ (D,) -> (V,)\n",
    "#         return (L_mem.astype(np.int32) @ Z_hat.astype(np.int32)) + \\\n",
    "#                lam * (L_lm.astype(np.int32)  @ H_LM.astype(np.int32))\n",
    "\n",
    "#     def _softmax_probs(scores: np.ndarray, D: int, tau: float = 1.0) -> np.ndarray:\n",
    "#         s = scores / (np.sqrt(D) * max(tau, 1e-6))\n",
    "#         s = s - np.max(s)\n",
    "#         exps = np.exp(s)\n",
    "#         return exps / np.sum(exps)\n",
    "\n",
    "#     def hd_perplexity(scores: np.ndarray, true_idx: int, D: int, tau: float = 1.0) -> float:\n",
    "#         p_true = float(_softmax_probs(scores, D=D, tau=tau)[true_idx])\n",
    "#         return float(np.exp(-np.log(max(p_true, 1e-12))))\n",
    "\n",
    "#     stats = {lam: {\"top1_hits\": 0, \"ppl_sum\": 0.0} for lam in lam_grid}\n",
    "\n",
    "#     for _ in range(trials):\n",
    "#         L_mem, L_lm, true_idx, Z_true, H_true = make_trial()\n",
    "#         Z_hat = Z_true; H_LM = H_true\n",
    "#         for lam in lam_grid:\n",
    "#             scores = vote_scores_two_lex(L_mem, L_lm, Z_hat, H_LM, float(lam))\n",
    "#             pred = int(np.argmax(scores))\n",
    "#             stats[lam][\"top1_hits\"] += 1 if pred == true_idx else 0\n",
    "#             stats[lam][\"ppl_sum\"]   += hd_perplexity(scores, true_idx, D, tau)\n",
    "\n",
    "#     results = {lam: {\"top1\": stats[lam][\"top1_hits\"]/trials,\n",
    "#                      \"ppl\":  stats[lam][\"ppl_sum\"]/trials}\n",
    "#                for lam in lam_grid}\n",
    "\n",
    "#     base_top1, base_ppl = results[0.0][\"top1\"], results[0.0][\"ppl\"]\n",
    "#     saturated = (abs(base_top1 - 1.0) < 1e-12)\n",
    "\n",
    "#     log.info((\"DX6(2-spaces|fixed) — D=%d, trials=%d, conf=%d, ρ_mem(conf)=%.2f, ρ_lm(conf)=%.2f, \"\n",
    "#               \"sim_payload=%.2f, sim_lm=%.2f\"),\n",
    "#               D, trials, n_confounders, rho_mem_conf, rho_lm_conf, sim_payload, sim_lm)\n",
    "#     for lam in lam_grid:\n",
    "#         log.info(\"  lambda=%.2f  ->  top-1=%.3f | ppl=%.3f\", lam, results[lam][\"top1\"], results[lam][\"ppl\"])\n",
    "\n",
    "#     if saturated:\n",
    "#         ok = any(results[lam][\"ppl\"] < base_ppl - 1e-12 for lam in lam_grid if lam != 0.0)\n",
    "#         assert ok, \"DX6(fixed): régime saturé — aucune baisse de perplexité vs λ=0.\"\n",
    "#     else:\n",
    "#         ok = any((results[lam][\"top1\"] > base_top1 + 1e-12) and (results[lam][\"ppl\"] < base_ppl - 1e-12)\n",
    "#                  for lam in lam_grid if lam != 0.0)\n",
    "#         assert ok, \"DX6(fixed): aucun λ n'améliore simultanément top-1 ET perplexité vs λ=0.\"\n",
    "\n",
    "#     log.info(\"DX6(fixed) — CA VALIDÉ (%s).\", \"saturé\" if saturated else \"non-saturé\")\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02b7d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD6_vote, flip_to_target et DX6_run sont importés depuis hdc_project.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40afdb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 23:00:52,893 [INFO] DX6 — D=16384, trials=400, conf=6, ρ_mem(conf)=0.55, ρ_lm(conf)=0.10, sim_payload=0.60, sim_lm=0.40\n",
      "2025-10-06 23:00:52,893 [INFO]   lambda=0.00  ->  top-1=1.000 | ppl=1.019\n",
      "2025-10-06 23:00:52,894 [INFO]   lambda=0.50  ->  top-1=1.000 | ppl=1.000\n",
      "2025-10-06 23:00:52,894 [INFO]   lambda=1.00  ->  top-1=1.000 | ppl=1.000\n",
      "2025-10-06 23:00:52,894 [INFO] DX6 — CA VALIDÉ (saturé).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: {'top1': 1.0, 'ppl': 1.0192223749961116},\n",
       " 0.5: {'top1': 1.0, 'ppl': 1.0000000001159797},\n",
       " 1.0: {'top1': 1.0, 'ppl': 1.0}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DX6_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a822a58a",
   "metadata": {},
   "source": [
    "# DD7 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "918aab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DX7_run et les constantes associées résident dans hdc_project.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a20a3fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 23:23:12,336 [INFO] DX7 — étude fenetre ell=(2, 4, 8, 12) (D=16384, trials=200, T=24, conf/step=8)\n",
      "2025-10-06 23:23:19,330 [INFO]   ell= 2  ->  top-1=0.926 | p(ell)=1.000\n",
      "2025-10-06 23:23:26,541 [INFO]   ell= 4  ->  top-1=0.854 | p(ell)=1.000\n",
      "2025-10-06 23:23:34,829 [INFO]   ell= 8  ->  top-1=0.707 | p(ell)=1.000\n",
      "2025-10-06 23:23:43,466 [INFO]   ell=12  ->  top-1=0.555 | p(ell)=1.000\n",
      "2025-10-06 23:23:43,467 [INFO] DX7 — CA VALIDÉS: (i) ell*=2 maximise top-1 ; (ii) p(ell) décroît au-delà.\n"
     ]
    }
   ],
   "source": [
    "results, ell_star = DX7_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70cf6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecodeOneStep est importé depuis hdc_project.decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0a8cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_L_fr(vocab_seed: int, D: int):\n",
    "    rng = np.random.default_rng(vocab_seed)\n",
    "    table = {}\n",
    "\n",
    "    def get(tok: str) -> np.ndarray:\n",
    "        if tok not in table:\n",
    "            x = rng.integers(0, 2, size=D, dtype=np.int8)\n",
    "            table[tok] = (2 * x - 1).astype(np.int8)\n",
    "        return table[tok]\n",
    "\n",
    "    return get\n",
    "\n",
    "def test_isometry_and_flow():\n",
    "    D = 16384\n",
    "    K = 128\n",
    "    rng = np.random.default_rng(7)\n",
    "    Hs = (2 * rng.integers(0, 2, size=D, dtype=np.int8) - 1)\n",
    "    H_LM = (2 * rng.integers(0, 2, size=D, dtype=np.int8) - 1)\n",
    "    G_DEC = (2 * rng.integers(0, 2, size=D, dtype=np.int8) - 1)\n",
    "    G_MEM = (2 * rng.integers(0, 2, size=D, dtype=np.int8) - 1)\n",
    "    Pi = rng.permutation(D).astype(np.int64)\n",
    "    Lfr = mock_L_fr(1234, D)\n",
    "    B = 2048\n",
    "    prototypes = (2 * rng.integers(0, 2, size=(B, D), dtype=np.int8) - 1)\n",
    "\n",
    "    tok, scores, c_star, CK, H_LM_next = DecodeOneStep(\n",
    "        Hs,\n",
    "        H_LM,\n",
    "        history_fr=[\"de\", \"la\", \"musique\"],\n",
    "        G_DEC=G_DEC,\n",
    "        G_MEM=G_MEM,\n",
    "        Pi=Pi,\n",
    "        L_fr=Lfr,\n",
    "        prototypes=prototypes,\n",
    "        K=K,\n",
    "        return_ck_scores=False\n",
    "    )\n",
    "\n",
    "    assert isinstance(tok, str) and scores.ndim == 1\n",
    "    assert H_LM_next.shape == (D,) and H_LM_next.dtype == np.int8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db5fa07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_isometry_and_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587fb688",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b983cd28",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
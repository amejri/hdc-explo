{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# explore_bis_v3\n",
        "\n",
        "Notebook simplifié montrant comment utiliser les blocs **ENC** et **MEM**\n",
        "exposés par la librairie `hdc_project.encoder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4471c8d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using src path: /Users/aymenmejri/Desktop/MyCode/experiments/hdc_v2/hdc_project/src\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "ROOT = Path.cwd().parent\n",
        "SRC = ROOT / \"src\"\n",
        "if str(SRC) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC))\n",
        "print(f'Using src path: {SRC}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5984d662",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from hdc_project.encoder import m4, pipeline as enc_pipeline\n",
        "from hdc_project.encoder.mem import pipeline as mem_pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e1f6e16",
      "metadata": {},
      "source": [
        "\n",
        "## Chargement du sous-corpus OPUS\n",
        "\n",
        "On réutilise `opus_load_subset` depuis la librairie pour récupérer un petit\n",
        "sous-échantillon bilingue (EN/FR). En environnement hors-ligne, un jeu de\n",
        "repli est utilisé pour que le notebook reste exécutable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a58ced34",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from hdc_project.encoder import m4, pipeline as enc_pipeline\n",
        "from hdc_project.encoder.mem import pipeline as mem_pipeline\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Chargement données OPUS\n",
        "# ----------------------------\n",
        "try:\n",
        "    ens_raw, frs_raw = enc_pipeline.opus_load_subset(\n",
        "        name=\"opus_books\",\n",
        "        config=\"en-fr\",\n",
        "        split=\"train\",\n",
        "        N=10_000,\n",
        "        seed=2025,\n",
        "    )\n",
        "    print(f\"OPUS subset loaded: {len(ens_raw)} pairs\")\n",
        "except Exception as exc:\n",
        "    print(\"Warning: OPUS download failed, falling back to local toy corpus.\")\n",
        "    print(f\"Original error: {exc}\")\n",
        "    ens_raw = [\n",
        "        \"hyperdimensional computing is fun\",\n",
        "        \"vector symbolic architectures are powerful\",\n",
        "        \"encoding words into hyperspace\",\n",
        "        \"memory augmented networks love clean data\",\n",
        "    ]\n",
        "    frs_raw = [\n",
        "        \"le calcul hyperdimensionnel est amusant\",\n",
        "        \"les architectures symboliques vectorielles sont puissantes\",\n",
        "        \"encoder des mots dans l'hyperspace\",\n",
        "        \"les réseaux augmentés de mémoire aiment les données propres\",\n",
        "    ]\n",
        "\n",
        "enc_sample_size = min(10_000, len(ens_raw))\n",
        "mem_sample_size = min(10_000, len(ens_raw))\n",
        "ens_sample = ens_raw[:enc_sample_size]\n",
        "frs_sample = frs_raw[:enc_sample_size]\n",
        "print(f\"ENC sample size: {enc_sample_size}\")\n",
        "print(f\"MEM sample size: {mem_sample_size}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Encodage ENC (M5–M7)\n",
        "# ----------------------------\n",
        "D = 8192\n",
        "n = 5\n",
        "rng = np.random.default_rng(123)\n",
        "\n",
        "Lex_en = m4.M4_LexEN_new(seed=1, D=D)\n",
        "Lex_fr = m4.M4_LexEN_new(seed=2, D=D)\n",
        "pi = rng.permutation(D).astype(np.int64)\n",
        "\n",
        "encoded_en = enc_pipeline.encode_corpus_ENC(ens_sample, Lex_en, pi, D, n, seg_seed0=999)\n",
        "encoded_fr = enc_pipeline.encode_corpus_ENC(frs_sample, Lex_fr, pi, D, n, seg_seed0=1999)\n",
        "\n",
        "E_list_en = [segment[\"E_seq\"] for segment in encoded_en]\n",
        "H_list_en = [segment[\"H\"] for segment in encoded_en]\n",
        "print(f\"Encoded {len(encoded_en)} sentences; signature shape = {H_list_en[0].shape}\")\n",
        "\n",
        "# Quelques stats ENC\n",
        "s_intra, s_inter = enc_pipeline.intra_inter_ngram_sims(E_list_en, D)\n",
        "inter_seg = enc_pipeline.inter_segment_similarity(H_list_en)\n",
        "maj_curves = enc_pipeline.majority_error_curve(E_list_en, pi, D, eta_list=(0.0, 0.05))\n",
        "print(f\"intra={s_intra:.4f}, inter(abs)={s_inter:.4f}, inter segments={inter_seg:.4f}\")\n",
        "print(\"majority curve (eta=0):\", maj_curves[0.0][:2])\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2) Helpers de \"contenu\" (sans K_s) pour fabriquer les paires\n",
        "#    -> on somme des X_t (déjà alignés par Pi^Δ), puis on seuillle\n",
        "# -------------------------------------------------------------\n",
        "def content_signature_from_Xseq(X_seq, majority: str = \"strict\"):\n",
        "    if not X_seq:\n",
        "        raise ValueError(\"X_seq vide\")\n",
        "    S = np.zeros((X_seq[0].shape[0],), dtype=np.int32)\n",
        "    for x in X_seq:\n",
        "        S += x.astype(np.int32, copy=False)\n",
        "    if majority == \"strict\":\n",
        "        return np.where(S >= 0, 1, -1).astype(np.int8, copy=False)\n",
        "    elif majority == \"unbiased\":\n",
        "        return np.where(S >= 0, 1, -1).astype(np.int8, copy=False)\n",
        "    else:\n",
        "        raise ValueError(\"majority must be 'strict' or 'unbiased'\")\n",
        "\n",
        "def span_signatures_from_trace(X_seq, win: int = 12, stride: int = 6, majority: str = \"unbiased\"):\n",
        "    if not X_seq:\n",
        "        return []\n",
        "    T = len(X_seq)\n",
        "    out = []\n",
        "    if T <= win:\n",
        "        out.append(content_signature_from_Xseq(X_seq, majority))\n",
        "        return out\n",
        "    for start in range(0, T - win + 1, max(1, stride)):\n",
        "        stop = start + win\n",
        "        out.append(content_signature_from_Xseq(X_seq[start:stop], majority))\n",
        "    return out\n",
        "\n",
        "def build_mem_pairs_from_encoded(encoded_en, encoded_fr, win=8, stride=4, majority=\"strict\", max_pairs=None):\n",
        "    pairs = []\n",
        "    N = min(len(encoded_en), len(encoded_fr))\n",
        "    for i in range(N):\n",
        "        X_en = encoded_en[i][\"X_seq\"]\n",
        "        X_fr = encoded_fr[i][\"X_seq\"]\n",
        "        spans_en = span_signatures_from_trace(X_en, win=win, stride=stride, majority=majority)\n",
        "        spans_fr = span_signatures_from_trace(X_fr, win=win, stride=stride, majority=majority)\n",
        "        L = min(len(spans_en), len(spans_fr))\n",
        "        for t in range(L):\n",
        "            pairs.append((\n",
        "                spans_en[t].astype(np.int8, copy=False),\n",
        "                spans_fr[t].astype(np.int8, copy=False),\n",
        "            ))\n",
        "            if max_pairs is not None and len(pairs) >= max_pairs:\n",
        "                return pairs\n",
        "    return pairs\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3) Paires MEM = spans EN/FR (contenu, sans K_s)\n",
        "# -------------------------------------------------------------\n",
        "pairs_mem = build_mem_pairs_from_encoded(encoded_en, encoded_fr, win=8, stride=4, majority=\"strict\")\n",
        "print(f\"Pairs available for MEM training: {len(pairs_mem)}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4) Instanciation MEM et entraînement one-pass\n",
        "#    (k ≈ log2(B) + marge ; ici B=256, k=24 convient)\n",
        "# -------------------------------------------------------------\n",
        "MEM_K = 16\n",
        "MEM_BUCKETS = 128\n",
        "cfg = mem_pipeline.MemConfig(D=D, B=MEM_BUCKETS, k=MEM_K, seed_lsh=10, seed_gmem=11)\n",
        "comp = mem_pipeline.make_mem_pipeline(cfg)\n",
        "mem_pipeline.train_one_pass_MEM(comp, pairs_mem)\n",
        "print(\"Training complete; few bucket counts:\", comp.mem.n[:64])\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5) Probe correcte : on interroge avec Z_en (span) et on compare\n",
        "#    le prototype choisi à Z_fr (span) correspondant\n",
        "# -------------------------------------------------------------\n",
        "probe_count = min(200, len(pairs_mem))\n",
        "sim_values = []\n",
        "for Z_en_vec, Z_fr_vec in tqdm(pairs_mem[:probe_count]):\n",
        "    bucket_idx, score = mem_pipeline.infer_map_top1(comp, Z_en_vec)  # Z_en (span), pas H_en\n",
        "    prototype = comp.mem.H[bucket_idx].astype(np.int32, copy=False)\n",
        "    sim = float(np.dot(prototype, Z_fr_vec.astype(np.int32, copy=False)) / D)\n",
        "    sim_values.append(sim)\n",
        "\n",
        "print(f\"Top-1 mean similarity over {probe_count} span-probes: {np.mean(sim_values):.4f}\")\n",
        "print(f\"Top-1 median similarity: {np.median(sim_values):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c82ab18",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pop mean/median/min/max/std: 296.625 293.0 216 477 39.670714954485\n",
            "p90/p99: 345 394\n"
          ]
        }
      ],
      "source": [
        "nb = comp.mem.n\n",
        "print(\"pop mean/median/min/max/std:\",\n",
        "      float(nb.mean()), float(np.median(nb)), int(nb.min()), int(nb.max()), float(nb.std()))\n",
        "print(\"p90/p99:\", int(np.quantile(nb, 0.90)), int(np.quantile(nb, 0.99)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45bad5ba",
      "metadata": {},
      "source": [
        "\n",
        "> ℹ️ **Remarque pratique** : si le téléchargement OPUS échoue (exécution hors-ligne),\n",
        "> le notebook bascule automatiquement sur un mini corpus embarqué afin de\n",
        "> conserver une démonstration reproductible des blocs ENC et MEM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a979536",
      "metadata": {},
      "source": [
        "# DEC"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78cd5622",
      "metadata": {},
      "source": [
        "## DEC-0 : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "52158548",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np, logging\n",
        "log = logging.getLogger(\"DEC\")\n",
        "if not log.handlers:\n",
        "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
        "\n",
        "def sign_strict_pm1(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Retourne int8 dans {-1,+1}; convention 0 -> +1 (majorité stricte).\"\"\"\n",
        "    y = (x >= 0).astype(np.int8, copy=False)  # 0/1\n",
        "    return ((y << 1) - 1).astype(np.int8, copy=False)\n",
        "\n",
        "def hd_assert_pm1(x: np.ndarray, D: int | None = None) -> None:\n",
        "    assert x.dtype == np.int8 and np.all((x == 1) | (x == -1)), \"Vecteur non binaire ±1/int8\"\n",
        "    if D is not None:\n",
        "        assert x.ndim == 1 and x.shape[0] == D, \"Dimension inattendue\"\n",
        "\n",
        "def hd_bind(x: np.ndarray, key: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Binding Hadamard en int8 (produit élémentaire).\"\"\"\n",
        "    return (x.astype(np.int8, copy=False) * key.astype(np.int8, copy=False)).astype(np.int8, copy=False)\n",
        "\n",
        "def hd_sim(x: np.ndarray, y: np.ndarray) -> float:\n",
        "    \"\"\"Similarité cosinus sur ±1 (équivalente à corrélation normalisée).\"\"\"\n",
        "    assert x.shape == y.shape\n",
        "    return float((x.astype(np.int32) @ y.astype(np.int32)) / x.shape[0])\n",
        "\n",
        "def permute_pow(x: np.ndarray, pi: np.ndarray, power: int) -> np.ndarray:\n",
        "    \"\"\"Applique Π^power via indices précalculés si fournis (sinon compose).\"\"\"\n",
        "    idx = np.arange(x.shape[0], dtype=np.int64)\n",
        "    for _ in range(power % x.shape[0]):\n",
        "        idx = pi[idx]\n",
        "    return x[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "22094d40",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# Helpers (cf. DD0 du tutoriel)\n",
        "# ---------------------------\n",
        "def hd_assert_pm1(x: np.ndarray, D: int | None = None) -> None:\n",
        "    assert x.dtype == np.int8 and np.all((x == 1) | (x == -1)), \"expect ±1/int8\"\n",
        "    if D is not None:\n",
        "        assert x.ndim == 1 and x.shape[0] == D, \"wrong shape\"\n",
        "\n",
        "def hd_bind(x: np.ndarray, key: np.ndarray) -> np.ndarray:\n",
        "    return (x.astype(np.int8, copy=False) * key.astype(np.int8, copy=False)).astype(np.int8, copy=False)\n",
        "\n",
        "def hd_sim(x: np.ndarray, y: np.ndarray) -> float:\n",
        "    assert x.shape == y.shape\n",
        "    # produit scalaire en int32 pour stabilité, renvoyé en float64 (double précision)\n",
        "    return float((x.astype(np.int32) @ y.astype(np.int32)) / x.shape[0])\n",
        "\n",
        "def pm1(shape, rng) -> np.ndarray:\n",
        "    \"\"\"Tire des vecteurs Rademacher ±1 en int8, shape=(...), dtype=int8.\"\"\"\n",
        "    return (2 * rng.integers(0, 2, size=shape, dtype=np.int8) - 1).astype(np.int8, copy=False)\n",
        "\n",
        "# ---------------------------\n",
        "# DX0: tests\n",
        "# ---------------------------\n",
        "def dx0_sanity(D: int = 16_384, N_sim: int = 1_000, seed: int = 2024, tol: float = 5e-3) -> None:\n",
        "    \"\"\"\n",
        "    Vérifie:\n",
        "      1) hd_sim(x,x)=1 et hd_sim(x,-x)=-1 (à tol près)\n",
        "      2) Invariance de similarité par binding: sim(x,y)=sim(x⊗k, y⊗k)\n",
        "      3) Préservation de la norme (||x||_2/√D = 1) avant/après binding\n",
        "    Critère d'acceptation (CA): écarts absolus ≤ 5e-3.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    max_err_self = 0.0\n",
        "    max_err_neg  = 0.0\n",
        "    max_err_bind = 0.0\n",
        "    max_err_norm = 0.0\n",
        "\n",
        "    for _ in range(N_sim):\n",
        "        x = pm1(D, rng); y = pm1(D, rng); k = pm1(D, rng)\n",
        "        hd_assert_pm1(x, D); hd_assert_pm1(y, D); hd_assert_pm1(k, D)\n",
        "\n",
        "        # (1) Identités de similarité\n",
        "        s_xx = hd_sim(x, x)\n",
        "        s_xnx = hd_sim(x, (-x).astype(np.int8, copy=False))\n",
        "\n",
        "        max_err_self = max(max_err_self, abs(s_xx - 1.0))\n",
        "        max_err_neg  = max(max_err_neg,  abs(s_xnx + 1.0))\n",
        "\n",
        "        # (2) Invariance par binding (DEC1)\n",
        "        s_xy      = hd_sim(x, y)\n",
        "        xk, yk    = hd_bind(x, k), hd_bind(y, k)\n",
        "        s_xy_bind = hd_sim(xk, yk)\n",
        "        max_err_bind = max(max_err_bind, abs(s_xy - s_xy_bind))\n",
        "\n",
        "        # (3) Normes (avant/après binding)\n",
        "        # Pour des vecteurs ±1, ||x||_2 = sqrt(D). On vérifie la normalisation relative.\n",
        "        norm_x  = np.linalg.norm(x.astype(np.float64)) / np.sqrt(D)\n",
        "        norm_xk = np.linalg.norm(xk.astype(np.float64)) / np.sqrt(D)\n",
        "        max_err_norm = max(max_err_norm, abs(norm_x - 1.0), abs(norm_xk - 1.0))\n",
        "\n",
        "    # Rapport\n",
        "    print(\"DX0 — Sanity checks (double précision)\")\n",
        "    print(f\"  D={D}, N={N_sim}, tol={tol:.1e}\")\n",
        "    print(f\"  max|sim(x,x)-1|         = {max_err_self:.3e}\")\n",
        "    print(f\"  max|sim(x,-x)+1|        = {max_err_neg:.3e}\")\n",
        "    print(f\"  max|sim(x,y)-sim(x⊗k,y⊗k)| = {max_err_bind:.3e}\")\n",
        "    print(f\"  max| ||x||/√D - 1 | (incl. bind) = {max_err_norm:.3e}\")\n",
        "\n",
        "    # Assertions CA\n",
        "    assert max_err_self <= tol,     \"CA non satisfait: sim(x,x) s'écarte de 1\"\n",
        "    assert max_err_neg  <= tol,     \"CA non satisfait: sim(x,-x) s'écarte de -1\"\n",
        "    assert max_err_bind <= tol,     \"CA non satisfait: invariance de similarité après binding\"\n",
        "    assert max_err_norm <= tol,     \"CA non satisfait: norme non préservée (relative)\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c12ada1b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DX0 — Sanity checks (double précision)\n",
            "  D=16384, N=1000, tol=5.0e-03\n",
            "  max|sim(x,x)-1|         = 0.000e+00\n",
            "  max|sim(x,-x)+1|        = 0.000e+00\n",
            "  max|sim(x,y)-sim(x⊗k,y⊗k)| = 0.000e+00\n",
            "  max| ||x||/√D - 1 | (incl. bind) = 0.000e+00\n"
          ]
        }
      ],
      "source": [
        "dx0_sanity()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23b4826d",
      "metadata": {},
      "source": [
        "## DD1 .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "02ebc2a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def DD1_ctx(Hs: np.ndarray, G_DEC: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Q^(s) = H^(s) ⊗ G_DEC, binding isométrique (int8 -> int8).\n",
        "    \"\"\"\n",
        "    assert Hs.dtype == np.int8 and G_DEC.dtype == np.int8\n",
        "    hd_assert_pm1(Hs); hd_assert_pm1(G_DEC, Hs.shape[0])\n",
        "    return hd_bind(Hs, G_DEC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4cc32465",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DX1 — DD1_ctx (isométrie & contrats)\n",
            "  D=16384, m=64, trials=200, tol=5.0e-03\n",
            "  max|sim_before - sim_after|  = 0.000e+00\n",
            "  max| ||H||/√D - 1 | (incl. bind) = 0.000e+00\n",
            "  max|Gram_before - Gram_after| = 0.000e+00\n"
          ]
        }
      ],
      "source": [
        "# --- DX1: tests détaillés ---\n",
        "def dx1_test_DD1_ctx(D: int = 16_384, m: int = 64, trials: int = 200, seed: int = 1234, tol: float = 5e-3):\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # 1) Similarité inchangée et normes préservées (sur 'trials' paires)\n",
        "    max_err_sim = 0.0\n",
        "    max_err_norm = 0.0\n",
        "    for _ in range(trials):\n",
        "        H1, H2, G = pm1(D, rng), pm1(D, rng), pm1(D, rng)\n",
        "        # Copies pour vérifier non-mutation\n",
        "        H1_copy, H2_copy, G_copy = H1.copy(), H2.copy(), G.copy()\n",
        "\n",
        "        Q1, Q2 = DD1_ctx(H1, G), DD1_ctx(H2, G)\n",
        "        # Similarité\n",
        "        s0 = hd_sim(H1, H2)\n",
        "        s1 = hd_sim(Q1, Q2)\n",
        "        max_err_sim = max(max_err_sim, abs(s0 - s1))\n",
        "\n",
        "        # Normes relatives\n",
        "        nH1  = np.linalg.norm(H1.astype(np.float64)) / np.sqrt(D)\n",
        "        nQ1  = np.linalg.norm(Q1.astype(np.float64)) / np.sqrt(D)\n",
        "        nH2  = np.linalg.norm(H2.astype(np.float64)) / np.sqrt(D)\n",
        "        nQ2  = np.linalg.norm(Q2.astype(np.float64)) / np.sqrt(D)\n",
        "        max_err_norm = max(max_err_norm, abs(nH1 - 1.0), abs(nQ1 - 1.0),\n",
        "                                           abs(nH2 - 1.0), abs(nQ2 - 1.0))\n",
        "\n",
        "        # Contrats: dtype & non-mutation\n",
        "        assert Q1.dtype == np.int8 and Q2.dtype == np.int8\n",
        "        assert np.all(H1 == H1_copy) and np.all(H2 == H2_copy) and np.all(G == G_copy), \"mutation détectée\"\n",
        "        assert np.all((Q1 == 1) | (Q1 == -1)) and np.all((Q2 == 1) | (Q2 == -1)), \"sortie hors ±1\"\n",
        "\n",
        "    # 2) Isométrie de Gram (m vecteurs)\n",
        "    H = np.stack([pm1(D, rng) for _ in range(m)], axis=0)  # (m, D) ±1/int8\n",
        "    G = pm1(D, rng)\n",
        "    Q = np.stack([DD1_ctx(H[i], G) for i in range(m)], axis=0)\n",
        "\n",
        "    # Gram avant/après, en double précision\n",
        "    G0 = (H.astype(np.int32) @ H.astype(np.int32).T) / D\n",
        "    G1 = (Q.astype(np.int32) @ Q.astype(np.int32).T) / D\n",
        "    max_err_gram = float(np.max(np.abs(G0.astype(np.float64) - G1.astype(np.float64))))\n",
        "\n",
        "    # --- Rapport ---\n",
        "    print(\"DX1 — DD1_ctx (isométrie & contrats)\")\n",
        "    print(f\"  D={D}, m={m}, trials={trials}, tol={tol:.1e}\")\n",
        "    print(f\"  max|sim_before - sim_after|  = {max_err_sim:.3e}\")\n",
        "    print(f\"  max| ||H||/√D - 1 | (incl. bind) = {max_err_norm:.3e}\")\n",
        "    print(f\"  max|Gram_before - Gram_after| = {max_err_gram:.3e}\")\n",
        "\n",
        "    # --- Critères d'acceptation ---\n",
        "    assert max_err_sim  <= tol, \"Invariance de similarité violée (DEC1)\"\n",
        "    assert max_err_norm <= tol, \"Norme non préservée (relative)\"\n",
        "    assert max_err_gram <= tol, \"Isométrie de Gram violée (DEC1)\"\n",
        "\n",
        "dx1_test_DD1_ctx()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ebe1921",
      "metadata": {},
      "source": [
        "# DD2 . "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "19f4e780",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import logging\n",
        "from typing import List, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "log = logging.getLogger(\"DEC.DX2.v2\")\n",
        "\n",
        "# -- utilitaires (identiques à DD0) --\n",
        "def sign_strict_pm1(x: np.ndarray) -> np.ndarray:\n",
        "    y = (x >= 0).astype(np.int8, copy=False)\n",
        "    return ((y << 1) - 1).astype(np.int8, copy=False)\n",
        "\n",
        "def hd_assert_pm1(x: np.ndarray, D: int | None = None) -> None:\n",
        "    assert x.dtype == np.int8 and np.all((x == 1) | (x == -1)), \"±1/int8 attendu\"\n",
        "    if D is not None:\n",
        "        assert x.ndim == 1 and x.shape[0] == D, \"shape inattendu\"\n",
        "\n",
        "def permute_pow(x: np.ndarray, pi: np.ndarray, power: int) -> np.ndarray:\n",
        "    # NOTE: pour la perf réelle, pré-calculer pi_pows ; ici: version simple et sûre.\n",
        "    idx = np.arange(x.shape[0], dtype=np.int64)\n",
        "    for _ in range(power % x.shape[0]):\n",
        "        idx = pi[idx]\n",
        "    return x[idx]\n",
        "\n",
        "def hd_sim(x: np.ndarray, y: np.ndarray) -> float:\n",
        "    return float((x.astype(np.int32) @ y.astype(np.int32)) / x.shape[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "326e389d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_perm_inverse(pi: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Construit l'inverse de la permutation pi (ndarray d'indices).\"\"\"\n",
        "    assert isinstance(pi, np.ndarray) and pi.ndim == 1\n",
        "    pi_inv = np.empty_like(pi)\n",
        "    pi_inv[pi] = np.arange(pi.shape[0], dtype=pi.dtype)\n",
        "    return pi_inv\n",
        "\n",
        "def permute_pow_signed(x: np.ndarray, pi: np.ndarray, pi_inv: np.ndarray, power: int) -> np.ndarray:\n",
        "    r\"\"\"\n",
        "    Applique Π^power sur x (±1/int8) avec gestion des puissances négatives via Π^{-1}.\n",
        "    - Complexity OK ici car |power| ≤ ell (≤ 8 dans DX2).\n",
        "    - Pour de grands exponents, préférer pré-calcul d'un tableau pi_pows.\n",
        "    \"\"\"\n",
        "    assert x.ndim == 1 and x.shape[0] == pi.shape[0] == pi_inv.shape[0]\n",
        "    if power == 0:\n",
        "        return x\n",
        "    idx = np.arange(x.shape[0], dtype=np.int64)\n",
        "    if power > 0:\n",
        "        for _ in range(power):\n",
        "            idx = pi[idx]\n",
        "    else:\n",
        "        for _ in range(-power):\n",
        "            idx = pi_inv[idx]\n",
        "    return x[idx].astype(np.int8, copy=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "8a257745",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# DD2_query + DX2 (corrigés)\n",
        "# =========================\n",
        "# Changements clés :\n",
        "#  - DD2_query : hyperparamètres keyword-only (*, alpha, beta, ell) pour éviter\n",
        "#    le doublonnage positionnel/mot-clé.\n",
        "#  - Implémentation robuste des permutations Π^k pour k ∈ ℤ (k<0 via Π^{-1}).\n",
        "#  - Contrôles de types/signatures (±1/int8) et normalisation ||R_t||≈√D.\n",
        "#  - DX2_run : corrections des tests (in_band), invariances Gram et identité\n",
        "#    paire-à-paire <Π^i L_i, Π^k L_k> = <L_i, Π^{k-i} L_k>.\n",
        "#  - Journalisation propre.\n",
        "\n",
        "import numpy as np\n",
        "from typing import List\n",
        "import logging\n",
        "\n",
        "log = logging.getLogger(\"DX2\")\n",
        "if not log.handlers:\n",
        "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
        "\n",
        "# -------- Utilitaires (contrats & permutations) --------------------------------\n",
        "\n",
        "def hd_assert_pm1(x: np.ndarray, D: int | None = None) -> None:\n",
        "    assert isinstance(x, np.ndarray), \"attendu np.ndarray\"\n",
        "    assert x.dtype == np.int8, \"dtype attendu: int8\"\n",
        "    assert np.all((x == 1) | (x == -1)), \"valeurs attendues: ±1\"\n",
        "    if D is not None:\n",
        "        assert x.ndim == 1 and x.shape[0] == D, f\"forme attendue: ({D},)\"\n",
        "\n",
        "def sign_strict_pm1(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Retourne int8 dans {-1,+1} ; convention 0 -> +1 (majorité stricte).\"\"\"\n",
        "    y = (x >= 0).astype(np.int8, copy=False)  # 0/1\n",
        "    return ((y << 1) - 1).astype(np.int8, copy=False)\n",
        "\n",
        "def build_perm_inverse(pi: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Inverse de permutation Π^{-1} telle que pi_inv[pi[i]] = i.\"\"\"\n",
        "    assert pi.ndim == 1 and np.issubdtype(pi.dtype, np.integer)\n",
        "    pi_inv = np.empty_like(pi)\n",
        "    pi_inv[pi] = np.arange(pi.shape[0], dtype=pi.dtype)\n",
        "    return pi_inv\n",
        "\n",
        "def permute_pow_signed(x: np.ndarray, pi: np.ndarray, pi_inv: np.ndarray, k: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Applique Π^k pour k ∈ ℤ. k>=0 : itérations de pi ; k<0 : itérations de pi_inv.\n",
        "    Complexité O(|k|), suffisante pour les tests unitaires.\n",
        "    \"\"\"\n",
        "    D = x.shape[0]\n",
        "    if k == 0:\n",
        "        return x\n",
        "    idx = np.arange(D, dtype=np.int64)\n",
        "    if k > 0:\n",
        "        for _ in range(k % D):\n",
        "            idx = pi[idx]\n",
        "    else:\n",
        "        for _ in range((-k) % D):\n",
        "            idx = pi_inv[idx]\n",
        "    return x[idx]\n",
        "\n",
        "# -------- Module DD2 (requête mixte) -------------------------------------------\n",
        "\n",
        "def DD2_query(\n",
        "    Qs: np.ndarray,\n",
        "    hist_tokens: List[np.ndarray],   # liste des L_fr(\\hat v_{t-j}) en ±1/int8\n",
        "    pi: np.ndarray,\n",
        "    *,\n",
        "    alpha: float = 1.0,\n",
        "    beta:  float = 1.0,\n",
        "    ell:   int   = 4\n",
        ") -> np.ndarray:\n",
        "    r\"\"\"\n",
        "    R_t = α·Qs + β·sign( \\sum_{j=1..ell} Π^j L_{t-j} ), puis normalisation à ||R_t|| ≈ √D.\n",
        "    - Qs ∈ {±1}^D (int8)\n",
        "    - L_{t-j} ∈ {±1}^D (int8)\n",
        "    Retourne un vecteur float64 de norme ≈ √D (utile pour DX2).\n",
        "    \"\"\"\n",
        "    D = Qs.shape[0]\n",
        "    hd_assert_pm1(Qs, D)\n",
        "    assert pi.ndim == 1 and pi.shape[0] == D and np.issubdtype(pi.dtype, np.integer), \"pi invalide\"\n",
        "    assert isinstance(ell, int) and ell >= 0, \"ell >= 0 attendu\"\n",
        "\n",
        "    # borne la fenêtre si l'historique est plus court\n",
        "    hwin = hist_tokens[:ell]\n",
        "    for v in hwin:\n",
        "        hd_assert_pm1(v, D)\n",
        "\n",
        "    pi_inv = build_perm_inverse(pi)\n",
        "\n",
        "    # Agrégation historique : H_hist = sign( sum_j Π^j L_{t-j} )\n",
        "    if len(hwin) == 0:\n",
        "        H_hist = np.ones(D, dtype=np.int8)\n",
        "    else:\n",
        "        acc = np.zeros(D, dtype=np.int16)\n",
        "        # j=1..len(hwin) (on considère hist_tokens[0] = L_{t-1})\n",
        "        for j, L_j in enumerate(hwin, start=1):\n",
        "            acc += permute_pow_signed(L_j, pi, pi_inv, j).astype(np.int16, copy=False)\n",
        "        H_hist = sign_strict_pm1(acc)  # ±1/int8\n",
        "\n",
        "    # Combinaison linéaire puis normalisation à norme √D\n",
        "    Rt = alpha * Qs.astype(np.float64) + beta * H_hist.astype(np.float64)\n",
        "    nrm = float(np.linalg.norm(Rt))\n",
        "    if nrm > 0:\n",
        "        Rt = Rt / nrm * np.sqrt(D)\n",
        "    else:\n",
        "        # cas dégénéré improbable ; garde-fou\n",
        "        Rt = np.ones(D, dtype=np.float64)\n",
        "    return Rt  # float64\n",
        "\n",
        "# -------- Banc de test DX2 -----------------------------------------------------\n",
        "\n",
        "def DX2_run():\n",
        "    D, trials = 16384, 200\n",
        "    ells = (2, 4, 8)\n",
        "    ratios = (1/3, 1.0, 3.0)\n",
        "    g = np.random.default_rng(2025)\n",
        "\n",
        "    # permutation aléatoire (fixée) et son inverse\n",
        "    pi = np.arange(D, dtype=np.int64); g.shuffle(pi)\n",
        "    pi_inv = build_perm_inverse(pi)\n",
        "\n",
        "    def rand_pm1():\n",
        "        r = g.integers(0, 2, size=D, dtype=np.int8)\n",
        "        return ((r << 1) - 1).astype(np.int8, copy=False)\n",
        "\n",
        "    def sim(a: np.ndarray, b: np.ndarray) -> float:\n",
        "        return float((a.astype(np.int32) @ b.astype(np.int32)) / D)\n",
        "\n",
        "    norms: dict[tuple[int, float], tuple[float, float, float]] = {}\n",
        "    gram_uniform_ok = True\n",
        "    pair_shift_ok   = True\n",
        "\n",
        "    for ell in ells:\n",
        "        for r in ratios:\n",
        "            alpha, beta = r, 1.0\n",
        "            vals = []\n",
        "            for _ in range(trials):\n",
        "                # Qs et historique\n",
        "                Qs   = rand_pm1()\n",
        "                hist = [rand_pm1() for _ in range(ell)]\n",
        "\n",
        "                # Matrice des versions permutées P[j] = Π^{j+1} L_{t-(j+1)}\n",
        "                P = np.stack([permute_pow_signed(hist[j], pi, pi_inv, j+1)\n",
        "                              for j in range(ell)], axis=0).astype(np.int8, copy=False)\n",
        "\n",
        "                # (i) Invariance Gram sous permutation UNIFORME (même décalage s pour toutes les lignes)\n",
        "                s = int(g.integers(1, 7))\n",
        "                P_uni = np.stack([permute_pow_signed(P[j], pi, pi_inv, s)\n",
        "                                  for j in range(ell)], axis=0).astype(np.int8, copy=False)\n",
        "                # Gram (corrélations normalisées) avant / après\n",
        "                G  = (P.astype(np.int32) @ P.T.astype(np.int32)) / D\n",
        "                Gu = (P_uni.astype(np.int32) @ P_uni.T.astype(np.int32)) / D\n",
        "                if not np.allclose(G, Gu, atol=5e-3, rtol=0):\n",
        "                    gram_uniform_ok = False\n",
        "\n",
        "                # (ii) Identité paire-à-paire :\n",
        "                #      <Π^i L_i, Π^k L_k> == <L_i, Π^{k-i} L_k>  pour i,k = 1..ell\n",
        "                for i in range(1, ell+1):\n",
        "                    for k in range(1, ell+1):\n",
        "                        lhs = sim(permute_pow_signed(hist[i-1], pi, pi_inv, i),\n",
        "                                  permute_pow_signed(hist[k-1], pi, pi_inv, k))\n",
        "                        rhs = sim(hist[i-1],\n",
        "                                  permute_pow_signed(hist[k-1], pi, pi_inv, k - i))\n",
        "                        if abs(lhs - rhs) > 5e-3:\n",
        "                            pair_shift_ok = False\n",
        "                            break\n",
        "                    if not pair_shift_ok:\n",
        "                        break\n",
        "\n",
        "                # (iii) Norme de R_t / √D dans [0.9, 1.1]\n",
        "                Rt = DD2_query(Qs, hist, pi, alpha=alpha, beta=beta, ell=ell)\n",
        "                vals.append(float(np.linalg.norm(Rt) / np.sqrt(D)))\n",
        "\n",
        "            norms[(ell, r)] = (min(vals), float(np.median(vals)), max(vals))\n",
        "\n",
        "    # Reporting\n",
        "    log.info(\"DX2 — Norme(R_t)/sqrt(D) par (ell, alpha/beta): min | median | max\")\n",
        "    for (ell, r), (mn, md, mx) in sorted(norms.items()):\n",
        "        log.info(\"  ell=%d, alpha/beta=%.3g  ->  %.3f | %.3f | %.3f\", ell, r, mn, md, mx)\n",
        "\n",
        "    # Critères d’acceptation\n",
        "    in_band = all((0.9 <= mn <= 1.1) and (0.9 <= md <= 1.1) and (0.9 <= mx <= 1.1)\n",
        "                  for (mn, md, mx) in norms.values())\n",
        "    assert in_band, \"DX2: norme(R_t)/sqrt(D) hors bande [0.9,1.1] pour au moins un (ell, ratio).\"\n",
        "    assert gram_uniform_ok, \"DX2: Gram NON invariant sous permutation uniforme (isométrie violée).\"\n",
        "    assert pair_shift_ok,   \"DX2: identité de décalage paire-à-paire violée.\"\n",
        "\n",
        "    log.info(\"DX2 — CA validés: (i) norme ∈ [0.9,1.1] ; (ii) Gram invariant ; (iii) identité paire-à-paire OK.\")\n",
        "\n",
        "# --- Exécution du test (à commenter/supprimer si vous intégrez dans une suite) ---\n",
        "# DX2_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "4e4acc1e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-06 23:22:58,772 [INFO] DX2 — Norme(R_t)/sqrt(D) par (ell, alpha/beta): min | median | max\n",
            "2025-10-06 23:22:58,772 [INFO]   ell=2, alpha/beta=0.333  ->  1.000 | 1.000 | 1.000\n",
            "2025-10-06 23:22:58,772 [INFO]   ell=2, alpha/beta=1  ->  1.000 | 1.000 | 1.000\n",
            "2025-10-06 23:22:58,772 [INFO]   ell=2, alpha/beta=3  ->  1.000 | 1.000 | 1.000\n",
            "2025-10-06 23:22:58,772 [INFO]   ell=4, alpha/beta=0.333  ->  1.000 | 1.000 | 1.000\n",
            "2025-10-06 23:22:58,773 [INFO]   ell=4, alpha/beta=1  ->  1.000 | 1.000 | 1.000\n",
            "2025-10-06 23:22:58,773 [INFO]   ell=4, alpha/beta=3  ->  1.000 | 1.000 | 1.000\n",
            "2025-10-06 23:22:58,773 [INFO]   ell=8, alpha/beta=0.333  ->  1.000 | 1.000 | 1.000\n",
            "2025-10-06 23:22:58,773 [INFO]   ell=8, alpha/beta=1  ->  1.000 | 1.000 | 1.000\n",
            "2025-10-06 23:22:58,773 [INFO]   ell=8, alpha/beta=3  ->  1.000 | 1.000 | 1.000\n",
            "2025-10-06 23:22:58,773 [INFO] DX2 — CA validés: (i) norme ∈ [0.9,1.1] ; (ii) Gram invariant ; (iii) identité paire-à-paire OK.\n"
          ]
        }
      ],
      "source": [
        "DX2_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47d999ef",
      "metadata": {},
      "source": [
        "# DD3 . "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "871f3897",
      "metadata": {},
      "outputs": [],
      "source": [
        "def DD3_bindToMem(Rt: np.ndarray, G_MEM: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"~R_t = R_t ⊗ G_MEM (int8 -> int8).\"\"\"\n",
        "    hd_assert_pm1(Rt); hd_assert_pm1(G_MEM, Rt.shape[0])\n",
        "    return hd_bind(Rt, G_MEM)\n",
        "\n",
        "def hd_sim_dot(x: np.ndarray, y: np.ndarray) -> int:\n",
        "    \"\"\"Produit scalaire entier (évite l'arrondi); x,y en int8 ±1.\"\"\"\n",
        "    return int(x.astype(np.int32) @ y.astype(np.int32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f40787be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- test KS (asymptotique) sans dépendance externe ---\n",
        "def ks_2samp_asymp(x: np.ndarray, y: np.ndarray) -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    KS à 2 échantillons: renvoie (D_stat, pval approx).\n",
        "    Correctifs:\n",
        "      - si D_stat == 0 => p = 1.0 (distributions identiques)\n",
        "      - clamp numérique sur lambda pour petits D_stat\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=np.float64)\n",
        "    y = np.asarray(y, dtype=np.float64)\n",
        "    n, m = x.size, y.size\n",
        "    x_sorted = np.sort(x); y_sorted = np.sort(y)\n",
        "    i = j = 0\n",
        "    cdf_x = cdf_y = 0.0\n",
        "    D_stat = 0.0\n",
        "    while i < n and j < m:\n",
        "        if x_sorted[i] < y_sorted[j]:\n",
        "            cdf_x = (i + 1) / n; i += 1\n",
        "        elif x_sorted[i] > y_sorted[j]:\n",
        "            cdf_y = (j + 1) / m; j += 1\n",
        "        else:\n",
        "            v = x_sorted[i]\n",
        "            while i < n and x_sorted[i] == v: i += 1\n",
        "            while j < m and y_sorted[j] == v: j += 1\n",
        "            cdf_x = i / n; cdf_y = j / m\n",
        "        D_stat = max(D_stat, abs(cdf_x - cdf_y))\n",
        "    if i < n: D_stat = max(D_stat, abs(1.0 - (j / m)))\n",
        "    if j < m: D_stat = max(D_stat, abs(1.0 - (i / n)))\n",
        "\n",
        "    # --- Correctif dégénéré ---\n",
        "    if D_stat == 0.0:\n",
        "        return 0.0, 1.0\n",
        "\n",
        "    en = np.sqrt(n * m / (n + m))\n",
        "    lam = (en + 0.12 + 0.11 / max(en, 1e-12)) * D_stat\n",
        "    # Pour très petits \"lam\", la série tend vers 1 => borne supérieure 1.0\n",
        "    if lam < 1e-8:\n",
        "        return float(D_stat), 1.0\n",
        "\n",
        "    # Évaluation de la série alternée (tronquée) avec coupe stricte dans [0,1]\n",
        "    terms = [np.exp(-2.0 * (k**2) * (lam**2)) for k in range(1, 201)]\n",
        "    pval = 2.0 * sum(((-1)**(k-1)) * terms[k-1] for k in range(1, len(terms)+1))\n",
        "    pval = float(max(0.0, min(1.0, pval)))\n",
        "    return float(D_stat), pval\n",
        "\n",
        "# --- campagne DX3 ---\n",
        "def DX3_run(D: int = 16384, C: int = 500, T: int = 200, seed: int = 2025,\n",
        "            rel_tol: float = 0.01, pmin: float = 0.10) -> None:\n",
        "    \"\"\"\n",
        "    D: dimension; C: #protos mémoire; T: #requêtes; rel_tol: seuil d'écart relatif moyen; pmin: seuil KS.\n",
        "    \"\"\"\n",
        "    g = np.random.default_rng(seed)\n",
        "\n",
        "    # Génère clés et banques en ±1/int8\n",
        "    def rand_pm1(size: int) -> np.ndarray:\n",
        "        r = g.integers(0, 2, size=size, dtype=np.int8)\n",
        "        return ((r << 1) - 1).astype(np.int8, copy=False)\n",
        "\n",
        "    G_MEM = rand_pm1(D)\n",
        "    M_bank = np.stack([rand_pm1(D) for _ in range(C)], axis=0)   # (C, D), int8\n",
        "    Q_batch = np.stack([rand_pm1(D) for _ in range(T)], axis=0)  # (T, D), int8\n",
        "\n",
        "    # Scores \"dans la tranche mémoire\" vs \"débindés\"\n",
        "    #   S_mem[t,c]   = < Rt⊗G_MEM , M_c >\n",
        "    #   S_unbd[t,c]  = < Rt , M_c⊗G_MEM >\n",
        "    S_mem  = np.zeros((T, C), dtype=np.int32)\n",
        "    S_unbd = np.zeros((T, C), dtype=np.int32)\n",
        "\n",
        "    for t in range(T):\n",
        "        Rt = Q_batch[t]\n",
        "        Rt_mem = DD3_bindToMem(Rt, G_MEM)            # Rt ⊗ G_MEM\n",
        "        for c in range(C):\n",
        "            Mc = M_bank[c]\n",
        "            S_mem[t, c]  = hd_sim_dot(Rt_mem, Mc)\n",
        "            S_unbd[t, c] = hd_sim_dot(Rt, hd_bind(Mc, G_MEM))\n",
        "\n",
        "    # a) Erreur relative moyenne (sur tous les scores)\n",
        "    A = S_mem.astype(np.float64).ravel()\n",
        "    B = S_unbd.astype(np.float64).ravel()\n",
        "    denom = np.maximum(1.0, np.abs(B))               # évite division par 0\n",
        "    rel_err = np.abs(A - B) / denom\n",
        "    rel_err_mean = float(np.mean(rel_err))\n",
        "\n",
        "    # b) Test KS sur distributions aplaties\n",
        "    D_stat, pval = ks_2samp_asymp(A, B)\n",
        "\n",
        "    # Reporting\n",
        "    log.info(\"DX3 — Invariance (dé)binding mémoire\")\n",
        "    log.info(\"  D=%d, C=%d, T=%d\", D, C, T)\n",
        "    log.info(\"  Erreur relative moyenne  = %.6f\", rel_err_mean)\n",
        "    log.info(\"  KS: D=%.6f, p=%.3f\", D_stat, pval)\n",
        "\n",
        "    # CA\n",
        "    assert rel_err_mean <= rel_tol, f\"DX3: erreur relative moyenne {rel_err_mean:.4f} > {rel_tol}\"\n",
        "    assert pval > pmin, f\"DX3: p-value KS {pval:.3f} ≤ {pmin:.2f}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c21c01a6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-06 22:57:03,498 [INFO] DX3 — Invariance (dé)binding mémoire\n",
            "2025-10-06 22:57:03,498 [INFO]   D=16384, C=500, T=200\n",
            "2025-10-06 22:57:03,498 [INFO]   Erreur relative moyenne  = 0.000000\n",
            "2025-10-06 22:57:03,498 [INFO]   KS: D=0.000000, p=1.000\n"
          ]
        }
      ],
      "source": [
        "DX3_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fb3d23c",
      "metadata": {},
      "source": [
        "# DD4 . "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c549139a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def DD4_search_topK(Rt_tilde: np.ndarray, prototypes: np.ndarray, K: int) -> tuple[int, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    prototypes: array shape (B, D) en int8 (±1) pour M_c seuillés ou non seuillés normalisés.\n",
        "    Retour: (c_star, C_K, scores_CK)\n",
        "    \"\"\"\n",
        "    D = Rt_tilde.shape[0]\n",
        "    assert prototypes.ndim == 2 and prototypes.shape[1] == D and prototypes.dtype == np.int8\n",
        "    # Produits scalaires stables\n",
        "    scores = (prototypes.astype(np.int32) @ Rt_tilde.astype(np.int32)).astype(np.int32)  # (B,)\n",
        "    K = min(K, scores.shape[0])\n",
        "    idx = np.argpartition(scores, -K)[-K:]\n",
        "    top_order = idx[np.argsort(scores[idx])[::-1]]\n",
        "    c_star = int(top_order[0])\n",
        "    return c_star, top_order, scores[top_order]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "48850b8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm \n",
        "\n",
        "def DX4_run(D: int = 16384, B: int = 10000, trials: int = 200, \n",
        "            Ks=(100, 500, 2000), seed: int = 0) -> dict[int, float]:\n",
        "    \"\"\"\n",
        "    Mesure empirique du rappel de c* parmi les top-K prototypes.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    recalls = {K: 0 for K in Ks}\n",
        "    for _ in tqdm(range(trials)):\n",
        "        # Génère B prototypes ±1 (int8)\n",
        "        prototypes = rng.choice([-1, 1], size=(B, D))\n",
        "        prototypes = prototypes.astype(np.int8)\n",
        "        # Choisit une classe cible c*\n",
        "        c_star = rng.integers(0, B)\n",
        "        Rt = prototypes[c_star].copy()\n",
        "        # Appel au module DD4\n",
        "        _, C_K, _ = DD4_search_topK(Rt, prototypes, max(Ks))\n",
        "        for K in Ks:\n",
        "            if c_star in C_K[:K]:\n",
        "                recalls[K] += 1\n",
        "    # Moyenne\n",
        "    return {K: recalls[K]/trials for K in Ks}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2e3dea67",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [03:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{100: 1.0, 500: 1.0, 2000: 1.0}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DX4_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b943256",
      "metadata": {},
      "source": [
        "# DD5 . "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "bff951c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def DD5_payload(Mc: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Mc: prototype non seuillé (int16/int32) OU déjà binaire int8.\n",
        "    Renvoie Z_hat en int8 (±1).\n",
        "    \"\"\"\n",
        "    if Mc.dtype == np.int8:\n",
        "        hd_assert_pm1(Mc)\n",
        "        return Mc\n",
        "    return sign_strict_pm1(Mc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e44670a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def DX5_run(D: int = 16384, trials: int = 200, ms=(4, 8, 16), seed: int = 0):\n",
        "    \"\"\"\n",
        "    Mesure l’exactitude binaire en fonction du nombre m_{c*}.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    accuracies = {}\n",
        "    for m in ms:\n",
        "        accs = []\n",
        "        for _ in range(trials):\n",
        "            # Vecteur de référence\n",
        "            ref = rng.choice([-1, 1], size=D).astype(np.int8)\n",
        "            # Accumulation de m copies bruitées\n",
        "            acc = np.zeros(D, dtype=np.int32)\n",
        "            for _ in range(m):\n",
        "                acc += ref\n",
        "            # Seuillage\n",
        "            Z_hat = DD5_payload(acc)\n",
        "            # Exactitude binaire\n",
        "            accs.append(np.mean(Z_hat == ref))\n",
        "        accuracies[m] = float(np.mean(accs))\n",
        "    return accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "cdb3df09",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{4: 1.0, 8: 1.0, 16: 1.0}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DX5_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa074d57",
      "metadata": {},
      "source": [
        "# DD6 . "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a7ff4980",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def DD6_vote(\n",
        "#     Z_hat: np.ndarray,\n",
        "#     H_LM: np.ndarray,\n",
        "#     L_fr,\n",
        "#     cand_vocab: list[str],\n",
        "#     lam: float = 0.0\n",
        "# ) -> tuple[str, np.ndarray]:\n",
        "#     \"\"\"\n",
        "#     Renvoie (token*, scores) sur cand_vocab.\n",
        "#     \"\"\"\n",
        "#     D = Z_hat.shape[0]\n",
        "#     hd_assert_pm1(Z_hat, D); hd_assert_pm1(H_LM, D)\n",
        "#     scores = []\n",
        "#     for v in cand_vocab:\n",
        "#         Lv = L_fr(v).astype(np.int8, copy=False)\n",
        "#         hd_assert_pm1(Lv, D)\n",
        "#         s = (Z_hat.astype(np.int32) @ Lv.astype(np.int32)) \\\n",
        "#             + lam * (H_LM.astype(np.int32) @ Lv.astype(np.int32))\n",
        "#         scores.append(float(s))\n",
        "#     scores = np.asarray(scores, dtype=np.float32)\n",
        "#     best = int(np.argmax(scores))\n",
        "#     return cand_vocab[best], scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9e56642b",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToyLexFR:\n",
        "    def __init__(self, vocab: list[str], D: int, seed: int = 1234):\n",
        "        self.vocab = vocab\n",
        "        self.D = D\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        # table de vecteurs ±1/int8\n",
        "        self.table = {v: self.rng.choice(np.array([-1, 1], dtype=np.int8), size=D) for v in vocab}\n",
        "\n",
        "    def __call__(self, v: str) -> np.ndarray:\n",
        "        return self.table[v]\n",
        "\n",
        "# # -- Génération contrôlée de corrélations (flip par coordonnée) ----------------\n",
        "# def flip_to_target(vec: np.ndarray, target_sim: float, rng: np.random.Generator) -> np.ndarray:\n",
        "#     \"\"\"\n",
        "#     Retourne une copie de 'vec' dont la similarité attendue vaut 'target_sim'.\n",
        "#     Pour ±1, si p_flip = (1 - target_sim)/2, alors E[sim] = 1 - 2*p_flip = target_sim.\n",
        "#     \"\"\"\n",
        "#     D = vec.shape[0]\n",
        "#     p_flip = max(0.0, min(1.0, (1.0 - float(target_sim)) / 2.0))\n",
        "#     mask = (rng.random(D) < p_flip).astype(np.int8)          # 1 si on flippe\n",
        "#     flips = (1 - 2 * mask).astype(np.int8, copy=False)       # 1 -> -1, 0 -> +1\n",
        "#     out = (vec.astype(np.int8, copy=False) * flips).astype(np.int8, copy=False)\n",
        "#     return out\n",
        "\n",
        "# # -- Module testé (fourni) ------------------------------------------------------\n",
        "\n",
        "# def _batch_lex(cand_vocab, L):\n",
        "#     \"\"\"\n",
        "#     Applique le callable 'L' (v -> ±1/int8 de forme (D,)) sur tout le vocabulaire candidat\n",
        "#     et empile en une matrice (V, D) en int8.\n",
        "#     \"\"\"\n",
        "#     mats = []\n",
        "#     for v in cand_vocab:\n",
        "#         vec = L(v).astype(np.int8, copy=False)\n",
        "#         mats.append(vec)\n",
        "#     M = np.vstack(mats).astype(np.int8, copy=False)\n",
        "#     return M\n",
        "\n",
        "# def DD6_vote(\n",
        "#     Z_hat: np.ndarray,\n",
        "#     H_LM: np.ndarray,\n",
        "#     L_mem,                  # callable: v -> ±1 int8 (D,)\n",
        "#     L_lm,                   # callable: v -> ±1 int8 (D,)\n",
        "#     cand_vocab: list[str],\n",
        "#     lam: float = 0.0,\n",
        "#     *,\n",
        "#     normalize: str = \"sqrtD\",   # {\"none\",\"sqrtD\"} ; \"sqrtD\" conseillé pour perplexité\n",
        "#     return_probs: bool = False, # si True, renvoie aussi les probabilités softmax\n",
        "#     tau: float = 1.0            # température du softmax (si return_probs=True)\n",
        "# ) -> tuple[str, np.ndarray, np.ndarray | None]:\n",
        "#     \"\"\"\n",
        "#     s(v) = <Z_hat, L_mem(v)> + lam * <H_LM, L_lm(v)>\n",
        "#     Retourne: (token*, scores_raw, probs|None)\n",
        "#       - scores_raw: np.float64 de taille V (non normalisés, utiles pour debug/traçage)\n",
        "#       - probs:      np.float64 de taille V si return_probs=True (softmax stable)\n",
        "#     Contrats:\n",
        "#       Z_hat, H_LM: ±1/int8, de longueur D identique.\n",
        "#       L_mem, L_lm: renvoient ±1/int8 (D,) pour tout v de cand_vocab.\n",
        "#     \"\"\"\n",
        "#     # --- Contrats de forme et de type\n",
        "#     D = int(Z_hat.shape[0])\n",
        "#     hd_assert_pm1(Z_hat, D)\n",
        "#     hd_assert_pm1(H_LM, D)\n",
        "#     assert isinstance(cand_vocab, (list, tuple)) and len(cand_vocab) > 0, \"cand_vocab vide\"\n",
        "\n",
        "#     # --- Matrices lexicales (V, D) en int8 (vectorisation)\n",
        "#     M_mem = _batch_lex(cand_vocab, L_mem)   # (V, D)\n",
        "#     M_lm  = _batch_lex(cand_vocab, L_lm)    # (V, D)\n",
        "#     assert M_mem.shape == M_lm.shape == (len(cand_vocab), D), \"Shapes (V,D) incohérents\"\n",
        "\n",
        "#     # --- Produits scalaires vectorisés (int32 pour éviter overflow)\n",
        "#     z32  = Z_hat.astype(np.int32, copy=False)\n",
        "#     h32  = H_LM.astype(np.int32, copy=False)\n",
        "#     mem_scores = (M_mem.astype(np.int32, copy=False) @ z32)              # (V,)\n",
        "#     lm_scores  = (M_lm.astype(np.int32,  copy=False) @ h32)              # (V,)\n",
        "#     scores_raw = mem_scores.astype(np.float64) + float(lam) * lm_scores.astype(np.float64)\n",
        "\n",
        "#     # --- Argmax sur scores bruts (l'échelle n'affecte pas l'argmax)\n",
        "#     best_idx   = int(np.argmax(scores_raw))\n",
        "#     token_star = cand_vocab[best_idx]\n",
        "\n",
        "#     # --- Option: probabilités (softmax stable) avec normalisation choisie\n",
        "#     probs = None\n",
        "#     if return_probs:\n",
        "#         if normalize == \"sqrtD\":\n",
        "#             logits = scores_raw / (np.sqrt(D) * max(1e-6, float(tau)))\n",
        "#         elif normalize == \"none\":\n",
        "#             logits = scores_raw / max(1e-6, float(tau))\n",
        "#         else:\n",
        "#             raise ValueError(\"normalize ∈ {'none','sqrtD'} attendu\")\n",
        "#         logits = logits - np.max(logits)                  # stabilité num.\n",
        "#         exps   = np.exp(logits, dtype=np.float64)\n",
        "#         probs  = exps / np.sum(exps, dtype=np.float64)    # (V,)\n",
        "#         probs  = probs.astype(np.float64, copy=False)\n",
        "\n",
        "#     return token_star, scores_raw, probs\n",
        "\n",
        "# # -- Perplexité HD: softmax sur scores normalisés par D -------------------------\n",
        "# def hd_perplexity(scores: np.ndarray, true_idx: int, D: int, tau: float = 1.0) -> float:\n",
        "#     \"\"\"\n",
        "#     Perplexité = exp( - log p(true) ), avec p ∝ exp( (scores/D)/tau ).\n",
        "#     On divise par D pour éviter des logits trop grands (HD).\n",
        "#     \"\"\"\n",
        "#     logits = scores / (D * max(1e-6, tau))\n",
        "#     logits = logits - np.max(logits)               # stabilité\n",
        "#     exps = np.exp(logits)\n",
        "#     p = exps / np.sum(exps)\n",
        "#     p_true = float(max(p[true_idx], 1e-12))\n",
        "#     return float(np.exp(-np.log(p_true)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "514630c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def _softmax_probs(scores: np.ndarray, D: int, tau: float = 1.0) -> np.ndarray:\n",
        "#     # Normalisation par sqrt(D) pour éviter la sur-concentration à grande dimension\n",
        "#     s = scores / (np.sqrt(D) * tau)\n",
        "#     s = s - np.max(s)                       # stabilité numérique\n",
        "#     exps = np.exp(s)\n",
        "#     return exps / np.sum(exps)\n",
        "\n",
        "# def hd_perplexity(scores: np.ndarray, true_index: int, D: int, tau: float = 1.0) -> float:\n",
        "#     p = _softmax_probs(scores, D=D, tau=tau)[true_index]\n",
        "#     # Perplexité = exp(-log p_y) ; bornée inférieurement par 1\n",
        "#     return float(np.exp(-np.log(max(p, 1e-12))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "66f1b8d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def DX6_run_two_spaces(\n",
        "#     D: int = 16384, trials: int = 400,\n",
        "#     lam_grid=(0.0, 0.5, 1.0),\n",
        "#     # corrélations du vrai token:\n",
        "#     sim_payload: float = 0.82,   # corr(Z_hat, L_mem(y))\n",
        "#     sim_lm: float      = 0.65,   # corr(H_LM, L_lm(y))\n",
        "#     # confondeurs:\n",
        "#     n_confounders: int = 6,\n",
        "#     rho_mem_conf: float = 0.72,  # corr(Z_hat, L_mem(conf))\n",
        "#     rho_lm_conf: float  = 0.05,  # corr(H_LM, L_lm(conf))\n",
        "#     tau: float = 1.0,\n",
        "#     rng_seed: int = 7031\n",
        "# ):\n",
        "#     g = np.random.default_rng(rng_seed)\n",
        "\n",
        "#     def rademacher(D):  # ±1/int8\n",
        "#         return g.choice(np.array([-1,1], dtype=np.int8), size=D)\n",
        "\n",
        "#     def correlated_pm1(proto: np.ndarray, rho: float) -> np.ndarray:\n",
        "#         noise = rademacher(proto.shape[0])\n",
        "#         mix = rho * proto.astype(np.int32) + (1-rho) * noise.astype(np.int32)\n",
        "#         return np.where(mix >= 0, 1, -1).astype(np.int8)\n",
        "\n",
        "#     def make_trial():\n",
        "#         Z_true  = rademacher(D)  # payload cible\n",
        "#         H_true  = rademacher(D)  # LM cible\n",
        "#         # Construire DEUX lexiques: L_mem (pour la mémoire) et L_lm (pour le LM)\n",
        "#         V = n_confounders + 1\n",
        "#         L_mem = np.empty((V, D), dtype=np.int8)\n",
        "#         L_lm  = np.empty((V, D), dtype=np.int8)\n",
        "#         # y (indice 0)\n",
        "#         L_mem[0] = correlated_pm1(Z_true, sim_payload)\n",
        "#         L_lm[0]  = correlated_pm1(H_true, sim_lm)\n",
        "#         # confondeurs\n",
        "#         for i in range(1, V):\n",
        "#             L_mem[i] = correlated_pm1(Z_true, rho_mem_conf)\n",
        "#             L_lm[i]  = correlated_pm1(H_true, rho_lm_conf)\n",
        "#         return L_mem, L_lm, 0, Z_true, H_true  # (lexiques, true_id, payload, LM)\n",
        "\n",
        "#     def vote_scores_two_lex(L_mem: np.ndarray, L_lm: np.ndarray,\n",
        "#                             Z_hat: np.ndarray, H_LM: np.ndarray, lam: float) -> np.ndarray:\n",
        "#         # int32 pour éviter overflow ; (V,D) @ (D,) -> (V,)\n",
        "#         return (L_mem.astype(np.int32) @ Z_hat.astype(np.int32)) + \\\n",
        "#                lam * (L_lm.astype(np.int32)  @ H_LM.astype(np.int32))\n",
        "\n",
        "#     def _softmax_probs(scores: np.ndarray, D: int, tau: float = 1.0) -> np.ndarray:\n",
        "#         s = scores / (np.sqrt(D) * max(tau, 1e-6))\n",
        "#         s = s - np.max(s)\n",
        "#         exps = np.exp(s)\n",
        "#         return exps / np.sum(exps)\n",
        "\n",
        "#     def hd_perplexity(scores: np.ndarray, true_idx: int, D: int, tau: float = 1.0) -> float:\n",
        "#         p_true = float(_softmax_probs(scores, D=D, tau=tau)[true_idx])\n",
        "#         return float(np.exp(-np.log(max(p_true, 1e-12))))\n",
        "\n",
        "#     stats = {lam: {\"top1_hits\": 0, \"ppl_sum\": 0.0} for lam in lam_grid}\n",
        "\n",
        "#     for _ in range(trials):\n",
        "#         L_mem, L_lm, true_idx, Z_true, H_true = make_trial()\n",
        "#         Z_hat = Z_true; H_LM = H_true\n",
        "#         for lam in lam_grid:\n",
        "#             scores = vote_scores_two_lex(L_mem, L_lm, Z_hat, H_LM, float(lam))\n",
        "#             pred = int(np.argmax(scores))\n",
        "#             stats[lam][\"top1_hits\"] += 1 if pred == true_idx else 0\n",
        "#             stats[lam][\"ppl_sum\"]   += hd_perplexity(scores, true_idx, D, tau)\n",
        "\n",
        "#     results = {lam: {\"top1\": stats[lam][\"top1_hits\"]/trials,\n",
        "#                      \"ppl\":  stats[lam][\"ppl_sum\"]/trials}\n",
        "#                for lam in lam_grid}\n",
        "\n",
        "#     base_top1, base_ppl = results[0.0][\"top1\"], results[0.0][\"ppl\"]\n",
        "#     saturated = (abs(base_top1 - 1.0) < 1e-12)\n",
        "\n",
        "#     log.info((\"DX6(2-spaces|fixed) — D=%d, trials=%d, conf=%d, ρ_mem(conf)=%.2f, ρ_lm(conf)=%.2f, \"\n",
        "#               \"sim_payload=%.2f, sim_lm=%.2f\"),\n",
        "#               D, trials, n_confounders, rho_mem_conf, rho_lm_conf, sim_payload, sim_lm)\n",
        "#     for lam in lam_grid:\n",
        "#         log.info(\"  lambda=%.2f  ->  top-1=%.3f | ppl=%.3f\", lam, results[lam][\"top1\"], results[lam][\"ppl\"])\n",
        "\n",
        "#     if saturated:\n",
        "#         ok = any(results[lam][\"ppl\"] < base_ppl - 1e-12 for lam in lam_grid if lam != 0.0)\n",
        "#         assert ok, \"DX6(fixed): régime saturé — aucune baisse de perplexité vs λ=0.\"\n",
        "#     else:\n",
        "#         ok = any((results[lam][\"top1\"] > base_top1 + 1e-12) and (results[lam][\"ppl\"] < base_ppl - 1e-12)\n",
        "#                  for lam in lam_grid if lam != 0.0)\n",
        "#         assert ok, \"DX6(fixed): aucun λ n'améliore simultanément top-1 ET perplexité vs λ=0.\"\n",
        "\n",
        "#     log.info(\"DX6(fixed) — CA VALIDÉ (%s).\", \"saturé\" if saturated else \"non-saturé\")\n",
        "#     return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "02b7d9f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def hd_assert_pm1(x: np.ndarray, D: int | None = None) -> None:\n",
        "    assert x.dtype == np.int8 and np.all((x == 1) | (x == -1)), \"Vecteur non binaire ±1/int8\"\n",
        "    if D is not None:\n",
        "        assert x.ndim == 1 and x.shape[0] == D, \"Dimension inattendue\"\n",
        "\n",
        "def _softmax_probs(scores: np.ndarray, D: int, tau: float = 1.0) -> np.ndarray:\n",
        "    s = scores / (np.sqrt(D) * max(float(tau), 1e-6))\n",
        "    s = s - np.max(s)\n",
        "    exps = np.exp(s, dtype=np.float64)\n",
        "    return exps / np.sum(exps, dtype=np.float64)\n",
        "\n",
        "def hd_perplexity_from_scores(scores: np.ndarray, true_idx: int, D: int, tau: float = 1.0) -> float:\n",
        "    p_true = float(_softmax_probs(scores, D=D, tau=tau)[true_idx])\n",
        "    return float(np.exp(-np.log(max(p_true, 1e-12))))\n",
        "\n",
        "# --- Génération contrôlée: on impose une similarité cible ~ rho par flips coordonnés  ----\n",
        "def flip_to_target(vec: np.ndarray, target_sim: float, rng: np.random.Generator) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Pour ±1, si p_flip = (1 - target_sim)/2 alors E[sim] = 1 - 2*p_flip = target_sim.\n",
        "    \"\"\"\n",
        "    D = vec.shape[0]\n",
        "    p_flip = max(0.0, min(1.0, (1.0 - float(target_sim)) / 2.0))\n",
        "    mask  = (rng.random(D) < p_flip).astype(np.int8)  # 1 si on flippe\n",
        "    flips = (1 - 2 * mask).astype(np.int8, copy=False)  # 1->-1 quand mask=1\n",
        "    return (vec.astype(np.int8, copy=False) * flips).astype(np.int8, copy=False)\n",
        "\n",
        "# --- DD6_vote (version vectorisée, 2 espaces) -----------------------------------------\n",
        "def _batch_lex(cand_vocab, L):\n",
        "    mats = []\n",
        "    for v in cand_vocab:\n",
        "        vec = L(v).astype(np.int8, copy=False)\n",
        "        mats.append(vec)\n",
        "    return np.vstack(mats).astype(np.int8, copy=False)\n",
        "\n",
        "def DD6_vote(\n",
        "    Z_hat: np.ndarray,\n",
        "    H_LM: np.ndarray,\n",
        "    L_mem,                  # callable: v -> ±1 int8 (D,)\n",
        "    L_lm,                   # callable: v -> ±1 int8 (D,)\n",
        "    cand_vocab: list[str],\n",
        "    lam: float = 0.0,\n",
        "    *,\n",
        "    normalize: str = \"sqrtD\",   # {\"none\",\"sqrtD\"}\n",
        "    return_probs: bool = False,\n",
        "    tau: float = 1.0\n",
        ") -> tuple[str, np.ndarray, np.ndarray | None]:\n",
        "    D = int(Z_hat.shape[0])\n",
        "    hd_assert_pm1(Z_hat, D); hd_assert_pm1(H_LM, D)\n",
        "    assert isinstance(cand_vocab, (list, tuple)) and len(cand_vocab) > 0, \"cand_vocab vide\"\n",
        "    M_mem = _batch_lex(cand_vocab, L_mem)   # (V, D)\n",
        "    M_lm  = _batch_lex(cand_vocab, L_lm)    # (V, D)\n",
        "    assert M_mem.shape == M_lm.shape == (len(cand_vocab), D), \"Shapes (V,D) incohérents\"\n",
        "\n",
        "    z32 = Z_hat.astype(np.int32, copy=False)\n",
        "    h32 = H_LM.astype(np.int32, copy=False)\n",
        "    scores_raw = (M_mem.astype(np.int32, copy=False) @ z32).astype(np.float64) \\\n",
        "               + float(lam) * (M_lm.astype(np.int32, copy=False) @ h32).astype(np.float64)\n",
        "\n",
        "    best_idx   = int(np.argmax(scores_raw))\n",
        "    token_star = cand_vocab[best_idx]\n",
        "\n",
        "    probs = None\n",
        "    if return_probs:\n",
        "        if normalize == \"sqrtD\":\n",
        "            logits = scores_raw / (np.sqrt(D) * max(1e-6, float(tau)))\n",
        "        elif normalize == \"none\":\n",
        "            logits = scores_raw / max(1e-6, float(tau))\n",
        "        else:\n",
        "            raise ValueError(\"normalize ∈ {'none','sqrtD'}\")\n",
        "        logits = logits - np.max(logits)\n",
        "        exps   = np.exp(logits, dtype=np.float64)\n",
        "        probs  = (exps / np.sum(exps, dtype=np.float64)).astype(np.float64, copy=False)\n",
        "\n",
        "    return token_star, scores_raw, probs\n",
        "\n",
        "# --- DX6_run: simulation 2-espaces + mesure top-1 & perplexité -----------------------\n",
        "def DX6_run(\n",
        "    D: int = 16384, trials: int = 400,\n",
        "    lam_grid=(0.0, 0.5, 1.0),\n",
        "    # corrélations du vrai token:\n",
        "    sim_payload: float = 0.60,   # corr(Z_hat, L_mem(y))  — plus bas pour éviter saturation\n",
        "    sim_lm: float      = 0.40,   # corr(H_LM, L_lm(y))\n",
        "    # confondeurs:\n",
        "    n_confounders: int = 6,\n",
        "    rho_mem_conf: float = 0.55,  # corr(Z_hat, L_mem(conf)) < sim_payload mais proche\n",
        "    rho_lm_conf: float  = 0.10,  # corr(H_LM, L_lm(conf))  << sim_lm\n",
        "    tau: float = 1.0,\n",
        "    rng_seed: int = 7031\n",
        "):\n",
        "    \"\"\"\n",
        "    Évalue DD6_vote avec deux lexiques indépendants (mémoire & LM).\n",
        "    - Régime par défaut: NON SATURÉ (sim_payload ~ 0.60, conf proche 0.55).\n",
        "    Critère:\n",
        "      - Si top-1(λ=0) < 1.0 (non saturé): ∃ λ>0 tel que top-1 ↑ ET perplexité ↓.\n",
        "      - Sinon (saturé): ∃ λ>0 tel que perplexité ↓.\n",
        "    \"\"\"\n",
        "    g = np.random.default_rng(rng_seed)\n",
        "\n",
        "    def rademacher(D: int) -> np.ndarray:\n",
        "        return g.choice(np.array([-1, 1], dtype=np.int8), size=D)\n",
        "\n",
        "    stats = {lam: {\"top1_hits\": 0, \"ppl_sum\": 0.0} for lam in lam_grid}\n",
        "\n",
        "    for _ in range(trials):\n",
        "        # Prototypes vrais\n",
        "        Z_true = rademacher(D)   # payload seuillé\n",
        "        H_true = rademacher(D)   # LM courant\n",
        "\n",
        "        # Vocabulaire (strings) : y + confondeurs\n",
        "        V = n_confounders + 1\n",
        "        cand_vocab = [f\"tok{i}\" for i in range(V)]\n",
        "        true_tok   = cand_vocab[0]\n",
        "\n",
        "        # Construit des tables (dictionnaires) pour L_mem et L_lm\n",
        "        table_mem: dict[str, np.ndarray] = {}\n",
        "        table_lm:  dict[str, np.ndarray] = {}\n",
        "        # Vrai token\n",
        "        table_mem[true_tok] = flip_to_target(Z_true, sim_payload, g)\n",
        "        table_lm[true_tok]  = flip_to_target(H_true, sim_lm,      g)\n",
        "        # Conf:\n",
        "        for i in range(1, V):\n",
        "            ti = cand_vocab[i]\n",
        "            table_mem[ti] = flip_to_target(Z_true, rho_mem_conf, g)\n",
        "            table_lm[ti]  = flip_to_target(H_true, rho_lm_conf,  g)\n",
        "\n",
        "        # Callables lexicaux pour DD6_vote\n",
        "        def L_mem(v: str) -> np.ndarray: return table_mem[v]\n",
        "        def L_lm(v: str)  -> np.ndarray: return table_lm[v]\n",
        "\n",
        "        # Vote pour chaque lambda\n",
        "        for lam in lam_grid:\n",
        "            token_star, scores, probs = DD6_vote(\n",
        "                Z_hat=Z_true, H_LM=H_true,\n",
        "                L_mem=L_mem, L_lm=L_lm,\n",
        "                cand_vocab=cand_vocab,\n",
        "                lam=float(lam),\n",
        "                normalize=\"sqrtD\", return_probs=True, tau=tau\n",
        "            )\n",
        "            pred_is_true = 1 if token_star == true_tok else 0\n",
        "            stats[lam][\"top1_hits\"] += pred_is_true\n",
        "\n",
        "            # Perplexité HD (si probs non None, on l'utilise directement)\n",
        "            if probs is not None:\n",
        "                true_idx = 0\n",
        "                p_true = float(max(probs[true_idx], 1e-12))\n",
        "                ppl = float(np.exp(-np.log(p_true)))\n",
        "            else:\n",
        "                ppl = hd_perplexity_from_scores(scores, true_idx=0, D=D, tau=tau)\n",
        "            stats[lam][\"ppl_sum\"] += ppl\n",
        "\n",
        "    results = {\n",
        "        lam: {\"top1\": stats[lam][\"top1_hits\"]/trials,\n",
        "              \"ppl\":  stats[lam][\"ppl_sum\"]/trials}\n",
        "        for lam in lam_grid\n",
        "    }\n",
        "\n",
        "    # Logging des résultats\n",
        "    log.info((\"DX6 — D=%d, trials=%d, conf=%d, \"\n",
        "              \"ρ_mem(conf)=%.2f, ρ_lm(conf)=%.2f, sim_payload=%.2f, sim_lm=%.2f\"),\n",
        "             D, trials, n_confounders, rho_mem_conf, rho_lm_conf, sim_payload, sim_lm)\n",
        "    for lam in lam_grid:\n",
        "        log.info(\"  lambda=%.2f  ->  top-1=%.3f | ppl=%.3f\",\n",
        "                 float(lam), results[lam][\"top1\"], results[lam][\"ppl\"])\n",
        "\n",
        "    # Critère d'acceptation (bi-régime)\n",
        "    base_top1, base_ppl = results[0.0][\"top1\"], results[0.0][\"ppl\"]\n",
        "    saturated = (abs(base_top1 - 1.0) < 1e-12)\n",
        "    if saturated:\n",
        "        ok = any(results[lam][\"ppl\"] < base_ppl - 1e-12 for lam in lam_grid if lam != 0.0)\n",
        "        assert ok, \"DX6: régime saturé — aucune baisse de perplexité vs λ=0.\"\n",
        "    else:\n",
        "        ok = any((results[lam][\"top1\"] > base_top1 + 1e-12) and\n",
        "                 (results[lam][\"ppl\"] < base_ppl - 1e-12)\n",
        "                 for lam in lam_grid if lam != 0.0)\n",
        "        assert ok, \"DX6: aucun λ n'améliore simultanément top-1 ET perplexité vs λ=0.\"\n",
        "    log.info(\"DX6 — CA VALIDÉ (%s).\", \"saturé\" if saturated else \"non-saturé\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "40afdb2a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-06 23:00:52,893 [INFO] DX6 — D=16384, trials=400, conf=6, ρ_mem(conf)=0.55, ρ_lm(conf)=0.10, sim_payload=0.60, sim_lm=0.40\n",
            "2025-10-06 23:00:52,893 [INFO]   lambda=0.00  ->  top-1=1.000 | ppl=1.019\n",
            "2025-10-06 23:00:52,894 [INFO]   lambda=0.50  ->  top-1=1.000 | ppl=1.000\n",
            "2025-10-06 23:00:52,894 [INFO]   lambda=1.00  ->  top-1=1.000 | ppl=1.000\n",
            "2025-10-06 23:00:52,894 [INFO] DX6 — CA VALIDÉ (saturé).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{0.0: {'top1': 1.0, 'ppl': 1.0192223749961116},\n",
              " 0.5: {'top1': 1.0, 'ppl': 1.0000000001159797},\n",
              " 1.0: {'top1': 1.0, 'ppl': 1.0}}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DX6_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a822a58a",
      "metadata": {},
      "source": [
        "# DD7 . "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "918aab48",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -- Hypers sûrs par défaut -----------------------------------------------------\n",
        "DEFAULT_ELL_GRID = (2, 4, 8, 12)\n",
        "CONF_PER_STEP    = 8          # nb. de confondeurs par pas t\n",
        "TRIALS           = 200        # nb. de séquences indépendantes (moyennage)\n",
        "T_STEPS          = 24         # longueur d'une séquence\n",
        "SIM_Y_MEM        = 0.70       # corr(H_true(ell), L_fr(y_t)) attendue (oracle)\n",
        "SIM_CONF_LM      = 0.05       # confondeurs faiblement corrélés au LM\n",
        "D                = 16_384     # dimension HD (isométrie stable)\n",
        "RNG_SEED         = 9_117\n",
        "\n",
        "# -- Utilitaires HDC (contrats déjà définis ailleurs) ---------------------------\n",
        "def rademacher(D, rng): \n",
        "    return rng.choice(np.array([-1,1], dtype=np.int8), size=D)\n",
        "\n",
        "def correlated_pm1(proto: np.ndarray, rho: float, rng: np.random.Generator) -> np.ndarray:\n",
        "    \"\"\"Retourne ±1 corrélé à 'proto' avec corrélation ~rho (approx. en grande D).\"\"\"\n",
        "    noise = rademacher(proto.shape[0], rng)\n",
        "    mix = rho * proto.astype(np.int32) + (1 - rho) * noise.astype(np.int32)\n",
        "    return np.where(mix >= 0, 1, -1).astype(np.int8)\n",
        "\n",
        "def hd_sim(x: np.ndarray, y: np.ndarray) -> float:\n",
        "    return float((x.astype(np.int32) @ y.astype(np.int32)) / x.shape[0])\n",
        "\n",
        "def sign_strict_pm1(x: np.ndarray) -> np.ndarray:\n",
        "    y = (x >= 0).astype(np.int8, copy=False)\n",
        "    return ((y << 1) - 1).astype(np.int8, copy=False)\n",
        "\n",
        "def permute_pow(x: np.ndarray, pi: np.ndarray, power: int) -> np.ndarray:\n",
        "    idx = np.arange(x.shape[0], dtype=np.int64)\n",
        "    p = power % x.shape[0]\n",
        "    for _ in range(p):\n",
        "        idx = pi[idx]\n",
        "    return x[idx]\n",
        "\n",
        "def DD7_updateLM(H_LM: np.ndarray, v_hat: str, L_fr, Pi: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"H_LM' = sign( H_LM + Π^1 L_fr(v_hat) ) avec sign strict (0->+1).\"\"\"\n",
        "    D = H_LM.shape[0]\n",
        "    Lv = L_fr(v_hat).astype(np.int8, copy=False)\n",
        "    inc = permute_pow(Lv, Pi, 1).astype(np.int16, copy=False)\n",
        "    acc = H_LM.astype(np.int16) + inc\n",
        "    return sign_strict_pm1(acc)\n",
        "\n",
        "# -- Lexique jouet déterministe (vocab préfixé par indices) ---------------------\n",
        "class ToyLexFR:\n",
        "    def __init__(self, vocab: list[str], D: int, seed: int = 1234):\n",
        "        self.vocab = vocab; self.D = D\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.table = {v: self.rng.choice(np.array([-1, 1], dtype=np.int8), size=D) for v in vocab}\n",
        "    def __call__(self, v: str) -> np.ndarray:\n",
        "        return self.table[v]\n",
        "\n",
        "# -- Génération d'une séquence et évaluation pour un ell donné ------------------\n",
        "def DX7_eval_one_ell(ell: int, Pi: np.ndarray, L_fr, rng: np.random.Generator) -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Retourne (top1, p_ell) moyens sur TRIALS x T_STEPS avec confondeurs.\n",
        "    - On simule le 'champ LM oracle' H_true = sign(sum_{j=1..ell} Pi^j L_fr(y_{t-j})).\n",
        "    - Le vrai token y_t a un vecteur lexical corrélé à H_true via SIM_Y_MEM.\n",
        "    - Les confondeurs sont faiblement corrélés au LM (SIM_CONF_LM).\n",
        "    - On fait un vote purement LM: argmax_v <H_LM, L_fr(v)> où H_LM est mis à jour par DD7.\n",
        "    \"\"\"\n",
        "    top1_hits = 0\n",
        "    p_sum     = 0.0\n",
        "    D = Pi.shape[0]\n",
        "\n",
        "    # Vocabulaire jouet : on instancie des étiquettes nominales, le contenu est dans L_fr\n",
        "    vocab = [f\"tok_{i}\" for i in range(CONF_PER_STEP + 1)]\n",
        "    # Pour stabiliser across trials, on réutilise le même lexique symbolique\n",
        "    Lsym = ToyLexFR(vocab=vocab, D=D, seed=int(rng.integers(1, 2**31 - 1)))\n",
        "\n",
        "    for _ in range(TRIALS):\n",
        "        # historique vrai (buffer des derniers tokens) et états LM\n",
        "        hist_true = []  # liste de tokens y_{t-j}\n",
        "        H_LM_pred = rademacher(D, rng)  # état initial quelconque\n",
        "        for t in range(T_STEPS):\n",
        "            # 1) Champ LM oracle à partir des 'ell' derniers vrais tokens\n",
        "            if len(hist_true) < ell:\n",
        "                # bootstrap: champ ~ bruit\n",
        "                H_true = rademacher(D, rng)\n",
        "            else:\n",
        "                acc = np.zeros(D, dtype=np.int32)\n",
        "                for j in range(1, ell + 1):\n",
        "                    acc += permute_pow(Lsym(hist_true[-j]), Pi, j).astype(np.int32)\n",
        "                H_true = sign_strict_pm1(acc)\n",
        "\n",
        "            # 2) Générer le vrai token y_t: vecteur lexical corrélé au champ oracle\n",
        "            y = vocab[0]  # on fixe l'étiquette de la vérité au premier candidat\n",
        "            L_y = correlated_pm1(H_true, SIM_Y_MEM, rng)  # \"vrai\" vecteur lexical (corr. au champ)\n",
        "\n",
        "            # 3) Générer confondeurs faiblement corrélés au même champ\n",
        "            cand_vectors = [L_y]\n",
        "            cand_tokens  = [y]\n",
        "            for k in range(CONF_PER_STEP):\n",
        "                v = vocab[k + 1]\n",
        "                L_v = correlated_pm1(H_true, SIM_CONF_LM, rng)\n",
        "                cand_vectors.append(L_v); cand_tokens.append(v)\n",
        "            cand_vectors = np.stack(cand_vectors, axis=0)   # shape (1+conf, D)\n",
        "\n",
        "            # 4) Vote purement LM (isoler l'effet de DD7) : argmax <H_LM_pred, L_fr(v)>\n",
        "            #    Ici L_fr(v) := cand_vectors[i] au pas courant.\n",
        "            scores = cand_vectors.astype(np.int32) @ H_LM_pred.astype(np.int32)\n",
        "            pred_idx = int(np.argmax(scores))\n",
        "            v_hat = cand_tokens[pred_idx]\n",
        "            top1_hits += 1 if pred_idx == 0 else 0\n",
        "\n",
        "            # 5) Mettre à jour 'oracle-biais' p(ell) à partir de H_true vs L_y\n",
        "            sim = hd_sim(H_true, L_y)\n",
        "            p_sum += 0.5 * (1.0 + sim)  # probabilité alignement coordonné\n",
        "\n",
        "            # 6) Mise à jour de l'historique vrai et du LM prédictif (DD7 sur prédiction)\n",
        "            hist_true.append(y)\n",
        "            if len(hist_true) > ell:  # garder une fenêtre bornée pour la simulation de H_true\n",
        "                hist_true.pop(0)\n",
        "            # mappe temporaire pour fournir L_fr(v_hat) = vecteur choisi ci-dessus\n",
        "            L_fr_temp = lambda token: cand_vectors[cand_tokens.index(token)]\n",
        "            H_LM_pred = DD7_updateLM(H_LM_pred, v_hat=v_hat, L_fr=L_fr_temp, Pi=Pi)\n",
        "\n",
        "    total = TRIALS * T_STEPS\n",
        "    top1 = top1_hits / total\n",
        "    p_ell = p_sum / total\n",
        "    return top1, p_ell\n",
        "\n",
        "# -- Expérience principale DX7 --------------------------------------------------\n",
        "def DX7_run(\n",
        "    ell_grid=DEFAULT_ELL_GRID,\n",
        "    D: int = D,\n",
        "    seed_pi: int = 10_456,\n",
        "    rng_seed: int = RNG_SEED\n",
        "):\n",
        "    rng = np.random.default_rng(rng_seed)\n",
        "    # plan de permutation (arbitraire reproductible)\n",
        "    Pi = np.arange(D, dtype=np.int64)\n",
        "    rng.shuffle(Pi)\n",
        "    results = {}\n",
        "    log.info(\"DX7 — étude fenetre ell=%s (D=%d, trials=%d, T=%d, conf/step=%d)\",\n",
        "             ell_grid, D, TRIALS, T_STEPS, CONF_PER_STEP)\n",
        "    for ell in ell_grid:\n",
        "        top1, p_ell = DX7_eval_one_ell(ell=ell, Pi=Pi, L_fr=None, rng=rng)\n",
        "        results[int(ell)] = {\"top1\": top1, \"p\": p_ell}\n",
        "        log.info(\"  ell=%2d  ->  top-1=%.3f | p(ell)=%.3f\", ell, top1, p_ell)\n",
        "\n",
        "    # -- CA: existence d'un ell* max top-1 et décroissance de p(ell) au-delà ----\n",
        "    ells = sorted(results.keys())\n",
        "    top1s = np.array([results[e][\"top1\"] for e in ells], dtype=np.float64)\n",
        "    ps    = np.array([results[e][\"p\"]    for e in ells], dtype=np.float64)\n",
        "\n",
        "    ell_star = ells[int(np.argmax(top1s))]\n",
        "    # monotonie faible de p au-delà de ell*\n",
        "    tail = ps[ells.index(ell_star):]\n",
        "    nonincreasing_tail = np.all(tail[:-1] >= tail[1:] - 1e-9)\n",
        "\n",
        "    assert nonincreasing_tail, \"DX7: p(ell) ne décroît pas au-delà de ell* (dilution attendue de la majorité).\"\n",
        "    log.info(\"DX7 — CA VALIDÉS: (i) ell*=%d maximise top-1 ; (ii) p(ell) décroît au-delà.\", ell_star)\n",
        "    return results, ell_star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "a20a3fa7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-06 23:23:12,336 [INFO] DX7 — étude fenetre ell=(2, 4, 8, 12) (D=16384, trials=200, T=24, conf/step=8)\n",
            "2025-10-06 23:23:19,330 [INFO]   ell= 2  ->  top-1=0.926 | p(ell)=1.000\n",
            "2025-10-06 23:23:26,541 [INFO]   ell= 4  ->  top-1=0.854 | p(ell)=1.000\n",
            "2025-10-06 23:23:34,829 [INFO]   ell= 8  ->  top-1=0.707 | p(ell)=1.000\n",
            "2025-10-06 23:23:43,466 [INFO]   ell=12  ->  top-1=0.555 | p(ell)=1.000\n",
            "2025-10-06 23:23:43,467 [INFO] DX7 — CA VALIDÉS: (i) ell*=2 maximise top-1 ; (ii) p(ell) décroît au-delà.\n"
          ]
        }
      ],
      "source": [
        "results, ell_star = DX7_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "70cf6789",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Callable, Optional, Union\n",
        "\n",
        "def _as_vocab_from_buckets(\n",
        "    C_K: np.ndarray,\n",
        "    bucket2vocab: Optional[Union[dict[int, list[str]], Callable[[int], list[str]]]],\n",
        "    history_fr: list[str],\n",
        "    global_fallback_vocab: Optional[list[str]],\n",
        "    min_size: int = 1\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Construit un vocab candidat à partir des indices de buckets C_K, avec repli sur:\n",
        "    - historique (pour ne pas renvoyer vide),\n",
        "    - vocab global si fourni.\n",
        "    Déduplique en conservant l'ordre (top-K prioritaire).\n",
        "    \"\"\"\n",
        "    cand: list[str] = []\n",
        "    seen = set()\n",
        "    def add_many(lst: list[str]):\n",
        "        for t in lst:\n",
        "            if t not in seen:\n",
        "                seen.add(t); cand.append(t)\n",
        "\n",
        "    if bucket2vocab is not None:\n",
        "        for c in C_K:\n",
        "            toks = bucket2vocab(c) if callable(bucket2vocab) else bucket2vocab.get(int(c), [])\n",
        "            if toks: add_many(toks)\n",
        "\n",
        "    # Repli sur l'historique si nécessaire\n",
        "    if len(cand) < min_size and history_fr:\n",
        "        add_many(list(history_fr))\n",
        "\n",
        "    # Repli global si encore vide\n",
        "    if len(cand) < min_size and global_fallback_vocab is not None:\n",
        "        add_many(list(global_fallback_vocab))\n",
        "\n",
        "    # Dernier garde-fou\n",
        "    if len(cand) < min_size:\n",
        "        cand = [\"<unk>\"]\n",
        "    return cand\n",
        "\n",
        "def DecodeOneStep(\n",
        "    Hs: np.ndarray,                # H^(s) (±1)^D, int8\n",
        "    H_LM: np.ndarray,              # état LM (±1)^D, int8\n",
        "    history_fr: list[str],         # derniers tokens FR prédits\n",
        "    G_DEC: np.ndarray,             # clé décodage (±1)^D, int8\n",
        "    G_MEM: np.ndarray,             # clé mémoire   (±1)^D, int8\n",
        "    Pi: np.ndarray,                # permutation D\n",
        "    L_fr,                          # callable: str -> (±1)^D, int8\n",
        "    prototypes: np.ndarray,        # (B, D), int8 (seuillés) ou int16/int32 (non seuillés)\n",
        "    K: int = 500,\n",
        "    alpha: float = 1.0,\n",
        "    beta:  float = 1.0,\n",
        "    ell: int = 4,\n",
        "    lam: float = 0.5,\n",
        "    # -- nouvelles options pour les candidats lexicaux --\n",
        "    bucket2vocab: Optional[Union[dict[int, list[str]], Callable[[int], list[str]]]] = None,\n",
        "    global_fallback_vocab: Optional[list[str]] = None,\n",
        "    # -- compat et profiling --\n",
        "    return_ck_scores: bool = True\n",
        ") -> tuple[str, np.ndarray, int, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Retourne: (token*, scores_cand, c_star, CK_indices, CK_scores).\n",
        "\n",
        "    Pipeline:\n",
        "      DD1: Qs = DD1_ctx(Hs, G_DEC)\n",
        "      DD2: Rt = DD2_query(Qs, history_fr, L_fr, Pi, alpha, beta, ell)\n",
        "      DD3: Rt_tilde = DD3_bindToMem(Rt, G_MEM)\n",
        "      DD4: (c_star, C_K, scores_CK) = DD4_search_topK(Rt_tilde, prototypes, K)\n",
        "      DD5: Z_hat = DD5_payload(prototypes[c_star])\n",
        "      DD6: (token*, scores_cand) = DD6_vote(Z_hat, H_LM, L_fr, cand_vocab, lam)\n",
        "      DD7: H_LM_next = DD7_updateLM(H_LM, token*, L_fr, Pi)\n",
        "\n",
        "    Contrats:\n",
        "      - Tous les vecteurs binaires sont int8 dans {±1}.\n",
        "      - Les produits scalaires sont accumulés en int32 pour stabilité.\n",
        "    \"\"\"\n",
        "    # --- Contrats d'entrée de base -------------------------------------------\n",
        "    D = Hs.shape[0]\n",
        "    hd_assert_pm1(Hs, D); hd_assert_pm1(H_LM, D)\n",
        "    hd_assert_pm1(G_DEC, D); hd_assert_pm1(G_MEM, D)\n",
        "    assert Pi.ndim == 1 and Pi.shape[0] == D and np.issubdtype(Pi.dtype, np.integer), \"Pi invalide\"\n",
        "    assert prototypes.ndim == 2 and prototypes.shape[1] == D, \"prototypes de forme (B,D)\"\n",
        "\n",
        "    # --- DD1: binding décodage (isométrie) -----------------------------------\n",
        "    Qs = DD1_ctx(Hs, G_DEC)  # int8 ±1\n",
        "\n",
        "    # --- DD2: requête mixte source + historique ------------------------------\n",
        "    Rt = DD2_query(Qs, history_fr, L_fr, Pi, alpha=alpha, beta=beta, ell=ell)  # int8 ±1\n",
        "\n",
        "    # --- DD3: binding vers la mémoire (isométrie) ----------------------------\n",
        "    Rt_tilde = DD3_bindToMem(Rt, G_MEM)  # int8 ±1\n",
        "\n",
        "    # --- DD4: recherche top-K dans la banque mémoire -------------------------\n",
        "    c_star, C_K, scores_CK = DD4_search_topK(Rt_tilde, prototypes, K)\n",
        "\n",
        "    # --- DD5: seuillage robuste du prototype gagnant -------------------------\n",
        "    Z_hat = DD5_payload(prototypes[c_star])  # int8 ±1\n",
        "\n",
        "    # --- Candidats lexicaux depuis les buckets top-K (+ repli) ---------------\n",
        "    cand_vocab = _as_vocab_from_buckets(\n",
        "        C_K=C_K,\n",
        "        bucket2vocab=bucket2vocab,\n",
        "        history_fr=history_fr,\n",
        "        global_fallback_vocab=global_fallback_vocab,\n",
        "        min_size=1\n",
        "    )\n",
        "\n",
        "    # --- DD6: vote lexical (payload + LM) ------------------------------------\n",
        "    token_star, scores_cand = DD6_vote(Z_hat, H_LM, L_fr, cand_vocab, lam=lam)\n",
        "\n",
        "    # --- DD7: mise à jour de l'état LM ---------------------------------------\n",
        "    H_LM_next = DD7_updateLM(H_LM, token_star, L_fr, Pi)\n",
        "\n",
        "    # --- Retour (inclut scores top-K pour profilage) -------------------------\n",
        "    if return_ck_scores:\n",
        "        return token_star, scores_cand, int(c_star), C_K, scores_CK\n",
        "    else:\n",
        "        # Compat: certaines versions n'attendent pas CK_scores\n",
        "        return token_star, scores_cand, int(c_star), C_K, H_LM_next  # (déconseillé)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "a0a8cb5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def mock_L_fr(vocab_seed: int, D: int):\n",
        "    rng = np.random.default_rng(vocab_seed)\n",
        "    table = {}\n",
        "    def get(tok: str) -> np.ndarray:\n",
        "        if tok not in table:\n",
        "            x = rng.integers(0, 2, size=D, dtype=np.int8)\n",
        "            table[tok] = (2*x - 1).astype(np.int8)\n",
        "        return table[tok]\n",
        "    return get\n",
        "\n",
        "def test_isometry_and_flow():\n",
        "    D = 16384; K = 128\n",
        "    rng = np.random.default_rng(7)\n",
        "    Hs = (2*rng.integers(0,2,size=D,dtype=np.int8)-1)\n",
        "    H_LM = (2*rng.integers(0,2,size=D,dtype=np.int8)-1)\n",
        "    G_DEC = (2*rng.integers(0,2,size=D,dtype=np.int8)-1)\n",
        "    G_MEM = (2*rng.integers(0,2,size=D,dtype=np.int8)-1)\n",
        "    Pi = rng.permutation(D).astype(np.int64)\n",
        "    Lfr = mock_L_fr(1234, D)\n",
        "    # prototypes jouets (B x D)\n",
        "    B = 2048\n",
        "    prototypes = (2*rng.integers(0,2,size=(B,D),dtype=np.int8)-1)\n",
        "    # Décodage d'un pas\n",
        "    tok, scores, c_star, CK, H_LM_next = DecodeOneStep(\n",
        "        Hs, H_LM, history_fr=[\"de\",\"la\",\"musique\"], \n",
        "        G_DEC=G_DEC, G_MEM=G_MEM, Pi=Pi, L_fr=Lfr, prototypes=prototypes, K=K\n",
        "    )\n",
        "    assert isinstance(tok, str) and scores.ndim == 1\n",
        "    assert H_LM_next.shape == (D,) and H_LM_next.dtype == np.int8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "6514e310",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Callable, Optional, Union, Tuple\n",
        "\n",
        "# --- utilitaires déjà définis ailleurs (rappel minimal) -----------------------\n",
        "def hd_assert_pm1(x: np.ndarray, D: int | None = None) -> None:\n",
        "    assert isinstance(x, np.ndarray)\n",
        "    assert x.dtype == np.int8, \"attendu int8\"\n",
        "    assert np.all((x == 1) | (x == -1)), \"attendu valeurs ±1\"\n",
        "    if D is not None:\n",
        "        assert x.ndim == 1 and x.shape[0] == D\n",
        "\n",
        "def sign_strict_pm1(x: np.ndarray) -> np.ndarray:\n",
        "    y = (x >= 0).astype(np.int8, copy=False)  # 0/1\n",
        "    return ((y << 1) - 1).astype(np.int8, copy=False)\n",
        "\n",
        "def build_perm_inverse(pi: np.ndarray) -> np.ndarray:\n",
        "    assert pi.ndim == 1 and np.issubdtype(pi.dtype, np.integer)\n",
        "    pi_inv = np.empty_like(pi)\n",
        "    pi_inv[pi] = np.arange(pi.shape[0], dtype=pi.dtype)\n",
        "    return pi_inv\n",
        "\n",
        "def permute_pow_signed(x: np.ndarray, pi: np.ndarray, pi_inv: np.ndarray, k: int) -> np.ndarray:\n",
        "    \"\"\"Applique Π^k pour k∈ℤ en indexant (O(|k|), suffisant pour tests).\"\"\"\n",
        "    D = x.shape[0]\n",
        "    if k == 0:\n",
        "        return x\n",
        "    idx = np.arange(D, dtype=np.int64)\n",
        "    if k > 0:\n",
        "        for _ in range(k % D):\n",
        "            idx = pi[idx]\n",
        "    else:\n",
        "        for _ in range((-k) % D):\n",
        "            idx = pi_inv[idx]\n",
        "    return x[idx]\n",
        "\n",
        "# --- DD1/3/4/5/6/7 supposés définis : on réutilise leurs signatures -----------\n",
        "# DD1_ctx(Hs, G_DEC) -> int8 ±1\n",
        "# DD3_bindToMem(Rt_bin, G_MEM) -> int8 ±1  (contrat: entrée ±1/int8)\n",
        "# DD4_search_topK(Rt_tilde, prototypes, K) -> (c_star, C_K, scores_CK)\n",
        "# DD5_payload(Mc) -> int8 ±1\n",
        "# DD6_vote(Z_hat, H_LM, L_fr, cand_vocab, lam) -> (token*, scores)\n",
        "# DD7_updateLM(H_LM, token_star, L_fr, Pi) -> int8 ±1\n",
        "# _as_vocab_from_buckets(...) -> list[str]\n",
        "\n",
        "# --- DD2 : version \"continue\" (pour DX2 et analyses de norme) -----------------\n",
        "def DD2_query(\n",
        "    Qs: np.ndarray,\n",
        "    hist_vectors: List[np.ndarray],  # liste de L_fr(\\hat v_{t-j}) en ±1/int8\n",
        "    pi: np.ndarray,\n",
        "    *,\n",
        "    alpha: float = 1.0,\n",
        "    beta:  float = 1.0,\n",
        "    ell:   int   = 4\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Version continue (float64) : R_t = α·Qs + β·sign(Σ_{j=1..ell} Π^j L_{t-j}),\n",
        "    puis renvoie un vecteur de norme ≈ √D (utile pour DX2). NON utilisée par DD3.\n",
        "    \"\"\"\n",
        "    D = Qs.shape[0]\n",
        "    hd_assert_pm1(Qs, D)\n",
        "    assert pi.ndim == 1 and pi.shape[0] == D and np.issubdtype(pi.dtype, np.integer)\n",
        "    ell = min(ell, len(hist_vectors))\n",
        "    pi_inv = build_perm_inverse(pi)\n",
        "\n",
        "    if ell == 0:\n",
        "        H_hist = np.ones(D, dtype=np.int8)\n",
        "    else:\n",
        "        acc = np.zeros(D, dtype=np.int16)\n",
        "        for j in range(1, ell+1):\n",
        "            Lj = hist_vectors[j-1]; hd_assert_pm1(Lj, D)\n",
        "            acc += permute_pow_signed(Lj, pi, pi_inv, j).astype(np.int16, copy=False)\n",
        "        H_hist = sign_strict_pm1(acc)\n",
        "\n",
        "    Rt = alpha * Qs.astype(np.float64) + beta * H_hist.astype(np.float64)\n",
        "    nrm = float(np.linalg.norm(Rt))\n",
        "    if nrm > 0:\n",
        "        Rt = Rt / nrm * np.sqrt(D)\n",
        "    else:\n",
        "        Rt = np.ones(D, dtype=np.float64)\n",
        "    return Rt  # float64\n",
        "\n",
        "# --- DD2 : version \"binaire\" (pipeline DEC ; compatible DD3 int8) -------------\n",
        "def DD2_query_bin(\n",
        "    Qs: np.ndarray,\n",
        "    history_fr: List[str],    # liste de tokens FR\n",
        "    L_fr: Callable[[str], np.ndarray],\n",
        "    Pi: np.ndarray,\n",
        "    *,\n",
        "    alpha: float = 1.0,\n",
        "    beta:  float = 1.0,\n",
        "    ell:   int   = 4\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Version binaire : construit H_hist depuis les tokens (via L_fr),\n",
        "    combine α·Qs + β·H_hist puis seuillage strict -> int8 ±1.\n",
        "    Contrat de sortie : ±1/int8 (exigé par DD3_bindToMem).\n",
        "    \"\"\"\n",
        "    D = Qs.shape[0]\n",
        "    hd_assert_pm1(Qs, D)\n",
        "    assert Pi.ndim == 1 and Pi.shape[0] == D and np.issubdtype(Pi.dtype, np.integer)\n",
        "    pi_inv = build_perm_inverse(Pi)\n",
        "\n",
        "    # Convertir l'historique de tokens en vecteurs lexicaux ±1/int8\n",
        "    hist_vecs: List[np.ndarray] = []\n",
        "    for tok in history_fr[:ell]:\n",
        "        Lv = L_fr(tok).astype(np.int8, copy=False)\n",
        "        hd_assert_pm1(Lv, D)\n",
        "        hist_vecs.append(Lv)\n",
        "\n",
        "    if len(hist_vecs) == 0:\n",
        "        H_hist = np.ones(D, dtype=np.int8)\n",
        "    else:\n",
        "        acc = np.zeros(D, dtype=np.int16)\n",
        "        for j, L_j in enumerate(hist_vecs, start=1):\n",
        "            acc += permute_pow_signed(L_j, Pi, pi_inv, j).astype(np.int16, copy=False)\n",
        "        H_hist = sign_strict_pm1(acc)  # ±1/int8\n",
        "\n",
        "    # Combinaison et seuillage final pour rester en ±1\n",
        "    combo = (alpha * Qs.astype(np.int16)) + (beta * H_hist.astype(np.int16))\n",
        "    Rt_bin = sign_strict_pm1(combo.astype(np.int16))\n",
        "    return Rt_bin  # int8 ±1\n",
        "\n",
        "# --- Orchestrateur : DecodeOneStep (corrigé) ----------------------------------\n",
        "from typing import Optional, Union, Tuple\n",
        "\n",
        "def DecodeOneStep(\n",
        "    Hs: np.ndarray,                # H^(s) (±1)^D, int8\n",
        "    H_LM: np.ndarray,              # état LM (±1)^D, int8\n",
        "    history_fr: list[str],         # derniers tokens FR prédits\n",
        "    G_DEC: np.ndarray,             # clé décodage (±1)^D, int8\n",
        "    G_MEM: np.ndarray,             # clé mémoire   (±1)^D, int8\n",
        "    Pi: np.ndarray,                # permutation D\n",
        "    L_fr: Callable[[str], np.ndarray],  # str -> (±1)^D, int8\n",
        "    prototypes: np.ndarray,        # (B, D), int8 (seuillés) ou int16/int32 (non seuillés)\n",
        "    K: int = 500,\n",
        "    alpha: float = 1.0,\n",
        "    beta:  float = 1.0,\n",
        "    ell: int = 4,\n",
        "    lam: float = 0.5,\n",
        "    # -- options candidats --\n",
        "    bucket2vocab: Optional[Union[dict[int, list[str]], Callable[[int], list[str]]]] = None,\n",
        "    global_fallback_vocab: Optional[list[str]] = None,\n",
        "    # -- compat & profiling --\n",
        "    return_ck_scores: bool = True\n",
        ") -> Tuple[str, np.ndarray, int, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Retourne: (token*, scores_cand, c_star, CK_indices, CK_scores/H_LM_next selon option).\n",
        "    NOTE : DD2_query_bin est utilisé (sortie ±1) pour respecter le contrat de DD3.\n",
        "    \"\"\"\n",
        "    # Contrats d'entrée\n",
        "    D = Hs.shape[0]\n",
        "    hd_assert_pm1(Hs, D); hd_assert_pm1(H_LM, D)\n",
        "    hd_assert_pm1(G_DEC, D); hd_assert_pm1(G_MEM, D)\n",
        "    assert Pi.ndim == 1 and Pi.shape[0] == D and np.issubdtype(Pi.dtype, np.integer)\n",
        "    assert prototypes.ndim == 2 and prototypes.shape[1] == D\n",
        "\n",
        "    # (DD1) Binding décodage\n",
        "    Qs = DD1_ctx(Hs, G_DEC)  # ±1/int8\n",
        "\n",
        "    # (DD2) Requête mixte (version binaire compatible DD3)\n",
        "    Rt = DD2_query_bin(Qs, history_fr, L_fr, Pi, alpha=alpha, beta=beta, ell=ell)  # ±1/int8\n",
        "\n",
        "    # (DD3) Binding vers mémoire\n",
        "    Rt_tilde = DD3_bindToMem(Rt, G_MEM)  # ±1/int8\n",
        "\n",
        "    # (DD4) Recherche top-K\n",
        "    c_star, C_K, scores_CK = DD4_search_topK(Rt_tilde, prototypes, K)\n",
        "\n",
        "    # (DD5) Seuillage du prototype gagnant\n",
        "    Z_hat = DD5_payload(prototypes[c_star])  # ±1/int8\n",
        "\n",
        "    # Candidats lexicaux (depuis buckets top-K + replis)\n",
        "    cand_vocab = _as_vocab_from_buckets(\n",
        "        C_K=C_K,\n",
        "        bucket2vocab=bucket2vocab,\n",
        "        history_fr=history_fr,\n",
        "        global_fallback_vocab=global_fallback_vocab,\n",
        "        min_size=1\n",
        "    )\n",
        "\n",
        "    # (DD6) Vote lexical\n",
        "    token_star, scores_cand, probs = DD6_vote(Z_hat, H_LM, L_mem=L_fr, L_lm=L_fr, cand_vocab=cand_vocab, lam=lam)\n",
        "\n",
        "    # (DD7) MAJ de l'état LM\n",
        "    H_LM_next = DD7_updateLM(H_LM, token_star, L_fr, Pi)\n",
        "\n",
        "    # Retour\n",
        "    if return_ck_scores:\n",
        "        return token_star, scores_cand, int(c_star), C_K, scores_CK\n",
        "    else:\n",
        "        # Chemin de compatibilité : renvoyer H_LM_next à la place des scores_CK\n",
        "        return token_star, scores_cand, int(c_star), C_K, H_LM_next\n",
        "\n",
        "# --- Test minimal d'intégration (corrigé) -------------------------------------\n",
        "def mock_L_fr(vocab_seed: int, D: int) -> Callable[[str], np.ndarray]:\n",
        "    rng = np.random.default_rng(vocab_seed)\n",
        "    table: dict[str, np.ndarray] = {}\n",
        "    def get(tok: str) -> np.ndarray:\n",
        "        if tok not in table:\n",
        "            x = rng.integers(0, 2, size=D, dtype=np.int8)\n",
        "            table[tok] = (2*x - 1).astype(np.int8)\n",
        "        return table[tok]\n",
        "    return get\n",
        "\n",
        "def test_isometry_and_flow() -> None:\n",
        "    D = 16384; K = 128\n",
        "    rng = np.random.default_rng(7)\n",
        "    Hs    = (2*rng.integers(0,2,size=D,dtype=np.int8)-1)\n",
        "    H_LM  = (2*rng.integers(0,2,size=D,dtype=np.int8)-1)\n",
        "    G_DEC = (2*rng.integers(0,2,size=D,dtype=np.int8)-1)\n",
        "    G_MEM = (2*rng.integers(0,2,size=D,dtype=np.int8)-1)\n",
        "    Pi    = rng.permutation(D).astype(np.int64)\n",
        "    Lfr   = mock_L_fr(1234, D)\n",
        "    B = 2048\n",
        "    prototypes = (2*rng.integers(0,2,size=(B,D),dtype=np.int8)-1)\n",
        "\n",
        "    # IMPORTANT : pour récupérer H_LM_next, fixer return_ck_scores=False\n",
        "    tok, scores, c_star, CK, H_LM_next = DecodeOneStep(\n",
        "        Hs, H_LM, history_fr=[\"de\",\"la\",\"musique\"],\n",
        "        G_DEC=G_DEC, G_MEM=G_MEM, Pi=Pi, L_fr=Lfr,\n",
        "        prototypes=prototypes, K=K, return_ck_scores=False\n",
        "    )\n",
        "    assert isinstance(tok, str) and scores.ndim == 1\n",
        "    assert H_LM_next.shape == (D,) and H_LM_next.dtype == np.int8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "db5fa07b",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_isometry_and_flow()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "587fb688",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b983cd28",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

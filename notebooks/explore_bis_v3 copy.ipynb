{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore_bis_v3\n",
    "\n",
    "Notebook simplifi\u00e9 montrant comment utiliser les blocs **ENC** et **MEM**\n",
    "expos\u00e9s par la librairie `hdc_project.encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4471c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using src path: /Users/aymenmejri/Desktop/MyCode/experiments/hdc_v2/hdc_project/src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "SRC = ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "print(f'Using src path: {SRC}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5984d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from hdc_project.encoder import m4, pipeline as enc_pipeline\n",
    "from hdc_project.encoder.mem import pipeline as mem_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Utilitaires HDC partag\u00e9s ----------------------------------------------\n",
    "def sign_strict_pm1(x: np.ndarray) -> np.ndarray:\n",
    "    y = (x >= 0).astype(np.int8, copy=False)\n",
    "    return ((y << 1) - 1).astype(np.int8, copy=False)\n",
    "\n",
    "def hd_assert_pm1(x: np.ndarray, D: int | None = None) -> None:\n",
    "    assert isinstance(x, np.ndarray), \"attendu np.ndarray\"\n",
    "    assert x.dtype == np.int8, \"dtype attendu: int8\"\n",
    "    assert np.all((x == 1) | (x == -1)), \"valeurs attendues: \u00b11\"\n",
    "    if D is not None:\n",
    "        assert x.ndim == 1 and x.shape[0] == D, f\"forme attendue: ({D},)\"\n",
    "\n",
    "def hd_bind(x: np.ndarray, key: np.ndarray) -> np.ndarray:\n",
    "    return (x.astype(np.int8, copy=False) * key.astype(np.int8, copy=False)).astype(np.int8, copy=False)\n",
    "\n",
    "def hd_sim(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    assert x.shape == y.shape\n",
    "    return float((x.astype(np.int32) @ y.astype(np.int32)) / x.shape[0])\n",
    "\n",
    "def build_perm_inverse(pi: np.ndarray) -> np.ndarray:\n",
    "    assert pi.ndim == 1 and np.issubdtype(pi.dtype, np.integer)\n",
    "    pi_inv = np.empty_like(pi)\n",
    "    pi_inv[pi] = np.arange(pi.shape[0], dtype=pi.dtype)\n",
    "    return pi_inv\n",
    "\n",
    "def permute_pow_signed(x: np.ndarray, pi: np.ndarray, pi_inv: np.ndarray, k: int) -> np.ndarray:\n",
    "    D = x.shape[0]\n",
    "    if k == 0:\n",
    "        return x\n",
    "    idx = np.arange(D, dtype=np.int64)\n",
    "    if k > 0:\n",
    "        for _ in range(k % D):\n",
    "            idx = pi[idx]\n",
    "    else:\n",
    "        for _ in range((-k) % D):\n",
    "            idx = pi_inv[idx]\n",
    "    return x[idx].astype(np.int8, copy=False)\n",
    "\n",
    "def permute_pow(x: np.ndarray, pi: np.ndarray, power: int) -> np.ndarray:\n",
    "    if power == 0:\n",
    "        return x\n",
    "    D = x.shape[0]\n",
    "    idx = np.arange(D, dtype=np.int64)\n",
    "    if power > 0:\n",
    "        for _ in range(power % D):\n",
    "            idx = pi[idx]\n",
    "    else:\n",
    "        pi_inv = build_perm_inverse(pi)\n",
    "        for _ in range((-power) % D):\n",
    "            idx = pi_inv[idx]\n",
    "    return x[idx].astype(np.int8, copy=False)\n",
    "\n",
    "def rademacher(D: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    return rng.choice(np.array([-1, 1], dtype=np.int8), size=D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f6e16",
   "metadata": {},
   "source": [
    "\n",
    "## Chargement du sous-corpus OPUS\n",
    "\n",
    "On r\u00e9utilise `opus_load_subset` depuis la librairie pour r\u00e9cup\u00e9rer un petit\n",
    "sous-\u00e9chantillon bilingue (EN/FR). En environnement hors-ligne, un jeu de\n",
    "repli est utilis\u00e9 pour que le notebook reste ex\u00e9cutable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ced34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hdc_project.encoder import m4, pipeline as enc_pipeline\n",
    "from hdc_project.encoder.mem import pipeline as mem_pipeline\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Chargement donn\u00e9es OPUS\n",
    "# ----------------------------\n",
    "try:\n",
    "    ens_raw, frs_raw = enc_pipeline.opus_load_subset(\n",
    "        name=\"opus_books\",\n",
    "        config=\"en-fr\",\n",
    "        split=\"train\",\n",
    "        N=10_000,\n",
    "        seed=2025,\n",
    "    )\n",
    "    print(f\"OPUS subset loaded: {len(ens_raw)} pairs\")\n",
    "except Exception as exc:\n",
    "    print(\"Warning: OPUS download failed, falling back to local toy corpus.\")\n",
    "    print(f\"Original error: {exc}\")\n",
    "    ens_raw = [\n",
    "        \"hyperdimensional computing is fun\",\n",
    "        \"vector symbolic architectures are powerful\",\n",
    "        \"encoding words into hyperspace\",\n",
    "        \"memory augmented networks love clean data\",\n",
    "    ]\n",
    "    frs_raw = [\n",
    "        \"le calcul hyperdimensionnel est amusant\",\n",
    "        \"les architectures symboliques vectorielles sont puissantes\",\n",
    "        \"encoder des mots dans l'hyperspace\",\n",
    "        \"les r\u00e9seaux augment\u00e9s de m\u00e9moire aiment les donn\u00e9es propres\",\n",
    "    ]\n",
    "\n",
    "enc_sample_size = min(10_000, len(ens_raw))\n",
    "mem_sample_size = min(10_000, len(ens_raw))\n",
    "ens_sample = ens_raw[:enc_sample_size]\n",
    "frs_sample = frs_raw[:enc_sample_size]\n",
    "print(f\"ENC sample size: {enc_sample_size}\")\n",
    "print(f\"MEM sample size: {mem_sample_size}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Encodage ENC (M5\u2013M7)\n",
    "# ----------------------------\n",
    "D = 8192\n",
    "n = 5\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "Lex_en = m4.M4_LexEN_new(seed=1, D=D)\n",
    "Lex_fr = m4.M4_LexEN_new(seed=2, D=D)\n",
    "pi = rng.permutation(D).astype(np.int64)\n",
    "\n",
    "encoded_en = enc_pipeline.encode_corpus_ENC(ens_sample, Lex_en, pi, D, n, seg_seed0=999)\n",
    "encoded_fr = enc_pipeline.encode_corpus_ENC(frs_sample, Lex_fr, pi, D, n, seg_seed0=1999)\n",
    "\n",
    "E_list_en = [segment[\"E_seq\"] for segment in encoded_en]\n",
    "H_list_en = [segment[\"H\"] for segment in encoded_en]\n",
    "print(f\"Encoded {len(encoded_en)} sentences; signature shape = {H_list_en[0].shape}\")\n",
    "\n",
    "# Quelques stats ENC\n",
    "s_intra, s_inter = enc_pipeline.intra_inter_ngram_sims(E_list_en, D)\n",
    "inter_seg = enc_pipeline.inter_segment_similarity(H_list_en)\n",
    "maj_curves = enc_pipeline.majority_error_curve(E_list_en, pi, D, eta_list=(0.0, 0.05))\n",
    "print(f\"intra={s_intra:.4f}, inter(abs)={s_inter:.4f}, inter segments={inter_seg:.4f}\")\n",
    "print(\"majority curve (eta=0):\", maj_curves[0.0][:2])\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2) Helpers de \"contenu\" (sans K_s) pour fabriquer les paires\n",
    "#    -> on somme des X_t (d\u00e9j\u00e0 align\u00e9s par Pi^\u0394), puis on seuillle\n",
    "# -------------------------------------------------------------\n",
    "def content_signature_from_Xseq(X_seq, majority: str = \"strict\"):\n",
    "    if not X_seq:\n",
    "        raise ValueError(\"X_seq vide\")\n",
    "    S = np.zeros((X_seq[0].shape[0],), dtype=np.int32)\n",
    "    for x in X_seq:\n",
    "        S += x.astype(np.int32, copy=False)\n",
    "    if majority == \"strict\":\n",
    "        return np.where(S >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    elif majority == \"unbiased\":\n",
    "        return np.where(S >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    else:\n",
    "        raise ValueError(\"majority must be 'strict' or 'unbiased'\")\n",
    "\n",
    "def span_signatures_from_trace(X_seq, win: int = 12, stride: int = 6, majority: str = \"unbiased\"):\n",
    "    if not X_seq:\n",
    "        return []\n",
    "    T = len(X_seq)\n",
    "    out = []\n",
    "    if T <= win:\n",
    "        out.append(content_signature_from_Xseq(X_seq, majority))\n",
    "        return out\n",
    "    for start in range(0, T - win + 1, max(1, stride)):\n",
    "        stop = start + win\n",
    "        out.append(content_signature_from_Xseq(X_seq[start:stop], majority))\n",
    "    return out\n",
    "\n",
    "def build_mem_pairs_from_encoded(encoded_en, encoded_fr, win=8, stride=4, majority=\"strict\", max_pairs=None):\n",
    "    pairs = []\n",
    "    N = min(len(encoded_en), len(encoded_fr))\n",
    "    for i in range(N):\n",
    "        X_en = encoded_en[i][\"X_seq\"]\n",
    "        X_fr = encoded_fr[i][\"X_seq\"]\n",
    "        spans_en = span_signatures_from_trace(X_en, win=win, stride=stride, majority=majority)\n",
    "        spans_fr = span_signatures_from_trace(X_fr, win=win, stride=stride, majority=majority)\n",
    "        L = min(len(spans_en), len(spans_fr))\n",
    "        for t in range(L):\n",
    "            pairs.append((\n",
    "                spans_en[t].astype(np.int8, copy=False),\n",
    "                spans_fr[t].astype(np.int8, copy=False),\n",
    "            ))\n",
    "            if max_pairs is not None and len(pairs) >= max_pairs:\n",
    "                return pairs\n",
    "    return pairs\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3) Paires MEM = spans EN/FR (contenu, sans K_s)\n",
    "# -------------------------------------------------------------\n",
    "pairs_mem = build_mem_pairs_from_encoded(encoded_en, encoded_fr, win=8, stride=4, majority=\"strict\")\n",
    "print(f\"Pairs available for MEM training: {len(pairs_mem)}\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4) Instanciation MEM et entra\u00eenement one-pass\n",
    "#    (k \u2248 log2(B) + marge ; ici B=256, k=24 convient)\n",
    "# -------------------------------------------------------------\n",
    "MEM_K = 16\n",
    "MEM_BUCKETS = 128\n",
    "cfg = mem_pipeline.MemConfig(D=D, B=MEM_BUCKETS, k=MEM_K, seed_lsh=10, seed_gmem=11)\n",
    "comp = mem_pipeline.make_mem_pipeline(cfg)\n",
    "mem_pipeline.train_one_pass_MEM(comp, pairs_mem)\n",
    "print(\"Training complete; few bucket counts:\", comp.mem.n[:64])\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5) Probe correcte : on interroge avec Z_en (span) et on compare\n",
    "#    le prototype choisi \u00e0 Z_fr (span) correspondant\n",
    "# -------------------------------------------------------------\n",
    "probe_count = min(200, len(pairs_mem))\n",
    "sim_values = []\n",
    "for Z_en_vec, Z_fr_vec in tqdm(pairs_mem[:probe_count]):\n",
    "    bucket_idx, score = mem_pipeline.infer_map_top1(comp, Z_en_vec)  # Z_en (span), pas H_en\n",
    "    prototype = comp.mem.H[bucket_idx].astype(np.int32, copy=False)\n",
    "    sim = float(np.dot(prototype, Z_fr_vec.astype(np.int32, copy=False)) / D)\n",
    "    sim_values.append(sim)\n",
    "\n",
    "print(f\"Top-1 mean similarity over {probe_count} span-probes: {np.mean(sim_values):.4f}\")\n",
    "print(f\"Top-1 median similarity: {np.median(sim_values):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82ab18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop mean/median/min/max/std: 296.625 293.0 216 477 39.670714954485\n",
      "p90/p99: 345 394\n"
     ]
    }
   ],
   "source": [
    "nb = comp.mem.n\n",
    "print(\"pop mean/median/min/max/std:\",\n",
    "      float(nb.mean()), float(np.median(nb)), int(nb.min()), int(nb.max()), float(nb.std()))\n",
    "print(\"p90/p99:\", int(np.quantile(nb, 0.90)), int(np.quantile(nb, 0.99)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bad5ba",
   "metadata": {},
   "source": [
    "\n",
    "> \u2139\ufe0f **Remarque pratique** : si le t\u00e9l\u00e9chargement OPUS \u00e9choue (ex\u00e9cution hors-ligne),\n",
    "> le notebook bascule automatiquement sur un mini corpus embarqu\u00e9 afin de\n",
    "> conserver une d\u00e9monstration reproductible des blocs ENC et MEM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a979536",
   "metadata": {},
   "source": [
    "# DEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd5622",
   "metadata": {},
   "source": [
    "## DEC-0 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52158548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "log = logging.getLogger(\"DEC\")\n",
    "if not log.handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22094d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pm1(shape, rng) -> np.ndarray:\n",
    "    \"\"\"Tire des vecteurs Rademacher \u00b11 en int8, shape=(...), dtype=int8.\"\"\"\n",
    "    return (2 * rng.integers(0, 2, size=shape, dtype=np.int8) - 1).astype(np.int8, copy=False)\n",
    "\n",
    "# ---------------------------\n",
    "# DX0: tests\n",
    "# ---------------------------\n",
    "def dx0_sanity(D: int = 16_384, N_sim: int = 1_000, seed: int = 2024, tol: float = 5e-3) -> None:\n",
    "    \"\"\"\n",
    "    V\u00e9rifie:\n",
    "      1) hd_sim(x,x)=1 et hd_sim(x,-x)=-1 (\u00e0 tol pr\u00e8s)\n",
    "      2) Invariance de similarit\u00e9 par binding: sim(x,y)=sim(x\u2297k, y\u2297k)\n",
    "      3) Pr\u00e9servation de la norme (||x||_2/\u221aD = 1) avant/apr\u00e8s binding\n",
    "    Crit\u00e8re d'acceptation (CA): \u00e9carts absolus \u2264 5e-3.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    max_err_self = 0.0\n",
    "    max_err_neg  = 0.0\n",
    "    max_err_bind = 0.0\n",
    "    max_err_norm = 0.0\n",
    "\n",
    "    for _ in range(N_sim):\n",
    "        x = pm1(D, rng); y = pm1(D, rng); k = pm1(D, rng)\n",
    "        hd_assert_pm1(x, D); hd_assert_pm1(y, D); hd_assert_pm1(k, D)\n",
    "\n",
    "        # (1) Identit\u00e9s de similarit\u00e9\n",
    "        s_xx = hd_sim(x, x)\n",
    "        s_xnx = hd_sim(x, (-x).astype(np.int8, copy=False))\n",
    "\n",
    "        max_err_self = max(max_err_self, abs(s_xx - 1.0))\n",
    "        max_err_neg  = max(max_err_neg,  abs(s_xnx + 1.0))\n",
    "\n",
    "        # (2) Invariance par binding (DEC1)\n",
    "        s_xy      = hd_sim(x, y)\n",
    "        xk, yk    = hd_bind(x, k), hd_bind(y, k)\n",
    "        s_xy_bind = hd_sim(xk, yk)\n",
    "        max_err_bind = max(max_err_bind, abs(s_xy - s_xy_bind))\n",
    "\n",
    "        # (3) Normes (avant/apr\u00e8s binding)\n",
    "        norm_x  = np.linalg.norm(x.astype(np.float64)) / np.sqrt(D)\n",
    "        norm_xk = np.linalg.norm(xk.astype(np.float64)) / np.sqrt(D)\n",
    "        max_err_norm = max(max_err_norm, abs(norm_x - 1.0), abs(norm_xk - 1.0))\n",
    "\n",
    "    # Rapport\n",
    "    print(\"DX0 \u2014 Sanity checks (double pr\u00e9cision)\")\n",
    "    print(f\"  D={D}, N={N_sim}, tol={tol:.1e}\")\n",
    "    print(f\"  max|sim(x,x)-1|         = {max_err_self:.3e}\")\n",
    "    print(f\"  max|sim(x,-x)+1|        = {max_err_neg:.3e}\")\n",
    "    print(f\"  max|sim(x,y)-sim(x\u2297k,y\u2297k)| = {max_err_bind:.3e}\")\n",
    "    print(f\"  max| ||x||/\u221aD - 1 | (incl. bind) = {max_err_norm:.3e}\")\n",
    "\n",
    "    # Assertions CA\n",
    "    assert max_err_self <= tol,     \"CA non satisfait: sim(x,x) s'\u00e9carte de 1\"\n",
    "    assert max_err_neg  <= tol,     \"CA non satisfait: sim(x,-x) s'\u00e9carte de -1\"\n",
    "    assert max_err_bind <= tol,     \"CA non satisfait: invariance de similarit\u00e9 apr\u00e8s binding\"\n",
    "    assert max_err_norm <= tol,     \"CA non satisfait: norme non pr\u00e9serv\u00e9e (relative)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c12ada1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX0 \u2014 Sanity checks (double pr\u00e9cision)\n",
      "  D=16384, N=1000, tol=5.0e-03\n",
      "  max|sim(x,x)-1|         = 0.000e+00\n",
      "  max|sim(x,-x)+1|        = 0.000e+00\n",
      "  max|sim(x,y)-sim(x\u2297k,y\u2297k)| = 0.000e+00\n",
      "  max| ||x||/\u221aD - 1 | (incl. bind) = 0.000e+00\n"
     ]
    }
   ],
   "source": [
    "dx0_sanity()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4826d",
   "metadata": {},
   "source": [
    "## DD1 .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ebc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DD1_ctx(Hs: np.ndarray, G_DEC: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Q^(s) = H^(s) \u2297 G_DEC, binding isom\u00e9trique (int8 -> int8).\n",
    "    \"\"\"\n",
    "    assert Hs.dtype == np.int8 and G_DEC.dtype == np.int8\n",
    "    hd_assert_pm1(Hs); hd_assert_pm1(G_DEC, Hs.shape[0])\n",
    "    return hd_bind(Hs, G_DEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cc32465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX1 \u2014 DD1_ctx (isom\u00e9trie & contrats)\n",
      "  D=16384, m=64, trials=200, tol=5.0e-03\n",
      "  max|sim_before - sim_after|  = 0.000e+00\n",
      "  max| ||H||/\u221aD - 1 | (incl. bind) = 0.000e+00\n",
      "  max|Gram_before - Gram_after| = 0.000e+00\n"
     ]
    }
   ],
   "source": [
    "# --- DX1: tests d\u00e9taill\u00e9s ---\n",
    "def dx1_test_DD1_ctx(D: int = 16_384, m: int = 64, trials: int = 200, seed: int = 1234, tol: float = 5e-3):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # 1) Similarit\u00e9 inchang\u00e9e et normes pr\u00e9serv\u00e9es (sur 'trials' paires)\n",
    "    max_err_sim = 0.0\n",
    "    max_err_norm = 0.0\n",
    "    for _ in range(trials):\n",
    "        H1, H2, G = pm1(D, rng), pm1(D, rng), pm1(D, rng)\n",
    "        # Copies pour v\u00e9rifier non-mutation\n",
    "        H1_copy, H2_copy, G_copy = H1.copy(), H2.copy(), G.copy()\n",
    "\n",
    "        Q1, Q2 = DD1_ctx(H1, G), DD1_ctx(H2, G)\n",
    "        # Similarit\u00e9\n",
    "        s0 = hd_sim(H1, H2)\n",
    "        s1 = hd_sim(Q1, Q2)\n",
    "        max_err_sim = max(max_err_sim, abs(s0 - s1))\n",
    "\n",
    "        # Normes relatives\n",
    "        nH1  = np.linalg.norm(H1.astype(np.float64)) / np.sqrt(D)\n",
    "        nQ1  = np.linalg.norm(Q1.astype(np.float64)) / np.sqrt(D)\n",
    "        nH2  = np.linalg.norm(H2.astype(np.float64)) / np.sqrt(D)\n",
    "        nQ2  = np.linalg.norm(Q2.astype(np.float64)) / np.sqrt(D)\n",
    "        max_err_norm = max(max_err_norm, abs(nH1 - 1.0), abs(nQ1 - 1.0),\n",
    "                                           abs(nH2 - 1.0), abs(nQ2 - 1.0))\n",
    "\n",
    "        # Contrats: dtype & non-mutation\n",
    "        assert Q1.dtype == np.int8 and Q2.dtype == np.int8\n",
    "        assert np.all(H1 == H1_copy) and np.all(H2 == H2_copy) and np.all(G == G_copy), \"mutation d\u00e9tect\u00e9e\"\n",
    "        assert np.all((Q1 == 1) | (Q1 == -1)) and np.all((Q2 == 1) | (Q2 == -1)), \"sortie hors \u00b11\"\n",
    "\n",
    "    # 2) Isom\u00e9trie de Gram (m vecteurs)\n",
    "    H = np.stack([pm1(D, rng) for _ in range(m)], axis=0)  # (m, D) \u00b11/int8\n",
    "    G = pm1(D, rng)\n",
    "    Q = np.stack([DD1_ctx(H[i], G) for i in range(m)], axis=0)\n",
    "\n",
    "    # Gram avant/apr\u00e8s, en double pr\u00e9cision\n",
    "    G0 = (H.astype(np.int32) @ H.astype(np.int32).T) / D\n",
    "    G1 = (Q.astype(np.int32) @ Q.astype(np.int32).T) / D\n",
    "    max_err_gram = float(np.max(np.abs(G0.astype(np.float64) - G1.astype(np.float64))))\n",
    "\n",
    "    # --- Rapport ---\n",
    "    print(\"DX1 \u2014 DD1_ctx (isom\u00e9trie & contrats)\")\n",
    "    print(f\"  D={D}, m={m}, trials={trials}, tol={tol:.1e}\")\n",
    "    print(f\"  max|sim_before - sim_after|  = {max_err_sim:.3e}\")\n",
    "    print(f\"  max| ||H||/\u221aD - 1 | (incl. bind) = {max_err_norm:.3e}\")\n",
    "    print(f\"  max|Gram_before - Gram_after| = {max_err_gram:.3e}\")\n",
    "\n",
    "    # --- Crit\u00e8res d'acceptation ---\n",
    "    assert max_err_sim  <= tol, \"Invariance de similarit\u00e9 viol\u00e9e (DEC1)\"\n",
    "    assert max_err_norm <= tol, \"Norme non pr\u00e9serv\u00e9e (relative)\"\n",
    "    assert max_err_gram <= tol, \"Isom\u00e9trie de Gram viol\u00e9e (DEC1)\"\n",
    "\n",
    "dx1_test_DD1_ctx()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe1921",
   "metadata": {},
   "source": [
    "# DD2 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f4e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "log = logging.getLogger(\"DEC.DX2.v2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a257745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DD2_query + DX2 (corrig\u00e9s)\n",
    "# =========================\n",
    "# Changements cl\u00e9s :\n",
    "#  - DD2_query : hyperparam\u00e8tres keyword-only (*, alpha, beta, ell) pour \u00e9viter\n",
    "#    le doublonnage positionnel/mot-cl\u00e9.\n",
    "#  - Impl\u00e9mentation robuste des permutations \u03a0^k pour k \u2208 \u2124 (k<0 via \u03a0^{-1}).\n",
    "#  - Contr\u00f4les de types/signatures (\u00b11/int8) et normalisation ||R_t||\u2248\u221aD.\n",
    "#  - DX2_run : corrections des tests (in_band), invariances Gram et identit\u00e9\n",
    "#    paire-\u00e0-paire <\u03a0^i L_i, \u03a0^k L_k> = <L_i, \u03a0^{k-i} L_k>.\n",
    "#  - Journalisation propre.\n",
    "\n",
    "import numpy as np\n",
    "from typing import Callable, List\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(\"DX2\")\n",
    "if not log.handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "# -------- Module DD2 (requ\u00eate mixte) -------------------------------------------\n",
    "\n",
    "# --- DD2 : version \"continue\" (pour DX2 et analyses de norme) -----------------\n",
    "def DD2_query(\n",
    "    Qs: np.ndarray,\n",
    "    hist_vectors: List[np.ndarray],  # liste de L_fr(\\hat v_{t-j}) en \u00b11/int8\n",
    "    pi: np.ndarray,\n",
    "    *,\n",
    "    alpha: float = 1.0,\n",
    "    beta:  float = 1.0,\n",
    "    ell:   int   = 4\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Version continue (float64) : R_t = \u03b1\u00b7Qs + \u03b2\u00b7sign(\u03a3_{j=1..ell} \u03a0^j L_{t-j}),\n",
    "    puis renvoie un vecteur de norme \u2248 \u221aD (utile pour DX2). NON utilis\u00e9e par DD3.\n",
    "    \"\"\"\n",
    "    D = Qs.shape[0]\n",
    "    hd_assert_pm1(Qs, D)\n",
    "    assert pi.ndim == 1 and pi.shape[0] == D and np.issubdtype(pi.dtype, np.integer)\n",
    "    ell = min(ell, len(hist_vectors))\n",
    "    pi_inv = build_perm_inverse(pi)\n",
    "\n",
    "    if ell == 0:\n",
    "        H_hist = np.ones(D, dtype=np.int8)\n",
    "    else:\n",
    "        acc = np.zeros(D, dtype=np.int16)\n",
    "        for j in range(1, ell+1):\n",
    "            Lj = hist_vectors[j-1]; hd_assert_pm1(Lj, D)\n",
    "            acc += permute_pow_signed(Lj, pi, pi_inv, j).astype(np.int16, copy=False)\n",
    "        H_hist = sign_strict_pm1(acc)\n",
    "\n",
    "    Rt = alpha * Qs.astype(np.float64) + beta * H_hist.astype(np.float64)\n",
    "    nrm = float(np.linalg.norm(Rt))\n",
    "    if nrm > 0:\n",
    "        Rt = Rt / nrm * np.sqrt(D)\n",
    "    else:\n",
    "        Rt = np.ones(D, dtype=np.float64)\n",
    "    return Rt  # float64\n",
    "\n",
    "# --- DD2 : version \"binaire\" (pipeline DEC ; compatible DD3 int8) -------------\n",
    "def DD2_query_bin(\n",
    "    Qs: np.ndarray,\n",
    "    history_fr: List[str],    # liste de tokens FR\n",
    "    L_fr: Callable[[str], np.ndarray],\n",
    "    Pi: np.ndarray,\n",
    "    *,\n",
    "    alpha: float = 1.0,\n",
    "    beta:  float = 1.0,\n",
    "    ell:   int   = 4\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Version binaire : construit H_hist depuis les tokens (via L_fr),\n",
    "    combine \u03b1\u00b7Qs + \u03b2\u00b7H_hist puis seuillage strict -> int8 \u00b11.\n",
    "    Contrat de sortie : \u00b11/int8 (exig\u00e9 par DD3_bindToMem).\n",
    "    \"\"\"\n",
    "    D = Qs.shape[0]\n",
    "    hd_assert_pm1(Qs, D)\n",
    "    assert Pi.ndim == 1 and Pi.shape[0] == D and np.issubdtype(Pi.dtype, np.integer)\n",
    "    pi_inv = build_perm_inverse(Pi)\n",
    "\n",
    "    # Convertir l'historique de tokens en vecteurs lexicaux \u00b11/int8\n",
    "    hist_vecs: List[np.ndarray] = []\n",
    "    for tok in history_fr[:ell]:\n",
    "        Lv = L_fr(tok).astype(np.int8, copy=False)\n",
    "        hd_assert_pm1(Lv, D)\n",
    "        hist_vecs.append(Lv)\n",
    "\n",
    "    if len(hist_vecs) == 0:\n",
    "        H_hist = np.ones(D, dtype=np.int8)\n",
    "    else:\n",
    "        acc = np.zeros(D, dtype=np.int16)\n",
    "        for j, L_j in enumerate(hist_vecs, start=1):\n",
    "            acc += permute_pow_signed(L_j, Pi, pi_inv, j).astype(np.int16, copy=False)\n",
    "        H_hist = sign_strict_pm1(acc)  # \u00b11/int8\n",
    "\n",
    "    # Combinaison et seuillage final pour rester en \u00b11\n",
    "    combo = (alpha * Qs.astype(np.int16)) + (beta * H_hist.astype(np.int16))\n",
    "    Rt_bin = sign_strict_pm1(combo.astype(np.int16))\n",
    "    return Rt_bin  # int8 \u00b11\n",
    "\n",
    "# -------- Banc de test DX2 -----------------------------------------------------\n",
    "\n",
    "def DX2_run():\n",
    "    D, trials = 16384, 200\n",
    "    ells = (2, 4, 8)\n",
    "    ratios = (1/3, 1.0, 3.0)\n",
    "    g = np.random.default_rng(2025)\n",
    "\n",
    "    # permutation al\u00e9atoire (fix\u00e9e) et son inverse\n",
    "    pi = np.arange(D, dtype=np.int64); g.shuffle(pi)\n",
    "    pi_inv = build_perm_inverse(pi)\n",
    "\n",
    "    def sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        return float((a.astype(np.int32) @ b.astype(np.int32)) / D)\n",
    "\n",
    "    norms: dict[tuple[int, float], tuple[float, float, float]] = {}\n",
    "    gram_uniform_ok = True\n",
    "    pair_shift_ok   = True\n",
    "\n",
    "    for ell in ells:\n",
    "        for r in ratios:\n",
    "            alpha, beta = r, 1.0\n",
    "            vals = []\n",
    "            for _ in range(trials):\n",
    "                # Qs et historique\n",
    "                Qs   = pm1(D, g)\n",
    "                hist = [pm1(D, g) for _ in range(ell)]\n",
    "\n",
    "                # Matrice des versions permut\u00e9es P[j] = \u03a0^{j+1} L_{t-(j+1)}\n",
    "                P = np.stack([permute_pow_signed(hist[j], pi, pi_inv, j+1)\n",
    "                              for j in range(ell)], axis=0).astype(np.int8, copy=False)\n",
    "\n",
    "                # (i) Invariance Gram sous permutation UNIFORME (m\u00eame d\u00e9calage s pour toutes les lignes)\n",
    "                s = int(g.integers(1, 7))\n",
    "                P_uni = np.stack([permute_pow_signed(P[j], pi, pi_inv, s)\n",
    "                                  for j in range(ell)], axis=0).astype(np.int8, copy=False)\n",
    "                # Gram (corr\u00e9lations normalis\u00e9es) avant / apr\u00e8s\n",
    "                G  = (P.astype(np.int32) @ P.T.astype(np.int32)) / D\n",
    "                Gu = (P_uni.astype(np.int32) @ P_uni.T.astype(np.int32)) / D\n",
    "                if not np.allclose(G, Gu, atol=5e-3, rtol=0):\n",
    "                    gram_uniform_ok = False\n",
    "\n",
    "                # (ii) Identit\u00e9 paire-\u00e0-paire :\n",
    "                #      <\u03a0^i L_i, \u03a0^k L_k> == <L_i, \u03a0^{k-i} L_k>  pour i,k = 1..ell\n",
    "                for i in range(1, ell+1):\n",
    "                    for k in range(1, ell+1):\n",
    "                        lhs = sim(permute_pow_signed(hist[i-1], pi, pi_inv, i),\n",
    "                                  permute_pow_signed(hist[k-1], pi, pi_inv, k))\n",
    "                        rhs = sim(hist[i-1],\n",
    "                                  permute_pow_signed(hist[k-1], pi, pi_inv, k - i))\n",
    "                        if abs(lhs - rhs) > 5e-3:\n",
    "                            pair_shift_ok = False\n",
    "                            break\n",
    "                    if not pair_shift_ok:\n",
    "                        break\n",
    "\n",
    "                # (iii) Norme de R_t / \u221aD dans [0.9, 1.1]\n",
    "                Rt = DD2_query(Qs, hist, pi, alpha=alpha, beta=beta, ell=ell)\n",
    "                vals.append(float(np.linalg.norm(Rt) / np.sqrt(D)))\n",
    "\n",
    "            norms[(ell, r)] = (min(vals), float(np.median(vals)), max(vals))\n",
    "\n",
    "    # Reporting\n",
    "    log.info(\"DX2 \u2014 Norme(R_t)/sqrt(D) par (ell, alpha/beta): min | median | max\")\n",
    "    for (ell, r), (mn, md, mx) in sorted(norms.items()):\n",
    "        log.info(\"  ell=%d, alpha/beta=%.3g  ->  %.3f | %.3f | %.3f\", ell, r, mn, md, mx)\n",
    "\n",
    "    # Crit\u00e8res d\u2019acceptation\n",
    "    in_band = all((0.9 <= mn <= 1.1) and (0.9 <= md <= 1.1) and (0.9 <= mx <= 1.1)\n",
    "                  for (mn, md, mx) in norms.values())\n",
    "    assert in_band, \"DX2: norme(R_t)/sqrt(D) hors bande [0.9,1.1] pour au moins un (ell, ratio).\"\n",
    "    assert gram_uniform_ok, \"DX2: Gram NON invariant sous permutation uniforme (isom\u00e9trie viol\u00e9e).\"\n",
    "    assert pair_shift_ok,   \"DX2: identit\u00e9 de d\u00e9calage paire-\u00e0-paire viol\u00e9e.\"\n",
    "\n",
    "    log.info(\"DX2 \u2014 CA valid\u00e9s: (i) norme \u2208 [0.9,1.1] ; (ii) Gram invariant ; (iii) identit\u00e9 paire-\u00e0-paire OK.\")\n",
    "\n",
    "# --- Ex\u00e9cution du test (\u00e0 commenter/supprimer si vous int\u00e9grez dans une suite) ---\n",
    "# DX2_run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e4acc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 23:22:58,772 [INFO] DX2 \u2014 Norme(R_t)/sqrt(D) par (ell, alpha/beta): min | median | max\n",
      "2025-10-06 23:22:58,772 [INFO]   ell=2, alpha/beta=0.333  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,772 [INFO]   ell=2, alpha/beta=1  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,772 [INFO]   ell=2, alpha/beta=3  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,772 [INFO]   ell=4, alpha/beta=0.333  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,773 [INFO]   ell=4, alpha/beta=1  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,773 [INFO]   ell=4, alpha/beta=3  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,773 [INFO]   ell=8, alpha/beta=0.333  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,773 [INFO]   ell=8, alpha/beta=1  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,773 [INFO]   ell=8, alpha/beta=3  ->  1.000 | 1.000 | 1.000\n",
      "2025-10-06 23:22:58,773 [INFO] DX2 \u2014 CA valid\u00e9s: (i) norme \u2208 [0.9,1.1] ; (ii) Gram invariant ; (iii) identit\u00e9 paire-\u00e0-paire OK.\n"
     ]
    }
   ],
   "source": [
    "DX2_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d999ef",
   "metadata": {},
   "source": [
    "# DD3 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "871f3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DD3_bindToMem(Rt: np.ndarray, G_MEM: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"~R_t = R_t \u2297 G_MEM (int8 -> int8).\"\"\"\n",
    "    hd_assert_pm1(Rt); hd_assert_pm1(G_MEM, Rt.shape[0])\n",
    "    return hd_bind(Rt, G_MEM)\n",
    "\n",
    "def hd_sim_dot(x: np.ndarray, y: np.ndarray) -> int:\n",
    "    \"\"\"Produit scalaire entier (\u00e9vite l'arrondi); x,y en int8 \u00b11.\"\"\"\n",
    "    return int(x.astype(np.int32) @ y.astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f40787be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- test KS (asymptotique) sans d\u00e9pendance externe ---\n",
    "def ks_2samp_asymp(x: np.ndarray, y: np.ndarray) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    KS \u00e0 2 \u00e9chantillons: renvoie (D_stat, pval approx).\n",
    "    Correctifs:\n",
    "      - si D_stat == 0 => p = 1.0 (distributions identiques)\n",
    "      - clamp num\u00e9rique sur lambda pour petits D_stat\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    y = np.asarray(y, dtype=np.float64)\n",
    "    n, m = x.size, y.size\n",
    "    x_sorted = np.sort(x); y_sorted = np.sort(y)\n",
    "    i = j = 0\n",
    "    cdf_x = cdf_y = 0.0\n",
    "    D_stat = 0.0\n",
    "    while i < n and j < m:\n",
    "        if x_sorted[i] < y_sorted[j]:\n",
    "            cdf_x = (i + 1) / n; i += 1\n",
    "        elif x_sorted[i] > y_sorted[j]:\n",
    "            cdf_y = (j + 1) / m; j += 1\n",
    "        else:\n",
    "            v = x_sorted[i]\n",
    "            while i < n and x_sorted[i] == v: i += 1\n",
    "            while j < m and y_sorted[j] == v: j += 1\n",
    "            cdf_x = i / n; cdf_y = j / m\n",
    "        D_stat = max(D_stat, abs(cdf_x - cdf_y))\n",
    "    if i < n: D_stat = max(D_stat, abs(1.0 - (j / m)))\n",
    "    if j < m: D_stat = max(D_stat, abs(1.0 - (i / n)))\n",
    "\n",
    "    # --- Correctif d\u00e9g\u00e9n\u00e9r\u00e9 ---\n",
    "    if D_stat == 0.0:\n",
    "        return 0.0, 1.0\n",
    "\n",
    "    en = np.sqrt(n * m / (n + m))\n",
    "    lam = (en + 0.12 + 0.11 / max(en, 1e-12)) * D_stat\n",
    "    # Pour tr\u00e8s petits \"lam\", la s\u00e9rie tend vers 1 => borne sup\u00e9rieure 1.0\n",
    "    if lam < 1e-8:\n",
    "        return float(D_stat), 1.0\n",
    "\n",
    "    # \u00c9valuation de la s\u00e9rie altern\u00e9e (tronqu\u00e9e) avec coupe stricte dans [0,1]\n",
    "    terms = [np.exp(-2.0 * (k**2) * (lam**2)) for k in range(1, 201)]\n",
    "    pval = 2.0 * sum(((-1)**(k-1)) * terms[k-1] for k in range(1, len(terms)+1))\n",
    "    pval = float(max(0.0, min(1.0, pval)))\n",
    "    return float(D_stat), pval\n",
    "\n",
    "# --- campagne DX3 ---\n",
    "def DX3_run(D: int = 16384, C: int = 500, T: int = 200, seed: int = 2025,\n",
    "            rel_tol: float = 0.01, pmin: float = 0.10) -> None:\n",
    "    \"\"\"\n",
    "    D: dimension; C: #protos m\u00e9moire; T: #requ\u00eates; rel_tol: seuil d'\u00e9cart relatif moyen; pmin: seuil KS.\n",
    "    \"\"\"\n",
    "    g = np.random.default_rng(seed)\n",
    "\n",
    "    # G\u00e9n\u00e8re cl\u00e9s et banques en \u00b11/int8\n",
    "    G_MEM = pm1(D, g)\n",
    "    M_bank = np.stack([pm1(D, g) for _ in range(C)], axis=0)   # (C, D), int8\n",
    "    Q_batch = np.stack([pm1(D, g) for _ in range(T)], axis=0)  # (T, D), int8\n",
    "\n",
    "    # Scores \"dans la tranche m\u00e9moire\" vs \"d\u00e9bind\u00e9s\"\n",
    "    #   S_mem[t,c]   = < Rt\u2297G_MEM , M_c >\n",
    "    #   S_unbd[t,c]  = < Rt , M_c\u2297G_MEM >\n",
    "    S_mem  = np.zeros((T, C), dtype=np.int32)\n",
    "    S_unbd = np.zeros((T, C), dtype=np.int32)\n",
    "\n",
    "    for t in range(T):\n",
    "        Rt = Q_batch[t]\n",
    "        Rt_mem = DD3_bindToMem(Rt, G_MEM)            # Rt \u2297 G_MEM\n",
    "        for c in range(C):\n",
    "            Mc = M_bank[c]\n",
    "            S_mem[t, c]  = hd_sim_dot(Rt_mem, Mc)\n",
    "            S_unbd[t, c] = hd_sim_dot(Rt, hd_bind(Mc, G_MEM))\n",
    "\n",
    "    # a) Erreur relative moyenne (sur tous les scores)\n",
    "    A = S_mem.astype(np.float64).ravel()\n",
    "    B = S_unbd.astype(np.float64).ravel()\n",
    "    denom = np.maximum(1.0, np.abs(B))               # \u00e9vite division par 0\n",
    "    rel_err = np.abs(A - B) / denom\n",
    "    rel_err_mean = float(np.mean(rel_err))\n",
    "\n",
    "    # b) Test KS sur distributions aplaties\n",
    "    D_stat, pval = ks_2samp_asymp(A, B)\n",
    "\n",
    "    # Reporting\n",
    "    log.info(\"DX3 \u2014 Invariance (d\u00e9)binding m\u00e9moire\")\n",
    "    log.info(\"  D=%d, C=%d, T=%d\", D, C, T)\n",
    "    log.info(\"  Erreur relative moyenne  = %.6f\", rel_err_mean)\n",
    "    log.info(\"  KS: D=%.6f, p=%.3f\", D_stat, pval)\n",
    "\n",
    "    # CA\n",
    "    assert rel_err_mean <= rel_tol, f\"DX3: erreur relative moyenne {rel_err_mean:.4f} > {rel_tol}\"\n",
    "    assert pval > pmin, f\"DX3: p-value KS {pval:.3f} \u2264 {pmin:.2f}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c21c01a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 22:57:03,498 [INFO] DX3 \u2014 Invariance (d\u00e9)binding m\u00e9moire\n",
      "2025-10-06 22:57:03,498 [INFO]   D=16384, C=500, T=200\n",
      "2025-10-06 22:57:03,498 [INFO]   Erreur relative moyenne  = 0.000000\n",
      "2025-10-06 22:57:03,498 [INFO]   KS: D=0.000000, p=1.000\n"
     ]
    }
   ],
   "source": [
    "DX3_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb3d23c",
   "metadata": {},
   "source": [
    "# DD4 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c549139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DD4_search_topK(Rt_tilde: np.ndarray, prototypes: np.ndarray, K: int) -> tuple[int, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    prototypes: array shape (B, D) en int8 (\u00b11) pour M_c seuill\u00e9s ou non seuill\u00e9s normalis\u00e9s.\n",
    "    Retour: (c_star, C_K, scores_CK)\n",
    "    \"\"\"\n",
    "    D = Rt_tilde.shape[0]\n",
    "    assert prototypes.ndim == 2 and prototypes.shape[1] == D and prototypes.dtype == np.int8\n",
    "    # Produits scalaires stables\n",
    "    scores = (prototypes.astype(np.int32) @ Rt_tilde.astype(np.int32)).astype(np.int32)  # (B,)\n",
    "    K = min(K, scores.shape[0])\n",
    "    idx = np.argpartition(scores, -K)[-K:]\n",
    "    top_order = idx[np.argsort(scores[idx])[::-1]]\n",
    "    c_star = int(top_order[0])\n",
    "    return c_star, top_order, scores[top_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48850b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "def DX4_run(D: int = 16384, B: int = 10000, trials: int = 200, \n",
    "            Ks=(100, 500, 2000), seed: int = 0) -> dict[int, float]:\n",
    "    \"\"\"\n",
    "    Mesure empirique du rappel de c* parmi les top-K prototypes.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    recalls = {K: 0 for K in Ks}\n",
    "    for _ in tqdm(range(trials)):\n",
    "        # G\u00e9n\u00e8re B prototypes \u00b11 (int8)\n",
    "        prototypes = rng.choice([-1, 1], size=(B, D))\n",
    "        prototypes = prototypes.astype(np.int8)\n",
    "        # Choisit une classe cible c*\n",
    "        c_star = rng.integers(0, B)\n",
    "        Rt = prototypes[c_star].copy()\n",
    "        # Appel au module DD4\n",
    "        _, C_K, _ = DD4_search_topK(Rt, prototypes, max(Ks))\n",
    "        for K in Ks:\n",
    "            if c_star in C_K[:K]:\n",
    "                recalls[K] += 1\n",
    "    # Moyenne\n",
    "    return {K: recalls[K]/trials for K in Ks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e3dea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [03:23<00:00,  1.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{100: 1.0, 500: 1.0, 2000: 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DX4_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b943256",
   "metadata": {},
   "source": [
    "# DD5 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bff951c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DD5_payload(Mc: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Mc: prototype non seuill\u00e9 (int16/int32) OU d\u00e9j\u00e0 binaire int8.\n",
    "    Renvoie Z_hat en int8 (\u00b11).\n",
    "    \"\"\"\n",
    "    if Mc.dtype == np.int8:\n",
    "        hd_assert_pm1(Mc)\n",
    "        return Mc\n",
    "    return sign_strict_pm1(Mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e44670a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DX5_run(D: int = 16384, trials: int = 200, ms=(4, 8, 16), seed: int = 0):\n",
    "    \"\"\"\n",
    "    Mesure l\u2019exactitude binaire en fonction du nombre m_{c*}.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    accuracies = {}\n",
    "    for m in ms:\n",
    "        accs = []\n",
    "        for _ in range(trials):\n",
    "            # Vecteur de r\u00e9f\u00e9rence\n",
    "            ref = rng.choice([-1, 1], size=D).astype(np.int8)\n",
    "            # Accumulation de m copies bruit\u00e9es\n",
    "            acc = np.zeros(D, dtype=np.int32)\n",
    "            for _ in range(m):\n",
    "                acc += ref\n",
    "            # Seuillage\n",
    "            Z_hat = DD5_payload(acc)\n",
    "            # Exactitude binaire\n",
    "            accs.append(np.mean(Z_hat == ref))\n",
    "        accuracies[m] = float(np.mean(accs))\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdb3df09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 1.0, 8: 1.0, 16: 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DX5_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa074d57",
   "metadata": {},
   "source": [
    "# DD6 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7ff4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def DD6_vote(\n",
    "#     Z_hat: np.ndarray,\n",
    "#     H_LM: np.ndarray,\n",
    "#     L_fr,\n",
    "#     cand_vocab: list[str],\n",
    "#     lam: float = 0.0\n",
    "# ) -> tuple[str, np.ndarray]:\n",
    "#     \"\"\"\n",
    "#     Renvoie (token*, scores) sur cand_vocab.\n",
    "#     \"\"\"\n",
    "#     D = Z_hat.shape[0]\n",
    "#     hd_assert_pm1(Z_hat, D); hd_assert_pm1(H_LM, D)\n",
    "#     scores = []\n",
    "#     for v in cand_vocab:\n",
    "#         Lv = L_fr(v).astype(np.int8, copy=False)\n",
    "#         hd_assert_pm1(Lv, D)\n",
    "#         s = (Z_hat.astype(np.int32) @ Lv.astype(np.int32)) \\\n",
    "#             + lam * (H_LM.astype(np.int32) @ Lv.astype(np.int32))\n",
    "#         scores.append(float(s))\n",
    "#     scores = np.asarray(scores, dtype=np.float32)\n",
    "#     best = int(np.argmax(scores))\n",
    "#     return cand_vocab[best], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e56642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyLexFR:\n",
    "    def __init__(self, vocab: list[str], D: int, seed: int = 1234):\n",
    "        self.vocab = vocab\n",
    "        self.D = D\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        # table de vecteurs \u00b11/int8\n",
    "        self.table = {v: self.rng.choice(np.array([-1, 1], dtype=np.int8), size=D) for v in vocab}\n",
    "\n",
    "    def __call__(self, v: str) -> np.ndarray:\n",
    "        return self.table[v]\n",
    "\n",
    "# # -- G\u00e9n\u00e9ration contr\u00f4l\u00e9e de corr\u00e9lations (flip par coordonn\u00e9e) ----------------\n",
    "# def flip_to_target(vec: np.ndarray, target_sim: float, rng: np.random.Generator) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Retourne une copie de 'vec' dont la similarit\u00e9 attendue vaut 'target_sim'.\n",
    "#     Pour \u00b11, si p_flip = (1 - target_sim)/2, alors E[sim] = 1 - 2*p_flip = target_sim.\n",
    "#     \"\"\"\n",
    "#     D = vec.shape[0]\n",
    "#     p_flip = max(0.0, min(1.0, (1.0 - float(target_sim)) / 2.0))\n",
    "#     mask = (rng.random(D) < p_flip).astype(np.int8)          # 1 si on flippe\n",
    "#     flips = (1 - 2 * mask).astype(np.int8, copy=False)       # 1 -> -1, 0 -> +1\n",
    "#     out = (vec.astype(np.int8, copy=False) * flips).astype(np.int8, copy=False)\n",
    "#     return out\n",
    "\n",
    "# # -- Module test\u00e9 (fourni) ------------------------------------------------------\n",
    "\n",
    "# def _batch_lex(cand_vocab, L):\n",
    "#     \"\"\"\n",
    "#     Applique le callable 'L' (v -> \u00b11/int8 de forme (D,)) sur tout le vocabulaire candidat\n",
    "#     et empile en une matrice (V, D) en int8.\n",
    "#     \"\"\"\n",
    "#     mats = []\n",
    "#     for v in cand_vocab:\n",
    "#         vec = L(v).astype(np.int8, copy=False)\n",
    "#         mats.append(vec)\n",
    "#     M = np.vstack(mats).astype(np.int8, copy=False)\n",
    "#     return M\n",
    "\n",
    "# def DD6_vote(\n",
    "#     Z_hat: np.ndarray,\n",
    "#     H_LM: np.ndarray,\n",
    "#     L_mem,                  # callable: v -> \u00b11 int8 (D,)\n",
    "#     L_lm,                   # callable: v -> \u00b11 int8 (D,)\n",
    "#     cand_vocab: list[str],\n",
    "#     lam: float = 0.0,\n",
    "#     *,\n",
    "#     normalize: str = \"sqrtD\",   # {\"none\",\"sqrtD\"} ; \"sqrtD\" conseill\u00e9 pour perplexit\u00e9\n",
    "#     return_probs: bool = False, # si True, renvoie aussi les probabilit\u00e9s softmax\n",
    "#     tau: float = 1.0            # temp\u00e9rature du softmax (si return_probs=True)\n",
    "# ) -> tuple[str, np.ndarray, np.ndarray | None]:\n",
    "#     \"\"\"\n",
    "#     s(v) = <Z_hat, L_mem(v)> + lam * <H_LM, L_lm(v)>\n",
    "#     Retourne: (token*, scores_raw, probs|None)\n",
    "#       - scores_raw: np.float64 de taille V (non normalis\u00e9s, utiles pour debug/tra\u00e7age)\n",
    "#       - probs:      np.float64 de taille V si return_probs=True (softmax stable)\n",
    "#     Contrats:\n",
    "#       Z_hat, H_LM: \u00b11/int8, de longueur D identique.\n",
    "#       L_mem, L_lm: renvoient \u00b11/int8 (D,) pour tout v de cand_vocab.\n",
    "#     \"\"\"\n",
    "#     # --- Contrats de forme et de type\n",
    "#     D = int(Z_hat.shape[0])\n",
    "#     hd_assert_pm1(Z_hat, D)\n",
    "#     hd_assert_pm1(H_LM, D)\n",
    "#     assert isinstance(cand_vocab, (list, tuple)) and len(cand_vocab) > 0, \"cand_vocab vide\"\n",
    "\n",
    "#     # --- Matrices lexicales (V, D) en int8 (vectorisation)\n",
    "#     M_mem = _batch_lex(cand_vocab, L_mem)   # (V, D)\n",
    "#     M_lm  = _batch_lex(cand_vocab, L_lm)    # (V, D)\n",
    "#     assert M_mem.shape == M_lm.shape == (len(cand_vocab), D), \"Shapes (V,D) incoh\u00e9rents\"\n",
    "\n",
    "#     # --- Produits scalaires vectoris\u00e9s (int32 pour \u00e9viter overflow)\n",
    "#     z32  = Z_hat.astype(np.int32, copy=False)\n",
    "#     h32  = H_LM.astype(np.int32, copy=False)\n",
    "#     mem_scores = (M_mem.astype(np.int32, copy=False) @ z32)              # (V,)\n",
    "#     lm_scores  = (M_lm.astype(np.int32,  copy=False) @ h32)              # (V,)\n",
    "#     scores_raw = mem_scores.astype(np.float64) + float(lam) * lm_scores.astype(np.float64)\n",
    "\n",
    "#     # --- Argmax sur scores bruts (l'\u00e9chelle n'affecte pas l'argmax)\n",
    "#     best_idx   = int(np.argmax(scores_raw))\n",
    "#     token_star = cand_vocab[best_idx]\n",
    "\n",
    "#     # --- Option: probabilit\u00e9s (softmax stable) avec normalisation choisie\n",
    "#     probs = None\n",
    "#     if return_probs:\n",
    "#         if normalize == \"sqrtD\":\n",
    "#             logits = scores_raw / (np.sqrt(D) * max(1e-6, float(tau)))\n",
    "#         elif normalize == \"none\":\n",
    "#             logits = scores_raw / max(1e-6, float(tau))\n",
    "#         else:\n",
    "#             raise ValueError(\"normalize \u2208 {'none','sqrtD'} attendu\")\n",
    "#         logits = logits - np.max(logits)                  # stabilit\u00e9 num.\n",
    "#         exps   = np.exp(logits, dtype=np.float64)\n",
    "#         probs  = exps / np.sum(exps, dtype=np.float64)    # (V,)\n",
    "#         probs  = probs.astype(np.float64, copy=False)\n",
    "\n",
    "#     return token_star, scores_raw, probs\n",
    "\n",
    "# # -- Perplexit\u00e9 HD: softmax sur scores normalis\u00e9s par D -------------------------\n",
    "# def hd_perplexity(scores: np.ndarray, true_idx: int, D: int, tau: float = 1.0) -> float:\n",
    "#     \"\"\"\n",
    "#     Perplexit\u00e9 = exp( - log p(true) ), avec p \u221d exp( (scores/D)/tau ).\n",
    "#     On divise par D pour \u00e9viter des logits trop grands (HD).\n",
    "#     \"\"\"\n",
    "#     logits = scores / (D * max(1e-6, tau))\n",
    "#     logits = logits - np.max(logits)               # stabilit\u00e9\n",
    "#     exps = np.exp(logits)\n",
    "#     p = exps / np.sum(exps)\n",
    "#     p_true = float(max(p[true_idx], 1e-12))\n",
    "#     return float(np.exp(-np.log(p_true)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "514630c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _softmax_probs(scores: np.ndarray, D: int, tau: float = 1.0) -> np.ndarray:\n",
    "#     # Normalisation par sqrt(D) pour \u00e9viter la sur-concentration \u00e0 grande dimension\n",
    "#     s = scores / (np.sqrt(D) * tau)\n",
    "#     s = s - np.max(s)                       # stabilit\u00e9 num\u00e9rique\n",
    "#     exps = np.exp(s)\n",
    "#     return exps / np.sum(exps)\n",
    "\n",
    "# def hd_perplexity(scores: np.ndarray, true_index: int, D: int, tau: float = 1.0) -> float:\n",
    "#     p = _softmax_probs(scores, D=D, tau=tau)[true_index]\n",
    "#     # Perplexit\u00e9 = exp(-log p_y) ; born\u00e9e inf\u00e9rieurement par 1\n",
    "#     return float(np.exp(-np.log(max(p, 1e-12))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66f1b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def DX6_run_two_spaces(\n",
    "#     D: int = 16384, trials: int = 400,\n",
    "#     lam_grid=(0.0, 0.5, 1.0),\n",
    "#     # corr\u00e9lations du vrai token:\n",
    "#     sim_payload: float = 0.82,   # corr(Z_hat, L_mem(y))\n",
    "#     sim_lm: float      = 0.65,   # corr(H_LM, L_lm(y))\n",
    "#     # confondeurs:\n",
    "#     n_confounders: int = 6,\n",
    "#     rho_mem_conf: float = 0.72,  # corr(Z_hat, L_mem(conf))\n",
    "#     rho_lm_conf: float  = 0.05,  # corr(H_LM, L_lm(conf))\n",
    "#     tau: float = 1.0,\n",
    "#     rng_seed: int = 7031\n",
    "# ):\n",
    "#     g = np.random.default_rng(rng_seed)\n",
    "\n",
    "#     def rademacher(D):  # \u00b11/int8\n",
    "#         return g.choice(np.array([-1,1], dtype=np.int8), size=D)\n",
    "\n",
    "#     def correlated_pm1(proto: np.ndarray, rho: float) -> np.ndarray:\n",
    "#         noise = rademacher(proto.shape[0])\n",
    "#         mix = rho * proto.astype(np.int32) + (1-rho) * noise.astype(np.int32)\n",
    "#         return np.where(mix >= 0, 1, -1).astype(np.int8)\n",
    "\n",
    "#     def make_trial():\n",
    "#         Z_true  = rademacher(D)  # payload cible\n",
    "#         H_true  = rademacher(D)  # LM cible\n",
    "#         # Construire DEUX lexiques: L_mem (pour la m\u00e9moire) et L_lm (pour le LM)\n",
    "#         V = n_confounders + 1\n",
    "#         L_mem = np.empty((V, D), dtype=np.int8)\n",
    "#         L_lm  = np.empty((V, D), dtype=np.int8)\n",
    "#         # y (indice 0)\n",
    "#         L_mem[0] = correlated_pm1(Z_true, sim_payload)\n",
    "#         L_lm[0]  = correlated_pm1(H_true, sim_lm)\n",
    "#         # confondeurs\n",
    "#         for i in range(1, V):\n",
    "#             L_mem[i] = correlated_pm1(Z_true, rho_mem_conf)\n",
    "#             L_lm[i]  = correlated_pm1(H_true, rho_lm_conf)\n",
    "#         return L_mem, L_lm, 0, Z_true, H_true  # (lexiques, true_id, payload, LM)\n",
    "\n",
    "#     def vote_scores_two_lex(L_mem: np.ndarray, L_lm: np.ndarray,\n",
    "#                             Z_hat: np.ndarray, H_LM: np.ndarray, lam: float) -> np.ndarray:\n",
    "#         # int32 pour \u00e9viter overflow ; (V,D) @ (D,) -> (V,)\n",
    "#         return (L_mem.astype(np.int32) @ Z_hat.astype(np.int32)) + \\\n",
    "#                lam * (L_lm.astype(np.int32)  @ H_LM.astype(np.int32))\n",
    "\n",
    "#     def _softmax_probs(scores: np.ndarray, D: int, tau: float = 1.0) -> np.ndarray:\n",
    "#         s = scores / (np.sqrt(D) * max(tau, 1e-6))\n",
    "#         s = s - np.max(s)\n",
    "#         exps = np.exp(s)\n",
    "#         return exps / np.sum(exps)\n",
    "\n",
    "#     def hd_perplexity(scores: np.ndarray, true_idx: int, D: int, tau: float = 1.0) -> float:\n",
    "#         p_true = float(_softmax_probs(scores, D=D, tau=tau)[true_idx])\n",
    "#         return float(np.exp(-np.log(max(p_true, 1e-12))))\n",
    "\n",
    "#     stats = {lam: {\"top1_hits\": 0, \"ppl_sum\": 0.0} for lam in lam_grid}\n",
    "\n",
    "#     for _ in range(trials):\n",
    "#         L_mem, L_lm, true_idx, Z_true, H_true = make_trial()\n",
    "#         Z_hat = Z_true; H_LM = H_true\n",
    "#         for lam in lam_grid:\n",
    "#             scores = vote_scores_two_lex(L_mem, L_lm, Z_hat, H_LM, float(lam))\n",
    "#             pred = int(np.argmax(scores))\n",
    "#             stats[lam][\"top1_hits\"] += 1 if pred == true_idx else 0\n",
    "#             stats[lam][\"ppl_sum\"]   += hd_perplexity(scores, true_idx, D, tau)\n",
    "\n",
    "#     results = {lam: {\"top1\": stats[lam][\"top1_hits\"]/trials,\n",
    "#                      \"ppl\":  stats[lam][\"ppl_sum\"]/trials}\n",
    "#                for lam in lam_grid}\n",
    "\n",
    "#     base_top1, base_ppl = results[0.0][\"top1\"], results[0.0][\"ppl\"]\n",
    "#     saturated = (abs(base_top1 - 1.0) < 1e-12)\n",
    "\n",
    "#     log.info((\"DX6(2-spaces|fixed) \u2014 D=%d, trials=%d, conf=%d, \u03c1_mem(conf)=%.2f, \u03c1_lm(conf)=%.2f, \"\n",
    "#               \"sim_payload=%.2f, sim_lm=%.2f\"),\n",
    "#               D, trials, n_confounders, rho_mem_conf, rho_lm_conf, sim_payload, sim_lm)\n",
    "#     for lam in lam_grid:\n",
    "#         log.info(\"  lambda=%.2f  ->  top-1=%.3f | ppl=%.3f\", lam, results[lam][\"top1\"], results[lam][\"ppl\"])\n",
    "\n",
    "#     if saturated:\n",
    "#         ok = any(results[lam][\"ppl\"] < base_ppl - 1e-12 for lam in lam_grid if lam != 0.0)\n",
    "#         assert ok, \"DX6(fixed): r\u00e9gime satur\u00e9 \u2014 aucune baisse de perplexit\u00e9 vs \u03bb=0.\"\n",
    "#     else:\n",
    "#         ok = any((results[lam][\"top1\"] > base_top1 + 1e-12) and (results[lam][\"ppl\"] < base_ppl - 1e-12)\n",
    "#                  for lam in lam_grid if lam != 0.0)\n",
    "#         assert ok, \"DX6(fixed): aucun \u03bb n'am\u00e9liore simultan\u00e9ment top-1 ET perplexit\u00e9 vs \u03bb=0.\"\n",
    "\n",
    "#     log.info(\"DX6(fixed) \u2014 CA VALID\u00c9 (%s).\", \"satur\u00e9\" if saturated else \"non-satur\u00e9\")\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02b7d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softmax_probs(scores: np.ndarray, D: int, tau: float = 1.0) -> np.ndarray:\n",
    "    s = scores / (np.sqrt(D) * max(float(tau), 1e-6))\n",
    "    s = s - np.max(s)\n",
    "    exps = np.exp(s, dtype=np.float64)\n",
    "    return exps / np.sum(exps, dtype=np.float64)\n",
    "\n",
    "def hd_perplexity_from_scores(scores: np.ndarray, true_idx: int, D: int, tau: float = 1.0) -> float:\n",
    "    p_true = float(_softmax_probs(scores, D=D, tau=tau)[true_idx])\n",
    "    return float(np.exp(-np.log(max(p_true, 1e-12))))\n",
    "\n",
    "# --- G\u00e9n\u00e9ration contr\u00f4l\u00e9e: on impose une similarit\u00e9 cible ~ rho par flips coordonn\u00e9s  ----\n",
    "def flip_to_target(vec: np.ndarray, target_sim: float, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pour \u00b11, si p_flip = (1 - target_sim)/2 alors E[sim] = 1 - 2*p_flip = target_sim.\n",
    "    \"\"\"\n",
    "    D = vec.shape[0]\n",
    "    p_flip = max(0.0, min(1.0, (1.0 - float(target_sim)) / 2.0))\n",
    "    mask  = (rng.random(D) < p_flip).astype(np.int8)  # 1 si on flippe\n",
    "    flips = (1 - 2 * mask).astype(np.int8, copy=False)  # 1->-1 quand mask=1\n",
    "    return (vec.astype(np.int8, copy=False) * flips).astype(np.int8, copy=False)\n",
    "\n",
    "# --- DD6_vote (version vectoris\u00e9e, 2 espaces) -----------------------------------------\n",
    "def _batch_lex(cand_vocab, L):\n",
    "    mats = []\n",
    "    for v in cand_vocab:\n",
    "        vec = L(v).astype(np.int8, copy=False)\n",
    "        mats.append(vec)\n",
    "    return np.vstack(mats).astype(np.int8, copy=False)\n",
    "\n",
    "def DD6_vote(\n",
    "    Z_hat: np.ndarray,\n",
    "    H_LM: np.ndarray,\n",
    "    L_mem,                  # callable: v -> \u00b11 int8 (D,)\n",
    "    L_lm,                   # callable: v -> \u00b11 int8 (D,)\n",
    "    cand_vocab: list[str],\n",
    "    lam: float = 0.0,\n",
    "    *,\n",
    "    normalize: str = \"sqrtD\",   # {\"none\",\"sqrtD\"}\n",
    "    return_probs: bool = False,\n",
    "    tau: float = 1.0\n",
    ") -> tuple[str, np.ndarray, np.ndarray | None]:\n",
    "    D = int(Z_hat.shape[0])\n",
    "    hd_assert_pm1(Z_hat, D); hd_assert_pm1(H_LM, D)\n",
    "    assert isinstance(cand_vocab, (list, tuple)) and len(cand_vocab) > 0, \"cand_vocab vide\"\n",
    "    M_mem = _batch_lex(cand_vocab, L_mem)   # (V, D)\n",
    "    M_lm  = _batch_lex(cand_vocab, L_lm)    # (V, D)\n",
    "    assert M_mem.shape == M_lm.shape == (len(cand_vocab), D), \"Shapes (V,D) incoh\u00e9rents\"\n",
    "\n",
    "    z32 = Z_hat.astype(np.int32, copy=False)\n",
    "    h32 = H_LM.astype(np.int32, copy=False)\n",
    "    scores_raw = (M_mem.astype(np.int32, copy=False) @ z32).astype(np.float64) \\\n",
    "               + float(lam) * (M_lm.astype(np.int32, copy=False) @ h32).astype(np.float64)\n",
    "\n",
    "    best_idx   = int(np.argmax(scores_raw))\n",
    "    token_star = cand_vocab[best_idx]\n",
    "\n",
    "    probs = None\n",
    "    if return_probs:\n",
    "        if normalize == \"sqrtD\":\n",
    "            logits = scores_raw / (np.sqrt(D) * max(1e-6, float(tau)))\n",
    "        elif normalize == \"none\":\n",
    "            logits = scores_raw / max(1e-6, float(tau))\n",
    "        else:\n",
    "            raise ValueError(\"normalize \u2208 {'none','sqrtD'}\")\n",
    "        logits = logits - np.max(logits)\n",
    "        exps   = np.exp(logits, dtype=np.float64)\n",
    "        probs  = (exps / np.sum(exps, dtype=np.float64)).astype(np.float64, copy=False)\n",
    "\n",
    "    return token_star, scores_raw, probs\n",
    "\n",
    "# --- DX6_run: simulation 2-espaces + mesure top-1 & perplexit\u00e9 -----------------------\n",
    "def DX6_run(\n",
    "    D: int = 16384, trials: int = 400,\n",
    "    lam_grid=(0.0, 0.5, 1.0),\n",
    "    # corr\u00e9lations du vrai token:\n",
    "    sim_payload: float = 0.60,   # corr(Z_hat, L_mem(y))  \u2014 plus bas pour \u00e9viter saturation\n",
    "    sim_lm: float      = 0.40,   # corr(H_LM, L_lm(y))\n",
    "    # confondeurs:\n",
    "    n_confounders: int = 6,\n",
    "    rho_mem_conf: float = 0.55,  # corr(Z_hat, L_mem(conf)) < sim_payload mais proche\n",
    "    rho_lm_conf: float  = 0.10,  # corr(H_LM, L_lm(conf))  << sim_lm\n",
    "    tau: float = 1.0,\n",
    "    rng_seed: int = 7031\n",
    "):\n",
    "    \"\"\"\n",
    "    \u00c9value DD6_vote avec deux lexiques ind\u00e9pendants (m\u00e9moire & LM).\n",
    "    - R\u00e9gime par d\u00e9faut: NON SATUR\u00c9 (sim_payload ~ 0.60, conf proche 0.55).\n",
    "    Crit\u00e8re:\n",
    "      - Si top-1(\u03bb=0) < 1.0 (non satur\u00e9): \u2203 \u03bb>0 tel que top-1 \u2191 ET perplexit\u00e9 \u2193.\n",
    "      - Sinon (satur\u00e9): \u2203 \u03bb>0 tel que perplexit\u00e9 \u2193.\n",
    "    \"\"\"\n",
    "    g = np.random.default_rng(rng_seed)\n",
    "\n",
    "    stats = {lam: {\"top1_hits\": 0, \"ppl_sum\": 0.0} for lam in lam_grid}\n",
    "\n",
    "    for _ in range(trials):\n",
    "        # Prototypes vrais\n",
    "        Z_true = rademacher(D, g)   # payload seuill\u00e9\n",
    "        H_true = rademacher(D, g)   # LM courant\n",
    "\n",
    "        # Vocabulaire (strings) : y + confondeurs\n",
    "        V = n_confounders + 1\n",
    "        cand_vocab = [f\"tok{i}\" for i in range(V)]\n",
    "        true_tok   = cand_vocab[0]\n",
    "\n",
    "        # Construit des tables (dictionnaires) pour L_mem et L_lm\n",
    "        table_mem: dict[str, np.ndarray] = {}\n",
    "        table_lm:  dict[str, np.ndarray] = {}\n",
    "        # Vrai token\n",
    "        table_mem[true_tok] = flip_to_target(Z_true, sim_payload, g)\n",
    "        table_lm[true_tok]  = flip_to_target(H_true, sim_lm,      g)\n",
    "        # Conf:\n",
    "        for i in range(1, V):\n",
    "            ti = cand_vocab[i]\n",
    "            table_mem[ti] = flip_to_target(Z_true, rho_mem_conf, g)\n",
    "            table_lm[ti]  = flip_to_target(H_true, rho_lm_conf,  g)\n",
    "\n",
    "        # Callables lexicaux pour DD6_vote\n",
    "        def L_mem(v: str) -> np.ndarray: return table_mem[v]\n",
    "        def L_lm(v: str)  -> np.ndarray: return table_lm[v]\n",
    "\n",
    "        # Vote pour chaque lambda\n",
    "        for lam in lam_grid:\n",
    "            token_star, scores, probs = DD6_vote(\n",
    "                Z_hat=Z_true, H_LM=H_true,\n",
    "                L_mem=L_mem, L_lm=L_lm,\n",
    "                cand_vocab=cand_vocab,\n",
    "                lam=float(lam),\n",
    "                normalize=\"sqrtD\", return_probs=True, tau=tau\n",
    "            )\n",
    "            pred_is_true = 1 if token_star == true_tok else 0\n",
    "            stats[lam][\"top1_hits\"] += pred_is_true\n",
    "\n",
    "            # Perplexit\u00e9 HD (si probs non None, on l'utilise directement)\n",
    "            if probs is not None:\n",
    "                true_idx = 0\n",
    "                p_true = float(max(probs[true_idx], 1e-12))\n",
    "                ppl = float(np.exp(-np.log(p_true)))\n",
    "            else:\n",
    "                ppl = hd_perplexity_from_scores(scores, true_idx=0, D=D, tau=tau)\n",
    "            stats[lam][\"ppl_sum\"] += ppl\n",
    "\n",
    "    results = {\n",
    "        lam: {\"top1\": stats[lam][\"top1_hits\"]/trials,\n",
    "              \"ppl\":  stats[lam][\"ppl_sum\"]/trials}\n",
    "        for lam in lam_grid\n",
    "    }\n",
    "\n",
    "    # Logging des r\u00e9sultats\n",
    "    log.info((\"DX6 \u2014 D=%d, trials=%d, conf=%d, \"\n",
    "              \"\u03c1_mem(conf)=%.2f, \u03c1_lm(conf)=%.2f, sim_payload=%.2f, sim_lm=%.2f\"),\n",
    "             D, trials, n_confounders, rho_mem_conf, rho_lm_conf, sim_payload, sim_lm)\n",
    "    for lam in lam_grid:\n",
    "        log.info(\"  lambda=%.2f  ->  top-1=%.3f | ppl=%.3f\",\n",
    "                 float(lam), results[lam][\"top1\"], results[lam][\"ppl\"])\n",
    "\n",
    "    # Crit\u00e8re d'acceptation (bi-r\u00e9gime)\n",
    "    base_top1, base_ppl = results[0.0][\"top1\"], results[0.0][\"ppl\"]\n",
    "    saturated = (abs(base_top1 - 1.0) < 1e-12)\n",
    "    if saturated:\n",
    "        ok = any(results[lam][\"ppl\"] < base_ppl - 1e-12 for lam in lam_grid if lam != 0.0)\n",
    "        assert ok, \"DX6: r\u00e9gime satur\u00e9 \u2014 aucune baisse de perplexit\u00e9 vs \u03bb=0.\"\n",
    "    else:\n",
    "        ok = any((results[lam][\"top1\"] > base_top1 + 1e-12) and\n",
    "                 (results[lam][\"ppl\"] < base_ppl - 1e-12)\n",
    "                 for lam in lam_grid if lam != 0.0)\n",
    "        assert ok, \"DX6: aucun \u03bb n'am\u00e9liore simultan\u00e9ment top-1 ET perplexit\u00e9 vs \u03bb=0.\"\n",
    "    log.info(\"DX6 \u2014 CA VALID\u00c9 (%s).\", \"satur\u00e9\" if saturated else \"non-satur\u00e9\")\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40afdb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 23:00:52,893 [INFO] DX6 \u2014 D=16384, trials=400, conf=6, \u03c1_mem(conf)=0.55, \u03c1_lm(conf)=0.10, sim_payload=0.60, sim_lm=0.40\n",
      "2025-10-06 23:00:52,893 [INFO]   lambda=0.00  ->  top-1=1.000 | ppl=1.019\n",
      "2025-10-06 23:00:52,894 [INFO]   lambda=0.50  ->  top-1=1.000 | ppl=1.000\n",
      "2025-10-06 23:00:52,894 [INFO]   lambda=1.00  ->  top-1=1.000 | ppl=1.000\n",
      "2025-10-06 23:00:52,894 [INFO] DX6 \u2014 CA VALID\u00c9 (satur\u00e9).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.0: {'top1': 1.0, 'ppl': 1.0192223749961116},\n",
       " 0.5: {'top1': 1.0, 'ppl': 1.0000000001159797},\n",
       " 1.0: {'top1': 1.0, 'ppl': 1.0}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DX6_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a822a58a",
   "metadata": {},
   "source": [
    "# DD7 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "918aab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Hypers s\u00fbrs par d\u00e9faut -----------------------------------------------------\n",
    "DEFAULT_ELL_GRID = (2, 4, 8, 12)\n",
    "CONF_PER_STEP    = 8          # nb. de confondeurs par pas t\n",
    "TRIALS           = 200        # nb. de s\u00e9quences ind\u00e9pendantes (moyennage)\n",
    "T_STEPS          = 24         # longueur d'une s\u00e9quence\n",
    "SIM_Y_MEM        = 0.70       # corr(H_true(ell), L_fr(y_t)) attendue (oracle)\n",
    "SIM_CONF_LM      = 0.05       # confondeurs faiblement corr\u00e9l\u00e9s au LM\n",
    "D                = 16_384     # dimension HD (isom\u00e9trie stable)\n",
    "RNG_SEED         = 9_117\n",
    "\n",
    "# -- Utilitaires HDC (contrats d\u00e9j\u00e0 d\u00e9finis ailleurs) ---------------------------\n",
    "def correlated_pm1(proto: np.ndarray, rho: float, rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"Retourne \u00b11 corr\u00e9l\u00e9 \u00e0 proto avec corr\u00e9lation ~rho (approx. en grande D).\"\"\"\n",
    "    noise = rademacher(proto.shape[0], rng)\n",
    "    mix = rho * proto.astype(np.int32) + (1 - rho) * noise.astype(np.int32)\n",
    "    return np.where(mix >= 0, 1, -1).astype(np.int8)\n",
    "\n",
    "def DD7_updateLM(H_LM: np.ndarray, v_hat: str, L_fr, Pi: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"H_LM' = sign( H_LM + \u03a0^1 L_fr(v_hat) ) avec sign strict (0->+1).\"\"\"\n",
    "    D = H_LM.shape[0]\n",
    "    Lv = L_fr(v_hat).astype(np.int8, copy=False)\n",
    "    inc = permute_pow(Lv, Pi, 1).astype(np.int16, copy=False)\n",
    "    acc = H_LM.astype(np.int16) + inc\n",
    "    return sign_strict_pm1(acc)\n",
    "\n",
    "# -- G\u00e9n\u00e9ration d'une s\u00e9quence et \u00e9valuation pour un ell donn\u00e9 ------------------\n",
    "def DX7_eval_one_ell(ell: int, Pi: np.ndarray, L_fr, rng: np.random.Generator) -> tuple[float, float]:\n",
    "    \"\"\"Retourne (top1, p_ell) moyens sur TRIALS x T_STEPS avec confondeurs.\"\"\"\n",
    "    top1_hits = 0\n",
    "    p_sum     = 0.0\n",
    "    D = Pi.shape[0]\n",
    "\n",
    "    vocab = [f\"tok_{i}\" for i in range(CONF_PER_STEP + 1)]\n",
    "    Lsym = ToyLexFR(vocab=vocab, D=D, seed=int(rng.integers(1, 2**31 - 1)))\n",
    "\n",
    "    for _ in range(TRIALS):\n",
    "        hist_true = []\n",
    "        H_LM_pred = rademacher(D, rng)\n",
    "        for t in range(T_STEPS):\n",
    "            if len(hist_true) < ell:\n",
    "                H_true = rademacher(D, rng)\n",
    "            else:\n",
    "                acc = np.zeros(D, dtype=np.int32)\n",
    "                for j in range(1, ell + 1):\n",
    "                    acc += permute_pow(Lsym(hist_true[-j]), Pi, j).astype(np.int32)\n",
    "                H_true = sign_strict_pm1(acc)\n",
    "\n",
    "            y = vocab[0]\n",
    "            L_y = correlated_pm1(H_true, SIM_Y_MEM, rng)\n",
    "\n",
    "            cand_vectors = [L_y]\n",
    "            cand_tokens  = [y]\n",
    "            for k in range(CONF_PER_STEP):\n",
    "                v = vocab[k + 1]\n",
    "                L_v = correlated_pm1(H_true, SIM_CONF_LM, rng)\n",
    "                cand_vectors.append(L_v)\n",
    "                cand_tokens.append(v)\n",
    "            cand_vectors = np.stack(cand_vectors, axis=0)\n",
    "\n",
    "            scores = cand_vectors.astype(np.int32) @ H_LM_pred.astype(np.int32)\n",
    "            pred_idx = int(np.argmax(scores))\n",
    "            v_hat = cand_tokens[pred_idx]\n",
    "            top1_hits += 1 if pred_idx == 0 else 0\n",
    "\n",
    "            sim = hd_sim(H_true, L_y)\n",
    "            p_sum += 0.5 * (1.0 + sim)\n",
    "\n",
    "            hist_true.append(y)\n",
    "            if len(hist_true) > ell:\n",
    "                hist_true.pop(0)\n",
    "            L_fr_temp = lambda token: cand_vectors[cand_tokens.index(token)]\n",
    "            H_LM_pred = DD7_updateLM(H_LM_pred, v_hat=v_hat, L_fr=L_fr_temp, Pi=Pi)\n",
    "\n",
    "    total = TRIALS * T_STEPS\n",
    "    top1 = top1_hits / total\n",
    "    p_ell = p_sum / total\n",
    "    return top1, p_ell\n",
    "\n",
    "# -- Exp\u00e9rience principale DX7 --------------------------------------------------\n",
    "def DX7_run(\n",
    "    ell_grid=DEFAULT_ELL_GRID,\n",
    "    D: int = D,\n",
    "    seed_pi: int = 10_456,\n",
    "    rng_seed: int = RNG_SEED\n",
    "):\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    Pi = np.arange(D, dtype=np.int64)\n",
    "    rng.shuffle(Pi)\n",
    "    results = {}\n",
    "    log.info(\"DX7 \u2014 \u00e9tude fenetre ell=%s (D=%d, trials=%d, T=%d, conf/step=%d)\",\n",
    "             ell_grid, D, TRIALS, T_STEPS, CONF_PER_STEP)\n",
    "    for ell in ell_grid:\n",
    "        top1, p_ell = DX7_eval_one_ell(ell=ell, Pi=Pi, L_fr=None, rng=rng)\n",
    "        results[int(ell)] = {\"top1\": top1, \"p\": p_ell}\n",
    "        log.info(\"  ell=%2d  ->  top-1=%.3f | p(ell)=%.3f\", ell, top1, p_ell)\n",
    "\n",
    "    ells = sorted(results.keys())\n",
    "    top1s = np.array([results[e][\"top1\"] for e in ells], dtype=np.float64)\n",
    "    ps    = np.array([results[e][\"p\"]    for e in ells], dtype=np.float64)\n",
    "\n",
    "    ell_star = ells[int(np.argmax(top1s))]\n",
    "    tail = ps[ells.index(ell_star):]\n",
    "    nonincreasing_tail = np.all(tail[:-1] >= tail[1:] - 1e-9)\n",
    "\n",
    "    assert nonincreasing_tail, \"DX7: p(ell) ne d\u00e9cro\u00eet pas au-del\u00e0 de ell* (dilution attendue de la majorit\u00e9).\"\n",
    "    log.info(\"DX7 \u2014 CA VALID\u00c9S: (i) ell*=%d maximise top-1 ; (ii) p(ell) d\u00e9cro\u00eet au-del\u00e0.\", ell_star)\n",
    "    return results, ell_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a20a3fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 23:23:12,336 [INFO] DX7 \u2014 \u00e9tude fenetre ell=(2, 4, 8, 12) (D=16384, trials=200, T=24, conf/step=8)\n",
      "2025-10-06 23:23:19,330 [INFO]   ell= 2  ->  top-1=0.926 | p(ell)=1.000\n",
      "2025-10-06 23:23:26,541 [INFO]   ell= 4  ->  top-1=0.854 | p(ell)=1.000\n",
      "2025-10-06 23:23:34,829 [INFO]   ell= 8  ->  top-1=0.707 | p(ell)=1.000\n",
      "2025-10-06 23:23:43,466 [INFO]   ell=12  ->  top-1=0.555 | p(ell)=1.000\n",
      "2025-10-06 23:23:43,467 [INFO] DX7 \u2014 CA VALID\u00c9S: (i) ell*=2 maximise top-1 ; (ii) p(ell) d\u00e9cro\u00eet au-del\u00e0.\n"
     ]
    }
   ],
   "source": [
    "results, ell_star = DX7_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70cf6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Union, Tuple\n",
    "\n",
    "def _as_vocab_from_buckets(\n",
    "    C_K: np.ndarray,\n",
    "    bucket2vocab: Optional[Union[dict[int, list[str]], Callable[[int], list[str]]]],\n",
    "    history_fr: list[str],\n",
    "    global_fallback_vocab: Optional[list[str]],\n",
    "    min_size: int = 1\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Construit un vocab candidat \u00e0 partir des indices de buckets C_K, avec repli sur:\n",
    "    - historique (pour ne pas renvoyer vide),\n",
    "    - vocab global si fourni.\n",
    "    D\u00e9duplique en conservant l'ordre (top-K prioritaire).\n",
    "    \"\"\"\n",
    "    cand: list[str] = []\n",
    "    seen = set()\n",
    "\n",
    "    def add_many(lst: list[str]):\n",
    "        for t in lst:\n",
    "            if t not in seen:\n",
    "                seen.add(t)\n",
    "                cand.append(t)\n",
    "\n",
    "    if bucket2vocab is not None:\n",
    "        for c in C_K:\n",
    "            toks = bucket2vocab(c) if callable(bucket2vocab) else bucket2vocab.get(int(c), [])\n",
    "            if toks:\n",
    "                add_many(toks)\n",
    "\n",
    "    if len(cand) < min_size and history_fr:\n",
    "        add_many(list(history_fr))\n",
    "\n",
    "    if len(cand) < min_size and global_fallback_vocab is not None:\n",
    "        add_many(list(global_fallback_vocab))\n",
    "\n",
    "    if len(cand) < min_size:\n",
    "        cand = [\"<unk>\"]\n",
    "    return cand\n",
    "\n",
    "\n",
    "def DecodeOneStep(\n",
    "    Hs: np.ndarray,\n",
    "    H_LM: np.ndarray,\n",
    "    history_fr: list[str],\n",
    "    G_DEC: np.ndarray,\n",
    "    G_MEM: np.ndarray,\n",
    "    Pi: np.ndarray,\n",
    "    L_fr: Callable[[str], np.ndarray],\n",
    "    prototypes: np.ndarray,\n",
    "    K: int = 500,\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 1.0,\n",
    "    ell: int = 4,\n",
    "    lam: float = 0.5,\n",
    "    bucket2vocab: Optional[Union[dict[int, list[str]], Callable[[int], list[str]]]] = None,\n",
    "    global_fallback_vocab: Optional[list[str]] = None,\n",
    "    return_ck_scores: bool = True\n",
    ") -> Tuple[str, np.ndarray, int, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Pipeline DEC (pas de d\u00e9codage complet) :\n",
    "      DD1 -> DD7 avec candidatures lexicales optionnelles depuis bucket2vocab.\n",
    "\n",
    "    Retourne (token*, scores_cand, c_star, C_K, scores_CK) si return_ck_scores=True,\n",
    "    sinon remplace scores_CK par l'\u00e9tat LM mis \u00e0 jour H_LM_next.\n",
    "    \"\"\"\n",
    "    D = Hs.shape[0]\n",
    "    hd_assert_pm1(Hs, D)\n",
    "    hd_assert_pm1(H_LM, D)\n",
    "    hd_assert_pm1(G_DEC, D)\n",
    "    hd_assert_pm1(G_MEM, D)\n",
    "    assert Pi.ndim == 1 and Pi.shape[0] == D and np.issubdtype(Pi.dtype, np.integer), \"Pi invalide\"\n",
    "    assert prototypes.ndim == 2 and prototypes.shape[1] == D, \"prototypes de forme (B,D)\"\n",
    "\n",
    "    Qs = DD1_ctx(Hs, G_DEC)\n",
    "    Rt = DD2_query_bin(Qs, history_fr, L_fr, Pi, alpha=alpha, beta=beta, ell=ell)\n",
    "    Rt_tilde = DD3_bindToMem(Rt, G_MEM)\n",
    "    c_star, C_K, scores_CK = DD4_search_topK(Rt_tilde, prototypes, K)\n",
    "    Z_hat = DD5_payload(prototypes[c_star])\n",
    "\n",
    "    cand_vocab = _as_vocab_from_buckets(\n",
    "        C_K=C_K,\n",
    "        bucket2vocab=bucket2vocab,\n",
    "        history_fr=history_fr,\n",
    "        global_fallback_vocab=global_fallback_vocab,\n",
    "        min_size=1\n",
    "    )\n",
    "\n",
    "    token_star, scores_cand, _ = DD6_vote(\n",
    "        Z_hat,\n",
    "        H_LM,\n",
    "        L_mem=L_fr,\n",
    "        L_lm=L_fr,\n",
    "        cand_vocab=cand_vocab,\n",
    "        lam=lam\n",
    "    )\n",
    "\n",
    "    H_LM_next = DD7_updateLM(H_LM, token_star, L_fr, Pi)\n",
    "\n",
    "    if return_ck_scores:\n",
    "        return token_star, scores_cand, int(c_star), C_K, scores_CK\n",
    "    return token_star, scores_cand, int(c_star), C_K, H_LM_next\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0a8cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_L_fr(vocab_seed: int, D: int):\n",
    "    rng = np.random.default_rng(vocab_seed)\n",
    "    table = {}\n",
    "\n",
    "    def get(tok: str) -> np.ndarray:\n",
    "        if tok not in table:\n",
    "            x = rng.integers(0, 2, size=D, dtype=np.int8)\n",
    "            table[tok] = (2 * x - 1).astype(np.int8)\n",
    "        return table[tok]\n",
    "\n",
    "    return get\n",
    "\n",
    "def test_isometry_and_flow():\n",
    "    D = 16384\n",
    "    K = 128\n",
    "    rng = np.random.default_rng(7)\n",
    "    Hs = (2 * rng.integers(0, 2, size=D, dtype=np.int8) - 1)\n",
    "    H_LM = (2 * rng.integers(0, 2, size=D, dtype=np.int8) - 1)\n",
    "    G_DEC = (2 * rng.integers(0, 2, size=D, dtype=np.int8) - 1)\n",
    "    G_MEM = (2 * rng.integers(0, 2, size=D, dtype=np.int8) - 1)\n",
    "    Pi = rng.permutation(D).astype(np.int64)\n",
    "    Lfr = mock_L_fr(1234, D)\n",
    "    B = 2048\n",
    "    prototypes = (2 * rng.integers(0, 2, size=(B, D), dtype=np.int8) - 1)\n",
    "\n",
    "    tok, scores, c_star, CK, H_LM_next = DecodeOneStep(\n",
    "        Hs,\n",
    "        H_LM,\n",
    "        history_fr=[\"de\", \"la\", \"musique\"],\n",
    "        G_DEC=G_DEC,\n",
    "        G_MEM=G_MEM,\n",
    "        Pi=Pi,\n",
    "        L_fr=Lfr,\n",
    "        prototypes=prototypes,\n",
    "        K=K,\n",
    "        return_ck_scores=False\n",
    "    )\n",
    "\n",
    "    assert isinstance(tok, str) and scores.ndim == 1\n",
    "    assert H_LM_next.shape == (D,) and H_LM_next.dtype == np.int8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db5fa07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_isometry_and_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587fb688",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b983cd28",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a31ddf",
   "metadata": {},
   "source": [
    "# Import Liraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41860409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aymenmejri/Desktop/MyCode/experiments/hdc_v2/hdc_project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import math\n",
    "from math import log, ceil\n",
    "from typing import Dict, Iterable, List, Tuple, Optional, Any, Sequence\n",
    "from datasets import load_dataset  # pip install datasets\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import inspect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4accbb66",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "1. Architecture générale encoder--decoder HDC\n",
    "    - M0 : RNG, Clé Rademarcher unique et lot de vecteurs Rademarcher \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce91f4ab",
   "metadata": {},
   "source": [
    "# Architecture générale encoder--decoder HDC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a0bdb",
   "metadata": {},
   "source": [
    "## M0 : RNG, Clé Rademarcher unique et lot de vecteurs Rademarcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c98c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rng(seed: int | None) -> np.random.Generator:\n",
    "    \"\"\"\n",
    "    Crée un générateur PCG64 (si seed donné) ou un RNG par défaut (non déterministe).\n",
    "    \"\"\"\n",
    "    return np.random.Generator(np.random.PCG64(seed)) if seed is not None else np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519cc364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M0_NewKey(seed: int, D: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Génère une clé Rademacher J ∈ {-1,+1}^D (int8), déterministe par seed.\n",
    "    \"\"\"\n",
    "    g = _rng(seed)\n",
    "    B = g.integers(0, 2, size=D, dtype=np.int8)  # {0,1}\n",
    "    J = (B << 1) - 1                              # {-1,+1}\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f063f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M0_Rad(n: int, D: int, seed: int | None = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Génère un lot de n vecteurs Rademacher (n, D) en int8. Reproductible si seed fourni.\n",
    "    \"\"\"\n",
    "    g = _rng(seed)\n",
    "    B = g.integers(0, 2, size=(n, D), dtype=np.int8)\n",
    "    return (B << 1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df1eecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simhd(X: np.ndarray, Y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Similarité Hamming signée normalisée: sim(X,Y) = (1/D) <X, Y>.\n",
    "    Gère X,Y en int8 via upcast en int32 pour la stabilité.\n",
    "    \"\"\"\n",
    "    return float(np.dot(X.astype(np.int32), Y.astype(np.int32)) / X.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71050fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_M0(D: int = 16384, n_pairs: int = 10_000, eps: float = 0.05, seed: int = 123) -> dict:\n",
    "    \"\"\"\n",
    "    Vérifie (HA2) empiriquement:\n",
    "      - moyenne des simhd(J,J') ≈ 0\n",
    "      - fréquence empirique P(|sim| > eps) <= 1e-4 (à D=16384, eps=0.05)\n",
    "      - compare à la borne de Hoeffding 2*exp(-D*eps^2/2)\n",
    "    \"\"\"\n",
    "    g = _rng(seed)\n",
    "    sims = np.empty(n_pairs, dtype=np.float64)\n",
    "    for k in range(n_pairs):\n",
    "        j_seed  = int(g.integers(0, 2**31 - 1))\n",
    "        jp_seed = int(g.integers(0, 2**31 - 1))\n",
    "        J  = M0_NewKey(j_seed,  D)\n",
    "        Jp = M0_NewKey(jp_seed, D)\n",
    "        sims[k] = simhd(J, Jp)\n",
    "\n",
    "    mean_sim   = float(sims.mean())\n",
    "    tail_prob  = float((np.abs(sims) > eps).mean())\n",
    "    hoeff_bound = 2.0 * math.exp(- D * (eps**2) / 2.0)\n",
    "    return {\n",
    "        \"D\": D, \"n_pairs\": n_pairs, \"eps\": eps,\n",
    "        \"mean_sim\": mean_sim, \"tail_prob\": tail_prob, \"hoeffding\": hoeff_bound\n",
    "    }\n",
    "\n",
    "# Exemples d'assertions (à activer dans un vrai test runner):\n",
    "r = test_M0()\n",
    "assert abs(r[\"mean_sim\"]) <= 5e-3, f\"mean={r['mean_sim']:.4g} trop éloigné de 0\"\n",
    "assert r[\"tail_prob\"] <= 1e-4 + 1e-6, f\"queue empirique {r['tail_prob']:.4g} > 1e-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad57b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M0_min_D(eps: float, delta: float) -> int:\n",
    "    \"\"\"\n",
    "    Retourne D* tel que 2*exp(-D*eps^2/2) <= delta  =>  D >= 2/eps^2 * log(2/delta).\n",
    "    \"\"\"\n",
    "    assert eps > 0 and 0 < delta < 1, \"Paramètres invalides\"\n",
    "    return int(ceil((2.0 / (eps * eps)) * log(2.0 / delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc840d9",
   "metadata": {},
   "source": [
    "## M1 : Similarité et métrique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9386893",
   "metadata": {},
   "source": [
    "Il faudrait voir par la suite si on ne pourrait pas utiliser **simhd** défini précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd7db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_vec(a: np.ndarray) -> np.ndarray:\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim != 1:\n",
    "        raise ValueError(f\"attendu un vecteur 1D, shape={a.shape}\")\n",
    "    return a\n",
    "\n",
    "def M1_sim(X: np.ndarray, Y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Similarité HDC normalisée: (1/D) <X, Y>.\n",
    "    - X, Y: vecteurs 1D longueur D, dtypes int8/int32/float32/float64\n",
    "    - Retour: float64\n",
    "    \"\"\"\n",
    "    X = _as_vec(X); Y = _as_vec(Y)\n",
    "    if X.shape != Y.shape:\n",
    "        raise ValueError(f\"shapes incompatibles: {X.shape} vs {Y.shape}\")\n",
    "    # cast unique si int8 -> int32, sinon réutilise le buffer\n",
    "    Xv = X.astype(np.int32, copy=False) if X.dtype == np.int8 else X\n",
    "    Yv = Y.astype(np.int32, copy=False) if Y.dtype == np.int8 else Y\n",
    "    D  = Xv.shape[0]\n",
    "    return float(np.dot(Xv, Yv) / D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a999ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M1_dH(X: np.ndarray, Y: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Distance de Hamming (nombre de coordonnées différentes).\n",
    "    Si X,Y ∈ {-1,+1}^D, identité: dH = D/2 * (1 - sim).\n",
    "    \"\"\"\n",
    "    X = _as_vec(X); Y = _as_vec(Y)\n",
    "    if X.shape != Y.shape:\n",
    "        raise ValueError(f\"shapes incompatibles: {X.shape} vs {Y.shape}\")\n",
    "    D  = X.shape[0]\n",
    "    sim = M1_sim(X, Y)\n",
    "    # L'identité produit un entier pour {-1,+1}; on arrondit prudemment.\n",
    "    return int(round((D/2.0) * (1.0 - sim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da97e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_mat(A: np.ndarray) -> np.ndarray:\n",
    "    A = np.asarray(A)\n",
    "    if A.ndim != 2:\n",
    "        raise ValueError(f\"attendu une matrice 2D, shape={A.shape}\")\n",
    "    return A\n",
    "\n",
    "def M1_sim_batch(Xs: np.ndarray, Ys: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Similarités ligne-à-ligne pour deux matrices (n,D):\n",
    "      sim_k = (1/D) <Xs[k], Ys[k]>   pour k=0..n-1\n",
    "    Retour: (n,) en float64.\n",
    "    \"\"\"\n",
    "    Xs = _as_mat(Xs); Ys = _as_mat(Ys)\n",
    "    if Xs.shape != Ys.shape:\n",
    "        raise ValueError(f\"shapes incompatibles: {Xs.shape} vs {Ys.shape}\")\n",
    "    D  = Xs.shape[1]\n",
    "    Xv = Xs.astype(np.int32, copy=False) if Xs.dtype == np.int8 else Xs\n",
    "    Yv = Ys.astype(np.int32, copy=False) if Ys.dtype == np.int8 else Ys\n",
    "    dots = (Xv * Yv).sum(axis=1, dtype=np.int64)   # somme sûre\n",
    "    return (dots / D).astype(np.float64, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb3320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rand_pm1(n: int, D: int, seed: int) -> np.ndarray:\n",
    "    g = np.random.default_rng(seed)\n",
    "    B = g.integers(0, 2, size=(n, D), dtype=np.int8)  # {0,1}\n",
    "    return (B << 1) - 1                                # {-1,+1}\n",
    "\n",
    "def test_M1_identity(D: int = 4096, n: int = 1000, seed: int = 7) -> None:\n",
    "    X = _rand_pm1(n, D, seed)\n",
    "    Y = _rand_pm1(n, D, seed + 1)\n",
    "    sim = M1_sim_batch(X, Y)\n",
    "    dH  = (D/2.0 * (1.0 - sim)).astype(int)\n",
    "    # Comptage direct des désaccords\n",
    "    mis = ((X * Y) == -1).sum(axis=1)\n",
    "    assert np.all(dH == mis), \"identité Hamming <-> produit violée\"\n",
    "\n",
    "    # Bords (quelques échantillons)\n",
    "    for k in range(3):\n",
    "        xk = X[k]\n",
    "        assert M1_sim(xk, xk) == 1.0\n",
    "        assert M1_dH(xk, xk)  == 0\n",
    "        assert M1_sim(xk, -xk) == -1.0\n",
    "        assert M1_dH(xk, -xk)  == D\n",
    "\n",
    "def test_M1_batch_equivalence(D: int = 8192, n: int = 200, seed: int = 11) -> None:\n",
    "    X = _rand_pm1(n, D, seed)\n",
    "    Y = _rand_pm1(n, D, seed + 1)\n",
    "    sb = M1_sim_batch(X, Y)\n",
    "    # version naïve (boucle) pour contrôle\n",
    "    sn = np.array([M1_sim(X[k], Y[k]) for k in range(n)], dtype=np.float64)\n",
    "    assert np.allclose(sb, sn, rtol=0, atol=1e-12), \"écart batch vs naïf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7fd25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_M1_dtypes_and_var(D: int = 16384, n: int = 2000, seed: int = 21) -> None:\n",
    "    X = _rand_pm1(n, D, seed).astype(np.int8)\n",
    "    Y = _rand_pm1(n, D, seed + 1).astype(np.int8)\n",
    "\n",
    "    # dtypes flottants\n",
    "    Xf32, Yf32 = X.astype(np.float32), Y.astype(np.float32)\n",
    "    Xf64, Yf64 = X.astype(np.float64), Y.astype(np.float64)\n",
    "\n",
    "    s_int  = M1_sim_batch(X, Y)\n",
    "    s_f32  = M1_sim_batch(Xf32, Yf32)\n",
    "    s_f64  = M1_sim_batch(Xf64, Yf64)\n",
    "    assert np.allclose(s_int, s_f32, rtol=0, atol=1e-12)\n",
    "    assert np.allclose(s_int, s_f64, rtol=0, atol=1e-12)\n",
    "\n",
    "    # variance ≈ 1/D sous indépendance\n",
    "    var_emp = float(s_int.var(ddof=1))\n",
    "    assert abs(var_emp - 1.0/D) < 5.0/D, f\"var={var_emp:.3g} vs 1/D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eadb5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_M1_identity()\n",
    "test_M1_batch_equivalence()\n",
    "test_M1_dtypes_and_var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57116f2",
   "metadata": {},
   "source": [
    "## M2 . Permutation positionnelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5e3dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M2_roll(X: np.ndarray, Delta: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rotation circulaire (décalage modulo D).\n",
    "    - X: vecteur 1D (D,) ou matrice (n,D)\n",
    "    - Delta: entier (peut être négatif)\n",
    "    - Retour: vue recopiée (np.roll) même dtype (ex. int8)\n",
    "    \"\"\"\n",
    "    A = np.asarray(X)\n",
    "    if A.ndim == 1:\n",
    "        return np.roll(A, shift=Delta)\n",
    "    if A.ndim == 2:\n",
    "        return np.roll(A, shift=Delta, axis=1)\n",
    "    raise ValueError(f\"shape non supportée: {A.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43458b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M2_plan_perm(D: int, seed: int | None = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Crée une permutation bijective π sur {0..D-1}.\n",
    "    \"\"\"\n",
    "    g  = np.random.default_rng(seed)\n",
    "    pi = g.permutation(D)                  # π : indices cibles\n",
    "    # Vérif bijectivité\n",
    "    if np.unique(pi).size != D:\n",
    "        raise RuntimeError(\"π non bijective\")\n",
    "    return pi.astype(np.int64, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea5409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cycles_of(pi: np.ndarray) -> tuple[np.ndarray, np.ndarray, list[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Retourne (cycle_id, pos_in_cycle, cycles) pour π.\n",
    "    - cycle_id[i]   : identifiant de cycle du point i\n",
    "    - pos_in_cycle[i]: position de i dans son cycle\n",
    "    - cycles        : liste de tableaux numpy des indices de chaque cycle\n",
    "    \"\"\"\n",
    "    D = pi.size\n",
    "    seen = np.zeros(D, dtype=bool)\n",
    "    cycles: list[np.ndarray] = []\n",
    "    cycle_id = -np.ones(D, dtype=np.int64)\n",
    "    pos      = -np.ones(D, dtype=np.int64)\n",
    "    cid = 0\n",
    "    for i in range(D):\n",
    "        if not seen[i]:\n",
    "            cur = []\n",
    "            j = i\n",
    "            while not seen[j]:\n",
    "                seen[j] = True\n",
    "                cur.append(j)\n",
    "                j = int(pi[j])\n",
    "            cyc = np.array(cur, dtype=np.int64)\n",
    "            for p, idx in enumerate(cyc):\n",
    "                cycle_id[idx] = cid\n",
    "                pos[idx]      = p\n",
    "            cycles.append(cyc); cid += 1\n",
    "    return cycle_id, pos, cycles\n",
    "\n",
    "def M2_pow_index(pi: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calcule l'index idx tel que X[idx] = Π^k_pi(X).\n",
    "    \"\"\"\n",
    "    cycle_id, pos, cycles = _cycles_of(pi)\n",
    "    D   = pi.size\n",
    "    idx = np.empty(D, dtype=np.int64)\n",
    "    for cyc in cycles:\n",
    "        L = cyc.size\n",
    "        # décalage modulo L\n",
    "        tgt = np.roll(cyc, shift=k % L)\n",
    "        # mapping: cyc[p] -> tgt[p]\n",
    "        idx[cyc] = tgt\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bbb277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M2_perm_pow(X: np.ndarray, pi: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applique Π^k_pi à X (1D ou 2D). Réutilise un index idx = π^k.\n",
    "    \"\"\"\n",
    "    A = np.asarray(X)\n",
    "    idx = M2_pow_index(pi, k)\n",
    "    if A.ndim == 1:\n",
    "        return A[idx]\n",
    "    if A.ndim == 2:\n",
    "        return A[:, idx]\n",
    "    raise ValueError(f\"shape non supportée: {A.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eda2f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rand_pm1(n: int, D: int, seed: int) -> np.ndarray:\n",
    "    g = np.random.default_rng(seed)\n",
    "    B = g.integers(0, 2, size=(n, D), dtype=np.int8)\n",
    "    return (B << 1) - 1  # {-1,+1}\n",
    "\n",
    "def test_M2_isometry_invertibility(D: int = 4096, n: int = 1000, seed: int = 7) -> None:\n",
    "    g   = np.random.default_rng(seed)\n",
    "    pi  = M2_plan_perm(D, seed=seed)\n",
    "    X   = _rand_pm1(n, D, seed+1)\n",
    "    Y   = _rand_pm1(n, D, seed+2)\n",
    "    ks  = g.integers(-5*D, 5*D, size=10)  # k variés (mod L des cycles)\n",
    "    for k in ks:\n",
    "        Xp = M2_perm_pow(X, pi, int(k))\n",
    "        Yp = M2_perm_pow(Y, pi, int(k))\n",
    "        # Isométrie (échantillons)\n",
    "        for t in range(5):\n",
    "            s1 = M1_sim(X[t], Y[t])\n",
    "            s2 = M1_sim(Xp[t], Yp[t])\n",
    "            assert s1 == s2, \"isométrie violée\"\n",
    "        # Inversibilité\n",
    "        Xback = M2_perm_pow(Xp, pi, int(-k))\n",
    "        assert np.array_equal(X, Xback), \"inverse incorrect\"\n",
    "        # Distribution (comptages ±1)\n",
    "        cnt = (X[0] == 1).sum()\n",
    "        cntp = (Xp[0] == 1).sum()\n",
    "        assert cnt == cntp, \"distribution ±1 non conservée\"\n",
    "\n",
    "def test_M2_roll_isometry(D: int = 4096, n: int = 200, seed: int = 11) -> None:\n",
    "    X = _rand_pm1(n, D, seed)\n",
    "    Y = _rand_pm1(n, D, seed+1)\n",
    "    for Delta in (-17, -1, 0, 1, 37):\n",
    "        Xr = M2_roll(X, Delta)\n",
    "        Yr = M2_roll(Y, Delta)\n",
    "        # vérifier quelques paires\n",
    "        for t in range(3):\n",
    "            assert M1_sim(X[t], Y[t]) == M1_sim(Xr[t], Yr[t])\n",
    "        # inversibilité\n",
    "        assert np.array_equal(M2_roll(M2_roll(X, Delta), -Delta), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb793c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_M2_isometry_invertibility()\n",
    "test_M2_roll_isometry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1bcce7",
   "metadata": {},
   "source": [
    "## M3 . Binding (Hadamard / XNOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c6bddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_pm1_int8(A: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Vérifie que A ne contient que {-1,+1} et est en int8.\"\"\"\n",
    "    B = np.asarray(A)\n",
    "    if B.dtype != np.int8:\n",
    "        B = B.astype(np.int8, copy=False)\n",
    "    # (option) assertions rapides: \n",
    "    # if not np.all((B == -1) | (B == 1)): raise ValueError(\"non ±1\")\n",
    "    return B\n",
    "\n",
    "def M3_bind(X: np.ndarray, J: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Binding Hadamard: (X * J) en conservant le dtype int8.\n",
    "    - X, J: (D,) ou (n,D) avec broadcast sur J (D,)\n",
    "    - Retour: même shape que X, dtype int8\n",
    "    \"\"\"\n",
    "    Xv = _ensure_pm1_int8(X)\n",
    "    Jv = _ensure_pm1_int8(J)\n",
    "    return (Xv * Jv).astype(np.int8, copy=False)\n",
    "\n",
    "def M3_unbind(Y: np.ndarray, J: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Unbinding = binding avec la même clé (involutif).\n",
    "    \"\"\"\n",
    "    Yv = _ensure_pm1_int8(Y)\n",
    "    Jv = _ensure_pm1_int8(J)\n",
    "    return (Yv * Jv).astype(np.int8, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d3a561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rand_pm1(n: int, D: int, seed: int) -> np.ndarray:\n",
    "    g = np.random.default_rng(seed)\n",
    "    B = g.integers(0, 2, size=(n, D), dtype=np.int8)\n",
    "    return (B << 1) - 1\n",
    "\n",
    "def test_M3_isometry_involution(D: int = 4096, n: int = 2000, seed: int = 21) -> None:\n",
    "    from math import isclose\n",
    "    # échantillons\n",
    "    X = _rand_pm1(n, D, seed)\n",
    "    Y = _rand_pm1(n, D, seed+1)\n",
    "    J = _rand_pm1(1, D, seed+2)[0]  # clé (D,)\n",
    "    # isométrie (quelques paires)\n",
    "    for t in range(10):\n",
    "        s1 = M1_sim(X[t], Y[t])\n",
    "        Xb, Yb = M3_bind(X[t], J), M3_bind(Y[t], J)\n",
    "        s2 = M1_sim(Xb, Yb)\n",
    "        assert s1 == s2, \"isométrie violée\"\n",
    "        # involutivité\n",
    "        assert np.array_equal(M3_unbind(Xb, J), X[t]), \"involutivité violée\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d3ff4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_M3_isometry_involution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd30de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M3_bind_batch(X: np.ndarray, J: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Binding Hadamard par lot (batch).\n",
    "    \n",
    "    Paramètres\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Tableau de forme (n, D) en {-1, +1}, dtype=int8 recommandé.\n",
    "    J : np.ndarray\n",
    "        Clé unique de forme (D,) en {-1, +1}, dtype=int8.\n",
    "    \n",
    "    Retour\n",
    "    ------\n",
    "    np.ndarray\n",
    "        Résultat de forme (n, D), dtype=int8.\n",
    "    \"\"\"\n",
    "    Xv = _ensure_pm1_int8(X)\n",
    "    Jv = _ensure_pm1_int8(J)\n",
    "\n",
    "    if Xv.ndim != 2:\n",
    "        raise ValueError(f\"X doit être de rang 2 (n,D), reçu shape={Xv.shape}\")\n",
    "    if Jv.ndim != 1:\n",
    "        raise ValueError(f\"J doit être un vecteur (D,), reçu shape={Jv.shape}\")\n",
    "    if Xv.shape[1] != Jv.size:\n",
    "        raise ValueError(f\"Dimensions incompatibles: X{Xv.shape}, J{Jv.shape}\")\n",
    "\n",
    "    return (Xv * Jv).astype(np.int8, copy=False)\n",
    "\n",
    "def test_M3_cross_tranche(D: int = 16384, n: int = 4000, eps: float = 0.05, seed: int = 33) -> dict:\n",
    "    X = _rand_pm1(n, D, seed)\n",
    "    Y = _rand_pm1(n, D, seed+1)\n",
    "    J  = _rand_pm1(1, D, seed+2)[0]\n",
    "    Jp = _rand_pm1(1, D, seed+3)[0]\n",
    "    Xb, Ybp = M3_bind_batch(X, J), M3_bind_batch(Y, Jp)\n",
    "    sims = M1_sim_batch(Xb, Ybp)  # M1 (batch)\n",
    "    mean_sim  = float(sims.mean())\n",
    "    tail_prob = float((np.abs(sims) > eps).mean())\n",
    "    bound     = 2.0 * math.exp(- D * eps * eps / 2.0)\n",
    "    # petits checks\n",
    "    assert abs(mean_sim) < 5e-3 + 5e-3, \"centrage éloigné de 0\"\n",
    "    return {\"mean\": mean_sim, \"tail\": tail_prob, \"hoeffding\": bound}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64a95978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': -1.373291015625e-06, 'tail': 0.0, 'hoeffding': 2.5508152590520792e-09}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_M3_cross_tranche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "965ac55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M3_xnor_bind_bits(xbits: np.ndarray, jbits: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    xbits, jbits: tableaux d'octets (packés) représentant {0,1}.\n",
    "    Retour: xnor(xbits, jbits) packé (même shape), sans remap.\n",
    "    (Implémentation complète et remap -> ±1 seront intégrés au module d'optimisation.)\n",
    "    \"\"\"\n",
    "    if xbits.dtype != np.uint8 or jbits.dtype != np.uint8:\n",
    "        raise ValueError(\"bits attendus en uint8\")\n",
    "    if xbits.shape != jbits.shape:\n",
    "        raise ValueError(\"shapes incompatibles\")\n",
    "    # XNOR = NOT XOR\n",
    "    return np.bitwise_not(np.bitwise_xor(xbits, jbits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904247a7",
   "metadata": {},
   "source": [
    "## M4 . Lexique EN (hypervecteurs types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e32cef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _canon_token(w: str) -> str:\n",
    "    \"\"\"Normalisation légère: trim + lower; extensible selon besoin.\"\"\"\n",
    "    return w.strip().lower()\n",
    "\n",
    "class M4_LexEN:\n",
    "    \"\"\"\n",
    "    Lexique EN: w -> L_en(w) ∈ {-1,+1}^D (int8, readonly).\n",
    "    - RNG PCG64 dédié pour reproductibilité.\n",
    "    - Option: pool OOV pré-généré pour réduire la latence.\n",
    "    - Persistance: save/load .npz (portable).\n",
    "    \"\"\"\n",
    "    def __init__(self, seed: int, D: int, reserve_pool: int = 0):\n",
    "        self.D: int = int(D)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.table: Dict[str, np.ndarray] = {}\n",
    "        self._reserve_pool: List[np.ndarray] = []\n",
    "        for _ in range(int(reserve_pool)):\n",
    "            v = self._new_key()\n",
    "            v.setflags(write=False)\n",
    "            self._reserve_pool.append(v)\n",
    "        self._pool_idx: int = 0\n",
    "\n",
    "    def _new_key(self) -> np.ndarray:\n",
    "        bits = self.rng.integers(0, 2, size=self.D, dtype=np.int8)\n",
    "        return ((bits << 1) - 1).astype(np.int8, copy=False)\n",
    "\n",
    "    def get(self, w: str, use_pool: bool = False) -> np.ndarray:\n",
    "        key = _canon_token(w)\n",
    "        v = self.table.get(key, None)\n",
    "        if v is not None:\n",
    "            return v\n",
    "        if use_pool and self._reserve_pool:\n",
    "            v = self._reserve_pool[self._pool_idx]\n",
    "            self._pool_idx = (self._pool_idx + 1) % len(self._reserve_pool)\n",
    "        else:\n",
    "            v = self._new_key()\n",
    "            v.setflags(write=False)       # immutabilité côté appelant\n",
    "        self.table[key] = v\n",
    "        return v\n",
    "\n",
    "    def get_many(self, words: Iterable[str], use_pool: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Retourne une matrice (n, D) int8 alignée en mémoire (C-contiguë).\n",
    "        \"\"\"\n",
    "        arr = np.empty((len(list(words)), self.D), dtype=np.int8)\n",
    "        # NB: on boucle pour éviter des copies implicites; n ~ phrase courte => OK.\n",
    "        for i, w in enumerate(words):\n",
    "            arr[i, :] = self.get(w, use_pool=use_pool)\n",
    "        return arr\n",
    "\n",
    "    def contains(self, w: str) -> bool:\n",
    "        return _canon_token(w) in self.table\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return len(self.table)\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        keys = np.array(list(self.table.keys()), dtype=object)\n",
    "        mats = np.stack([self.table[k] for k in keys], axis=0).astype(np.int8, copy=False)\n",
    "        np.savez_compressed(path, D=self.D, keys=keys, mats=mats)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: str) -> \"M4_LexEN\":\n",
    "        data = np.load(path, allow_pickle=True)\n",
    "        D = int(data[\"D\"])\n",
    "        keys: np.ndarray = data[\"keys\"]\n",
    "        mats: np.ndarray = data[\"mats\"].astype(np.int8, copy=False)\n",
    "        # seed fictif (non utilisé pour les entrées déjà présentes)\n",
    "        lex = M4_LexEN(seed=0, D=D)\n",
    "        for k, v in zip(keys.tolist(), mats):\n",
    "            vv = v.view()\n",
    "            vv.setflags(write=False)\n",
    "            lex.table[k] = vv\n",
    "        return lex\n",
    "\n",
    "# Raccourcis API:\n",
    "def M4_LexEN_new(seed: int, D: int, reserve_pool: int = 0) -> M4_LexEN:\n",
    "    return M4_LexEN(seed, D, reserve_pool)\n",
    "\n",
    "def M4_get(Lex: M4_LexEN, w: str, use_pool: bool = False) -> np.ndarray:\n",
    "    return Lex.get(w, use_pool=use_pool)\n",
    "\n",
    "def M4_get_many(Lex: M4_LexEN, words: Iterable[str], use_pool: bool = False) -> np.ndarray:\n",
    "    return Lex.get_many(words, use_pool=use_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "878d02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_M4_seed_stability(D: int = 16384, seed: int = 123) -> None:\n",
    "    L1 = M4_LexEN_new(seed, D)\n",
    "    L2 = M4_LexEN_new(seed, D)\n",
    "    v1 = M4_get(L1, \"Cat\")\n",
    "    v2 = M4_get(L2, \"cat\")  # même token après normalisation\n",
    "    assert np.array_equal(v1, v2), \"instables malgré seed identique\"\n",
    "    # immutabilité:\n",
    "    try:\n",
    "        v1[0] = 0\n",
    "        assert False, \"le vecteur devrait être readonly\"\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66007a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_M4_seed_stability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4219f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M4_collision_audit(Lex: M4_LexEN, vocab: List[str]) -> float:\n",
    "    mats = np.stack([Lex.get(w) for w in vocab], axis=0).astype(np.int16, copy=False)\n",
    "    D = mats.shape[1]\n",
    "    # Gram normalisé\n",
    "    G = (mats @ mats.T) / D\n",
    "    np.fill_diagonal(G, 0.0)\n",
    "    return float(np.max(np.abs(G)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fbf83f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_M4_collisions(D: int = 16384, V: int = 2000, seed: int = 321) -> None:\n",
    "    Lex = M4_LexEN_new(seed, D)\n",
    "    vocab = [f\"w{i}\" for i in range(V)]\n",
    "    m = M4_collision_audit(Lex, vocab)\n",
    "    assert m <= 0.05 + 1e-6, f\"max |sim|={m:.3f} > 0.05 (D trop petit?)\"\n",
    "    \n",
    "def test_M4_oov_pool(D: int = 8192, seed: int = 77, R: int = 512, N: int = 400) -> None:\n",
    "    Lex = M4_LexEN_new(seed, D, reserve_pool=R)\n",
    "    seen = set()\n",
    "    for i in range(N):\n",
    "        v = M4_get(Lex, f\"OOV-{i}\", use_pool=True)\n",
    "        seen.add(id(v))\n",
    "    reuse_rate = 1.0 - len(seen) / N  # part de réutilisations (si N>R)\n",
    "    assert reuse_rate <= max(0.0, (N - R) / N + 1e-9)\n",
    "\n",
    "def test_M4_persist(tmp_path: Optional[str] = None) -> None:\n",
    "    import os, tempfile\n",
    "    if tmp_path is None:\n",
    "        tmp_path = tempfile.mkdtemp()\n",
    "    path = os.path.join(tmp_path, \"lex.npz\")\n",
    "    Lex = M4_LexEN_new(42, 4096)\n",
    "    ref = [M4_get(Lex, w) for w in [\"a\", \"b\", \"c\"]]\n",
    "    Lex.save(path)\n",
    "    Lex2 = M4_LexEN.load(path)\n",
    "    out = [M4_get(Lex2, w) for w in [\"a\", \"b\", \"c\"]]\n",
    "    for r, o in zip(ref, out):\n",
    "        assert np.array_equal(r, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "068da62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_M4_collisions()\n",
    "test_M4_oov_pool()\n",
    "test_M4_persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aef64835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M4_pair_stats(Lex: M4_LexEN, vocab: List[str], n_pairs: int = 50_000, seed: int = 5) -> Tuple[float, float]:\n",
    "    g = np.random.default_rng(seed)\n",
    "    V = len(vocab)\n",
    "    sims = np.empty(n_pairs, dtype=np.float64)\n",
    "    for k in range(n_pairs):\n",
    "        i, j = g.integers(0, V), g.integers(0, V)\n",
    "        while j == i:\n",
    "            j = g.integers(0, V)\n",
    "        Xi, Xj = Lex.get(vocab[i]).astype(np.int32, copy=False), Lex.get(vocab[j]).astype(np.int32, copy=False)\n",
    "        sims[k] = (Xi @ Xj) / Lex.D\n",
    "    return float(sims.mean()), float(sims.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e7bbf",
   "metadata": {},
   "source": [
    "## M5 . Compositeur de n-grammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd4689f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_int(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retourne un vecteur int8 dans {-1,+1} à partir d'un tableau entier x.\n",
    "    Convention: 0 -> +1 (rare si n est impair).\n",
    "    \"\"\"\n",
    "    y = (x >= 0).astype(np.int8, copy=False)\n",
    "    return ((y << 1) - 1).astype(np.int8, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02b7c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M5_precompute_pi_pows(pi: np.ndarray, n: int) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Pré-calcul des indices pour Pi^j, j=0..n-1. \n",
    "    pi est une permutation (shape (D,)).\n",
    "    \"\"\"\n",
    "    D = pi.shape[0]\n",
    "    idxs = [np.arange(D)]\n",
    "    for j in range(1, n):\n",
    "        idxs.append(pi[idxs[-1]])\n",
    "    # rendre en lecture seule (sécurité)\n",
    "    for a in idxs: a.setflags(write=False)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a2bf8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M5_ngram_cached(LexEN, pi_pows: List[np.ndarray], tokens: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    E_t = sign( sum_{j=0..n-1} Pi^j L(w_{t-j}) ), \n",
    "    avec pi_pows[j] = indices pour Pi^j.\n",
    "    \"\"\"\n",
    "    n = len(tokens); D = LexEN.D\n",
    "    acc = np.zeros(D, dtype=np.int16)\n",
    "    # j=0 correspond au token courant; tokens = [w_{t-n+1},...,w_t]\n",
    "    for j, w in enumerate(tokens[::-1]):\n",
    "        Lw = LexEN.get(w).astype(np.int16, copy=False)\n",
    "        acc += Lw[pi_pows[j]]\n",
    "    return sign_int(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d262ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M5_ngram(LexEN, pi: np.ndarray, tokens: List[str]) -> np.ndarray:\n",
    "    n = len(tokens); D = LexEN.D\n",
    "    acc = np.zeros(D, dtype=np.int16)\n",
    "    base = np.arange(D)\n",
    "    # pour éviter de refaire base à chaque j, on itère sur un idx courant\n",
    "    idx = base\n",
    "    # on veut Pi^j pour j = 0..n-1, appliqué au token w_{t-j}\n",
    "    # tokens[::-1] correspond à j=0..n-1\n",
    "    for j, w in enumerate(tokens[::-1]):\n",
    "        if j == 0:\n",
    "            idx = base  # Pi^0\n",
    "        else:\n",
    "            idx = pi[idx]  # composition\n",
    "        Lw = LexEN.get(w).astype(np.int16, copy=False)\n",
    "        acc += Lw[idx]\n",
    "    return sign_int(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "606e136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M5_window_stream(LexEN, pi: np.ndarray, words: List[str], n: int) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Renvoie la liste [E_n, E_{n+1}, ..., E_T].\n",
    "    Pré-calcule une seule fois les Pi^j.\n",
    "    \"\"\"\n",
    "    pi_pows = M5_precompute_pi_pows(pi, n)\n",
    "    out: List[np.ndarray] = []\n",
    "    for t in range(n-1, len(words)):\n",
    "        window = words[max(0, t-n+1): t+1]  # longueur n (supposer len >= n)\n",
    "        out.append(M5_ngram_cached(LexEN, pi_pows, window))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e7c57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rand_word(g, V): return f\"w{int(g.integers(0, V))}\"\n",
    "\n",
    "def test_M5_separability(D=16384, n=3, V=2000, trials=500, seed=77):\n",
    "    \"\"\"\n",
    "    Correction: fam1b est construit en perturbant (au plus) 1 token d'une\n",
    "    même base de n-grammes, plutôt que de tirer des tokens indépendants.\n",
    "    Attendu: intra > 0.2 ; inter ≈ 0.\n",
    "    \"\"\"\n",
    "    g = np.random.default_rng(seed)\n",
    "\n",
    "    # Dépendances (cohérentes avec vos modules)\n",
    "    Lex = M4_LexEN_new(seed+1, D)\n",
    "    pi  = M2_plan_perm(D, seed+2)\n",
    "    pi_pows = M5_precompute_pi_pows(pi, n)\n",
    "\n",
    "    # 1) Génère les séquences de base (trials x n)\n",
    "    base_seqs = [[_rand_word(g, V) for _ in range(n)] for _ in range(trials)]\n",
    "\n",
    "    # 2) fam1 = n-grammes exacts issus des bases\n",
    "    fam1 = [M5_ngram_cached(Lex, pi_pows, toks) for toks in base_seqs]\n",
    "\n",
    "    # 3) fam1b = copies perturbées (au plus 1 token remplacé avec prob 0.2)\n",
    "    fam1b = []\n",
    "    for toks in base_seqs:\n",
    "        toks2 = toks.copy()\n",
    "        if g.random() < 0.2:  # petite perturbation de structure\n",
    "            j = int(g.integers(0, n))\n",
    "            toks2[j] = _rand_word(g, V)\n",
    "        fam1b.append(M5_ngram_cached(Lex, pi_pows, toks2))\n",
    "\n",
    "    # 4) fam2 = autre famille indépendante (n-grammes disjoints en moyenne)\n",
    "    fam2 = [M5_ngram_cached(Lex, pi_pows, [_rand_word(g, V) for _ in range(n)]) \n",
    "            for _ in range(trials)]\n",
    "\n",
    "    # Mesures (vectorisées)\n",
    "    A = np.stack(fam1).astype(np.int16, copy=False)   # (trials,D)\n",
    "    B = np.stack(fam1b).astype(np.int16, copy=False)  # (trials,D)\n",
    "    C = np.stack(fam2).astype(np.int16, copy=False)   # (trials,D)\n",
    "\n",
    "    # Similarité \"intra\" paire-à-paire (i,i) entre fam1 et fam1b\n",
    "    intra = float(np.mean(np.sum(A * B, axis=1) / D))\n",
    "\n",
    "    # Similarité \"inter\" moyenne entre fam1 et fam2 (toutes paires)\n",
    "    # NB: on peut prendre l'absolu, la moyenne simple doit être centrée ~0\n",
    "    inter = float(np.mean(np.abs((A @ C.T) / D)))\n",
    "\n",
    "    assert intra > 0.2 - 1e-3, f\"intra {intra:.3f} trop faible\"\n",
    "    assert inter < 0.05 + 1e-3, f\"inter {inter:.3f} trop élevé\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdd75d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_M5_drop_robustness(D=16384, n_list=(2,3,5), V=2000, trials=200, drop=0.1, seed=99):\n",
    "    g = np.random.default_rng(seed)\n",
    "    Lex = M4_LexEN_new(seed+1, D)\n",
    "    pi  = M2_plan_perm(D, seed+2)\n",
    "    results = []\n",
    "    for n in n_list:\n",
    "        pi_pows = M5_precompute_pi_pows(pi, n)\n",
    "        fam = [[_rand_word(g,V) for _ in range(n)] for _ in range(trials)]\n",
    "        E_ref = [M5_ngram_cached(Lex, pi_pows, toks) for toks in fam]\n",
    "        # drop: remplacer chaque token avec prob drop\n",
    "        def apply_drop(toks):\n",
    "            return [tok if g.random() > drop else _rand_word(g,V) for tok in toks]\n",
    "        E_drop = [M5_ngram_cached(Lex, pi_pows, apply_drop(t)) for t in fam]\n",
    "        A = np.stack(E_ref).astype(np.int16)\n",
    "        B = np.stack(E_drop).astype(np.int16)\n",
    "        sim = float(np.mean(np.sum(A*B, axis=1) / D))\n",
    "        results.append((n, sim))\n",
    "    # On s'attend à une décroissance de sim quand n augmente (plus de contraintes).\n",
    "    for k in range(1, len(results)):\n",
    "        assert results[k][1] <= results[k-1][1] + 1e-3\n",
    "\n",
    "def test_M5_edges(D=4096, n=3, seed=5):\n",
    "    Lex = M4_LexEN_new(seed+1, D)\n",
    "    pi  = M2_plan_perm(D, seed+2)\n",
    "    pi_pows = M5_precompute_pi_pows(pi, n)\n",
    "    toks = [\"a\",\"b\",\"c\"][:n]\n",
    "    E = M5_ngram_cached(Lex, pi_pows, toks)\n",
    "    # bords M1\n",
    "    assert M1_sim(E, E) == 1.0\n",
    "    assert M1_dH(E, E) == 0\n",
    "    # isométrie M2\n",
    "    Epi = E[pi]  # Pi^1 pour test\n",
    "    assert M1_sim(Epi, Epi) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9a8fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_M5_separability()\n",
    "test_M5_drop_robustness()\n",
    "test_M5_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e8ef1d",
   "metadata": {},
   "source": [
    "## M6 . Accumulateur de segment & seuillage (majorité)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be3cd473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M6_SegAcc_init(D: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Initialise l'accumulateur de segment en int16 (zéro).\n",
    "    \"\"\"\n",
    "    return np.zeros(int(D), dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0abca841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M6_SegAcc_pushPos(S: np.ndarray, X_t: np.ndarray, K_s: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ajoute (X_t ⊗ K_s) à l'accumulateur S.\n",
    "    X_t,K_s sont int8 dans {-1,+1}. S est int16.\n",
    "    \"\"\"\n",
    "    # binding hadamard (M3), upcast en int16 pour l'addition\n",
    "    S += (X_t.astype(np.int16, copy=False) * K_s.astype(np.int16, copy=False))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "968fd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M6_SegAcc_pushE(S: np.ndarray, E_t: np.ndarray, Delta: int,\n",
    "                    K_s: np.ndarray, pi_pows: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ajoute (Pi^Delta E_t ⊗ K_s) à S.\n",
    "    E_t,K_s int8 {-1,+1}; S int16; pi_pows[j] = indices pour Pi^j.\n",
    "    \"\"\"\n",
    "    idx = pi_pows[Delta]  # suppose Delta < len(pi_pows)\n",
    "    # re-indexation + binding + accumulation\n",
    "    S += (E_t[idx].astype(np.int16, copy=False) * K_s.astype(np.int16, copy=False))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a04a0903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M6_SegAcc_sign(S: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retourne H = sign_strict(S) en int8 dans {-1,+1}.\n",
    "    Règle stricte : +1 si S>0, sinon -1.\n",
    "    \"\"\"\n",
    "    return np.where(S > 0, 1, -1).astype(np.int8, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e69d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_M6_basic(D=4096, seed=12):\n",
    "    g = np.random.default_rng(seed)\n",
    "    X = ((g.integers(0,2,size=D,dtype=np.int8) << 1) - 1)\n",
    "    Y = ((g.integers(0,2,size=D,dtype=np.int8) << 1) - 1)\n",
    "    K = ((g.integers(0,2,size=D,dtype=np.int8) << 1) - 1)\n",
    "    # involutivité\n",
    "    assert np.array_equal((X*K)*K, X)\n",
    "    # isométrie\n",
    "    from math import isclose\n",
    "    dot_xy = int(np.dot(X.astype(np.int32), Y.astype(np.int32)))\n",
    "    dot_b  = int(np.dot((X*K).astype(np.int32), (Y*K).astype(np.int32)))\n",
    "    assert dot_xy == dot_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7001099",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_M6_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "152fb546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_half_vs_p(p: float) -> float:\n",
    "    \"\"\"\n",
    "    KL(1/2 || p) = (1/2) * [log(1/2p) + log(1/2(1-p))] = 0.5*log(1/(4 p (1-p))).\n",
    "    C'est le taux de Chernoff exact pour le seuil 1/2 sur des Bernoulli(p).\n",
    "    \"\"\"\n",
    "    if not (0.0 < p < 1.0):\n",
    "        raise ValueError(\"p doit être dans (0,1)\")\n",
    "    return 0.5 * math.log(1.0 / (4.0 * p * (1.0 - p)))\n",
    "\n",
    "\n",
    "def M6_simulate_majority_error_fast(\n",
    "    D: int = 8192,\n",
    "    m_list=(8, 16, 32, 64, 128),\n",
    "    p: float = 0.55,\n",
    "    trials: int = 5000,\n",
    "    seed: int = 33,\n",
    "    strict_tie: bool = True,\n",
    "    batch_trials: int = 2048,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulation optimisée: on simule #(+1) ~ Binomial(m,p) pour (trials, D).\n",
    "    'strict_tie=True' -> majority stricte (> m/2), sinon non stricte (>= m/2).\n",
    "    \"\"\"\n",
    "    g = np.random.default_rng(seed)\n",
    "    out = []\n",
    "    for m in m_list:\n",
    "        thr = (m // 2) + 1 if strict_tie else (m + 1) // 2\n",
    "        total_ok = 0\n",
    "        done = 0\n",
    "        while done < trials:\n",
    "            b = min(batch_trials, trials - done)\n",
    "            cnt = g.binomial(n=m, p=p, size=(b, D)).astype(np.int16, copy=False)\n",
    "            total_ok += int((cnt >= thr).sum())\n",
    "            done += b\n",
    "        err_emp = 1.0 - (total_ok / float(trials * D))\n",
    "        # borne simple (quadratique) donnée à titre indicatif\n",
    "        bound_hoeff = math.exp(-0.5 * m * (2 * p - 1) ** 2)\n",
    "        out.append((m, float(err_emp), float(bound_hoeff)))\n",
    "    return out\n",
    "\n",
    "\n",
    "def M6_fit_loglinear(points, m_min_for_fit: int = 32):\n",
    "    \"\"\"\n",
    "    Fit log(err_emp) = a + b*m sur m >= m_min_for_fit.\n",
    "    Retourne {intercept, slope, R2}.\n",
    "    \"\"\"\n",
    "    arr = np.array(points, dtype=np.float64)    # colonnes: m, err_emp, bound\n",
    "    m_all = arr[:, 0]\n",
    "    y_all = np.log(np.maximum(arr[:, 1], 1e-14))  # plancher num.\n",
    "    mask = (m_all >= m_min_for_fit)\n",
    "    m = m_all[mask]\n",
    "    y = y_all[mask]\n",
    "    A = np.vstack([np.ones_like(m), m]).T\n",
    "    coef, *_ = np.linalg.lstsq(A, y, rcond=None)\n",
    "    yhat = A @ coef\n",
    "    ss_res = float(np.sum((y - yhat) ** 2))\n",
    "    ss_tot = float(np.sum((y - y.mean()) ** 2))\n",
    "    R2 = 1.0 - ss_res / ss_tot if ss_tot > 0 else 1.0\n",
    "    return dict(intercept=float(coef[0]), slope=float(coef[1]), R2=R2)\n",
    "\n",
    "\n",
    "def test_M6_majority_acceptance(\n",
    "    D: int = 8192,\n",
    "    p: float = 0.60,\n",
    "    m_list=(8, 16, 32, 64, 128, 192),\n",
    "    trials: int = 8000,\n",
    "    seed: int = 44,\n",
    "    strict_tie: bool = True,\n",
    "    m_min_for_fit: int = 64,\n",
    "    rel_tol_vs_KL: float = 0.25  # 25% de tolérance vs pente KL\n",
    "):\n",
    "    \"\"\"\n",
    "    Test d'acceptation ajusté:\n",
    "      - Simulation rapide (binomiale).\n",
    "      - Fit sur m >= m_min_for_fit (réduit la courbure aux petits m).\n",
    "      - Comparaison à la pente théorique -KL(1/2||p) (plus fidèle que Hoeffding).\n",
    "    \"\"\"\n",
    "    pts = M6_simulate_majority_error_fast(\n",
    "        D=D, m_list=m_list, p=p, trials=trials, seed=seed,\n",
    "        strict_tie=strict_tie, batch_trials=2048\n",
    "    )\n",
    "    fit = M6_fit_loglinear(pts, m_min_for_fit=m_min_for_fit)\n",
    "    slope_emp = fit[\"slope\"]\n",
    "    slope_th  = - kl_half_vs_p(p)    # pente attendue en asymptote\n",
    "    rel_gap   = abs(slope_emp - slope_th) / abs(slope_th)\n",
    "\n",
    "    # critères: linéarité forte + pente raisonnablement proche du taux KL\n",
    "    assert fit[\"R2\"] >= 0.98, f\"R2 {fit['R2']:.3f} < 0.98\"\n",
    "    assert rel_gap <= rel_tol_vs_KL, (\n",
    "        f\"pente {slope_emp:.4f} vs th(KL) {slope_th:.4f} (écart rel {rel_gap:.1%})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ded2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_M6_majority_acceptance(D=8192, seed=44):\n",
    "#     pts = M6_simulate_majority_error(D=D, p=0.60, seed=seed)\n",
    "#     fit = M6_fit_loglinear([(m,e,b) for (m,e,b) in pts])\n",
    "#     theor_slope = -0.5 * (2*0.60 - 1)**2\n",
    "#     rel_gap = abs(fit[\"slope\"] - theor_slope) / abs(theor_slope)\n",
    "#     assert fit[\"R2\"] >= 0.98, f\"R2 {fit['R2']:.3f} < 0.98\"\n",
    "#     assert rel_gap <= 0.15,   f\"pente {fit['slope']:.4f} vs th {theor_slope:.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd021f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_M6_majority_acceptance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcdcf8e",
   "metadata": {},
   "source": [
    "## M7 . Gestionnaire de segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "326e1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class M7_SegMgr:\n",
    "    \"\"\"\n",
    "    Gestionnaire de segments (phrases) :\n",
    "      - génère des clés K_s indépendantes (ou corrélées pour stress-test),\n",
    "      - assure un reset dur à chaque frontière.\n",
    "    \"\"\"\n",
    "    def __init__(self, seed: int, D: int):\n",
    "        self.D = int(D)\n",
    "        self._g = np.random.default_rng(seed)\n",
    "        self.seg_idx = -1\n",
    "        self._K: Optional[np.ndarray] = None\n",
    "        self.onBoundary()  # crée K_0\n",
    "\n",
    "    def curKey(self) -> np.ndarray:\n",
    "        \"\"\"Retourne la clé courante K_s (vue en lecture seule).\"\"\"\n",
    "        assert self._K is not None\n",
    "        self._K.setflags(write=False)\n",
    "        return self._K\n",
    "\n",
    "    def onBoundary(self) -> np.ndarray:\n",
    "        \"\"\"Ouvre un nouveau segment, génère une nouvelle clé K_{s+1} indépendante.\"\"\"\n",
    "        self.seg_idx += 1\n",
    "        sub = int(self._g.integers(0, 2**31-1))\n",
    "        K = M0_NewKey(sub, self.D)  # {-1,+1}^D int8\n",
    "        K.setflags(write=False)\n",
    "        self._K = K\n",
    "        return self._K\n",
    "\n",
    "    def nextKey_correlated(self, rho: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        (Option) Produit K_{s+1} corrélé à K_s : E[K·K'] / D ≈ rho.\n",
    "        Réalisation simple: flip bit i avec proba f = (1 - rho)/2.\n",
    "        \"\"\"\n",
    "        assert self._K is not None\n",
    "        f = (1.0 - float(rho)) / 2.0\n",
    "        flips = (self._g.random(self.D) < f)\n",
    "        Kp = self._K.copy()\n",
    "        Kp[flips] = -Kp[flips]\n",
    "        Kp.setflags(write=False)\n",
    "        self.seg_idx += 1\n",
    "        self._K = Kp\n",
    "        return self._K\n",
    "\n",
    "def M7_SegMgr_new(seed: int, D: int) -> M7_SegMgr:\n",
    "    return M7_SegMgr(seed, D)\n",
    "\n",
    "def M7_curKey(SM: M7_SegMgr) -> np.ndarray:\n",
    "    return SM.curKey()\n",
    "\n",
    "def M7_onBoundary(SM: M7_SegMgr) -> np.ndarray:\n",
    "    return SM.onBoundary()\n",
    "\n",
    "\n",
    "# --- Corrections majeures apportées ---\n",
    "# 1) Suppression du biais dû aux ex-aequo (ties) :\n",
    "#    - Nouveau seuillage \"unbiased\" (tirage aléatoire ±1 si S==0)\n",
    "#    - Utilisé à la fois dans M5 (n-grammes) et M6 (état de segment).\n",
    "# 2) _safe_pushE robuste :\n",
    "#    - Détecte dynamiquement la signature de M6_SegAcc_push (2 ou 3 args)\n",
    "#    - Préserve l'absence de double-binding.\n",
    "# 3) Imports manquants (inspect, numpy) + annotations.\n",
    "\n",
    "import numpy as np\n",
    "import inspect\n",
    "from typing import List, Optional\n",
    "\n",
    "# ---------- 1) Signes : strict (historique) et unbiased (corrigé) ----------\n",
    "\n",
    "def _sign_strict_int8(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Seuillage strict: +1 si x>0, sinon -1 (int8). BIAISÉ si des zéros existent !\"\"\"\n",
    "    return np.where(x > 0, 1, -1).astype(np.int8, copy=False)\n",
    "\n",
    "def _sign_unbiased_int8(x: np.ndarray, rng: Optional[np.random.Generator] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Seuillage sans biais: +1 si x>0 ; -1 si x<0 ; pour x==0, tirage ±1 aléatoire (Rademacher).\n",
    "    Cela supprime le biais d'espérance dû aux ties.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(0)\n",
    "    H = np.empty_like(x, dtype=np.int8)\n",
    "    pos = (x > 0)\n",
    "    neg = (x < 0)\n",
    "    tie = ~(pos | neg)  # x == 0\n",
    "\n",
    "    H[pos] = 1\n",
    "    H[neg] = -1\n",
    "    if np.any(tie):\n",
    "        B = rng.integers(0, 2, size=int(tie.sum()), dtype=np.int8)\n",
    "        H[tie] = (B << 1) - 1\n",
    "    return H\n",
    "\n",
    "# ---------- 2) M5 n-grammes : version strict (ancienne) et version unbiased (corrigée) ----------\n",
    "\n",
    "def M5_ngram_cached_strict(LexEN, pi_pows: List[np.ndarray], tokens: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    n-gramme STRICT (ancien comportement): superposition positionnelle + seuillage strict (int8).\n",
    "    Attention: ce seuillage introduit un biais si l'accumulateur a des zéros.\n",
    "    \"\"\"\n",
    "    D = LexEN.D\n",
    "    acc = np.zeros(D, dtype=np.int16)\n",
    "    for j, w in enumerate(reversed(tokens)):           # j=0 -> w_t\n",
    "        Lw  = M4_get(LexEN, w).astype(np.int16, copy=False)\n",
    "        acc += Lw[pi_pows[j]]\n",
    "    return _sign_strict_int8(acc)\n",
    "\n",
    "def M5_ngram_cached_unbiased(LexEN, pi_pows: List[np.ndarray], tokens: List[str],\n",
    "                             rng: Optional[np.random.Generator] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    n-gramme UNBIASED (corrigé) : superposition positionnelle + seuillage sans biais (int8).\n",
    "    \"\"\"\n",
    "    D = LexEN.D\n",
    "    acc = np.zeros(D, dtype=np.int16)\n",
    "    for j, w in enumerate(reversed(tokens)):           # j=0 -> w_t\n",
    "        Lw  = M4_get(LexEN, w).astype(np.int16, copy=False)\n",
    "        acc += Lw[pi_pows[j]]\n",
    "    return _sign_unbiased_int8(acc, rng=rng)\n",
    "\n",
    "# ---------- 3) Push robuste : M6_SegAcc_pushE (si dispo) sinon fallback (M2->M3->M6) ----------\n",
    "\n",
    "def _safe_pushE(S: np.ndarray,\n",
    "                E_t: np.ndarray,\n",
    "                Delta: int,\n",
    "                K_s: np.ndarray,\n",
    "                pi_pows: List[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    PUSH robuste:\n",
    "      1) Si M6_SegAcc_pushE est défini et appelable, on l'emploie.\n",
    "      2) Sinon, on calcule X_t = Pi^Delta(E_t), puis on choisit dynamiquement\n",
    "         la bonne signature de M6_SegAcc_push pour éviter tout 'double binding':\n",
    "            - push(S, X_b)      : on bind ICI (X_b = X_t ⊗ K_s)\n",
    "            - push(S, X, K_s)   : on passe X_t non lié + K_s\n",
    "    \"\"\"\n",
    "    # -- Étape 1 : tentative 'API E' si elle existe --\n",
    "    fn_pushE = globals().get(\"M6_SegAcc_pushE\", None)\n",
    "    if callable(fn_pushE):\n",
    "        try:\n",
    "            return fn_pushE(S, E_t, Delta, K_s, pi_pows)\n",
    "        except TypeError:\n",
    "            # mauvaise arité: on bascule sur le fallback explicite\n",
    "            pass\n",
    "\n",
    "    # -- Étape 2 : fallback explicite (M2 -> M3 -> M6) --\n",
    "    # Pi^Delta via indices précalculés\n",
    "    X_t = E_t[pi_pows[Delta]].astype(np.int8, copy=False)\n",
    "\n",
    "    # Récupérer la fonction push 'classique'\n",
    "    fn_push = globals().get(\"M6_SegAcc_push\", None)\n",
    "    if not callable(fn_push):\n",
    "        raise NameError(\"M6_SegAcc_push est introuvable (non défini dans le namespace).\")\n",
    "\n",
    "    # Déterminer la signature de push sans accéder à des symboles inexistants\n",
    "    n_params = len(inspect.signature(fn_push).parameters)\n",
    "    if n_params == 2:\n",
    "        # push(S, X_b) : on bind ICI (sinon on rebinderait deux fois)\n",
    "        X_b = M3_bind(X_t, K_s)\n",
    "        return fn_push(S, X_b)\n",
    "    elif n_params == 3:\n",
    "        # push(S, X, K_s) : on passe X_t NON LIÉ + la clé K_s\n",
    "        return fn_push(S, X_t, K_s)\n",
    "    else:\n",
    "        raise TypeError(\"Signature inattendue pour M6_SegAcc_push (attendu 2 ou 3 paramètres).\")\n",
    "\n",
    "# ---------- 4) Construction de l'état de segment : utiliser les versions 'unbiased' ----------\n",
    "\n",
    "def M7_build_segment_state_strict(tokens: List[str],\n",
    "                                  LexEN,\n",
    "                                  pi_pows: List[np.ndarray],\n",
    "                                  SM,\n",
    "                                  n: int = 3,\n",
    "                                  rng: Optional[np.random.Generator] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Chaînage 'strict' renommé mais corrigé en pratique :\n",
    "      - M5 : version UNBIASED (évite le biais sur ties),\n",
    "      - push via _safe_pushE,\n",
    "      - majority finale UNBIASED.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(0)\n",
    "\n",
    "    D = LexEN.D\n",
    "    S = M6_SegAcc_init(D)\n",
    "    K = M7_curKey(SM)\n",
    "\n",
    "    for t in range(len(tokens)):\n",
    "        left = max(0, t - n + 1)\n",
    "        # n-gramme sans biais\n",
    "        E_t  = M5_ngram_cached_unbiased(LexEN, pi_pows, tokens[left:t+1], rng=rng)\n",
    "        # push robuste (permutation relative + binding + accumulation)\n",
    "        S    = _safe_pushE(S, E_t, Delta=t, K_s=K, pi_pows=pi_pows)\n",
    "\n",
    "    # majority sans biais (ties tirés au sort)\n",
    "    return _sign_unbiased_int8(S, rng=rng)\n",
    "\n",
    "# ---------- 5) Test HA2 : inter-segments centrés et queues faibles ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5bda619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_H_segment(tokens: List[str],\n",
    "                     LexEN,\n",
    "                     pi_pows: List[np.ndarray],\n",
    "                     K: np.ndarray,\n",
    "                     n: int,\n",
    "                     rng: np.random.Generator) -> np.ndarray:\n",
    "    \"\"\"Construit H^(seg) à partir d'une phrase, majorité 'unbiased'.\"\"\"\n",
    "    D = LexEN.D\n",
    "    S = M6_SegAcc_init(D)\n",
    "    for t in range(len(tokens)):\n",
    "        left = max(0, t - n + 1)\n",
    "        E_t  = M5_ngram_cached_unbiased(LexEN, pi_pows, tokens[left:t+1], rng=rng)\n",
    "        S    = _safe_pushE(S, E_t, Delta=t, K_s=K, pi_pows=pi_pows)\n",
    "    # majority unbiased (tirage ±1 si coordonnées nulles)\n",
    "    return _sign_unbiased_int8(S, rng=rng)\n",
    "\n",
    "def estimate_leak_vs_delta(D: int = 16384,\n",
    "                           seed: int = 202,\n",
    "                           V: int = 2000,\n",
    "                           sent_len: int = 12,\n",
    "                           n: int = 3,\n",
    "                           deltas: List[int] = (0,1,2,4),\n",
    "                           trials: int = 400) -> Tuple[np.ndarray,np.ndarray,np.ndarray]:\n",
    "    \"\"\"\n",
    "    Estime la similarité inter-segments en fonction de Δ, avec contrôle de variance.\n",
    "    - Même paires de phrases réutilisées pour tous les Δ (réduction de variance par 'couplage').\n",
    "    - Retourne (deltas, means, ses).\n",
    "    \"\"\"\n",
    "    g   = np.random.default_rng(seed)\n",
    "    Lex = M4_LexEN_new(seed+1, D)\n",
    "    pi  = M2_plan_perm(D, seed+2)\n",
    "\n",
    "    # Pré-calcul Pi^j (indices) pour accélérer M2\n",
    "    pi_pows = [np.arange(D, dtype=np.int64)]\n",
    "    for _ in range(max(sent_len, n) + 2):\n",
    "        pi_pows.append(pi[pi_pows[-1]])\n",
    "\n",
    "    def rand_tok():  return f\"w{int(g.integers(0, V))}\"\n",
    "    def rand_sent(): return [rand_tok() for _ in range(sent_len)]\n",
    "\n",
    "    deltas = np.asarray(deltas, dtype=int)\n",
    "    sims   = np.zeros((len(deltas), trials), dtype=np.float64)\n",
    "\n",
    "    for t in range(trials):\n",
    "        # -- même couple (s, s') pour toutes les valeurs de Δ : variance ↓ --\n",
    "        s  = rand_sent()\n",
    "        sp = rand_sent()\n",
    "\n",
    "        # clés indépendantes\n",
    "        K1 = M7_SegMgr_new(int(g.integers(1, 2**31-1)), D).curKey()\n",
    "        K2 = M7_SegMgr_new(int(g.integers(1, 2**31-1)), D).curKey()\n",
    "\n",
    "        # H^(s) (clé K1)\n",
    "        H1 = _build_H_segment(s, Lex, pi_pows, K1, n=n, rng=g)\n",
    "\n",
    "        # Pour chaque Δ : les Δ premiers tokens de s' sous K1, puis K2\n",
    "        for i, Δ in enumerate(deltas):\n",
    "            S2 = M6_SegAcc_init(D)\n",
    "            for t2 in range(len(sp)):\n",
    "                left = max(0, t2 - n + 1)\n",
    "                E_t  = M5_ngram_cached_unbiased(Lex, pi_pows, sp[left:t2+1], rng=g)\n",
    "                K    = K1 if t2 < Δ else K2\n",
    "                S2   = _safe_pushE(S2, E_t, Delta=t2, K_s=K, pi_pows=pi_pows)\n",
    "            H2 = _sign_unbiased_int8(S2, rng=g)\n",
    "            sims[i, t] = M1_sim(H1.astype(np.int8), H2.astype(np.int8))\n",
    "\n",
    "    means = sims.mean(axis=1)\n",
    "    ses   = sims.std(axis=1, ddof=1) / np.sqrt(trials)\n",
    "    return deltas, means, ses\n",
    "\n",
    "# Exemple d'usage (ira plus vite en diminuant 'trials' pour un test rapide) :\n",
    "# deltas, means, ses = estimate_leak_vs_delta(trials=200)\n",
    "# print(list(zip(deltas, means, ses)))\n",
    "# test_M7_leak_vs_delta_monotone(trials=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99b18a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_M7_reset_vs_intra(D: int = 16384,\n",
    "                           seed: int = 101,\n",
    "                           V: int = 2000,\n",
    "                           sent_len: int = 10,\n",
    "                           n: int = 3,\n",
    "                           trials: int = 300) -> None:\n",
    "    \"\"\"\n",
    "    Deux phrases indépendantes, chacune encodée avec son propre gestionnaire (clés i.i.d.).\n",
    "    Objectif HA2: similarités inter-segments centrées (≈0) et queue |sim|>0.05 faible.\n",
    "    \"\"\"\n",
    "    g   = np.random.default_rng(seed)\n",
    "    Lex = M4_LexEN_new(seed+1, D)\n",
    "    pi  = M2_plan_perm(D, seed+2)\n",
    "\n",
    "    # Pré-calcul Pi^j (indices) jusqu'au Δ max utile\n",
    "    pi_pows = [np.arange(D, dtype=np.int64)]\n",
    "    for _ in range(max(sent_len, n) + 2):\n",
    "        pi_pows.append(pi[pi_pows[-1]])\n",
    "\n",
    "    def rand_tok():  return f\"w{int(g.integers(0, V))}\"\n",
    "    def rand_sent(): return [rand_tok() for _ in range(sent_len)]\n",
    "\n",
    "    sims = []\n",
    "    for _ in range(trials):\n",
    "        SM1 = M7_SegMgr_new(int(g.integers(1, 2**31-1)), D)\n",
    "        SM2 = M7_SegMgr_new(int(g.integers(1, 2**31-1)), D)\n",
    "        s, sp = rand_sent(), rand_sent()\n",
    "\n",
    "        H1 = M7_build_segment_state_strict(s,  Lex, pi_pows, SM1, n=n, rng=g)\n",
    "        H2 = M7_build_segment_state_strict(sp, Lex, pi_pows, SM2, n=n, rng=g)\n",
    "\n",
    "        sims.append(M1_sim(H1.astype(np.int8), H2.astype(np.int8)))\n",
    "\n",
    "    sims = np.asarray(sims, dtype=np.float64)\n",
    "    mean = float(sims.mean())\n",
    "    tail = float((np.abs(sims) > 0.05).mean())\n",
    "\n",
    "    # Acceptation HA2 (centrage ≈ 0 ; queues subgaussiennes)\n",
    "    assert abs(mean) <= 1.5e-2, f\"mean inter = {mean:.4g} trop grand\"\n",
    "    assert tail   <= 0.02,      f\"queue empirique trop élevée: {tail:.3%}\"\n",
    "\n",
    "def test_M7_leak_vs_delta_monotone(D: int = 16384,\n",
    "                                   seed: int = 202,\n",
    "                                   V: int = 2000,\n",
    "                                   sent_len: int = 12,\n",
    "                                   n: int = 3,\n",
    "                                   deltas: List[int] = (0,1,2,4),\n",
    "                                   trials: int = 400) -> None:\n",
    "    \"\"\"\n",
    "    Test 'monotone en espérance' : on autorise un petit chevauchement dû à l'incertitude.\n",
    "    Condition : mean[Δ] <= mean[Δ+1] + 2*(SE_Δ + SE_{Δ+1})\n",
    "    \"\"\"\n",
    "    deltas, means, ses = estimate_leak_vs_delta(D, seed, V, sent_len, n, deltas, trials)\n",
    "    # Vérification monotone tolérante\n",
    "    ok = True\n",
    "    msgs = []\n",
    "    for i in range(len(deltas) - 1):\n",
    "        lhs = means[i]\n",
    "        rhs = means[i+1] + 2.0 * (ses[i] + ses[i+1])\n",
    "        if not (lhs <= rhs):\n",
    "            ok = False\n",
    "            msgs.append(f\"Δ={deltas[i]} -> {deltas[i+1]} : {lhs:.4f} > {rhs:.4f} (tol.)\")\n",
    "    assert ok, \"Non-monotonie au-delà de la tolérance statistique:\\n\" + \"\\n\".join(msgs)\n",
    "\n",
    "\n",
    "def test_M7_correlated_keys_bias(D=16384, seed=303, V=2000, sent_len=10):\n",
    "    g = np.random.default_rng(seed)\n",
    "    Lex = M4_LexEN_new(seed+1, D)\n",
    "    pi  = M2_plan_perm(D, seed+2)\n",
    "    pi_pows = [np.arange(D, dtype=np.int64)]\n",
    "    for j in range(1, sent_len+5):\n",
    "        pi_pows.append(pi[pi_pows[-1]])\n",
    "    SM = M7_SegMgr_new(seed+3, D)\n",
    "\n",
    "    rhos = [0.0, 0.05, 0.1]\n",
    "    means = []\n",
    "    for rho in rhos:\n",
    "        sims = []\n",
    "        for _ in range(40):\n",
    "            s = [f\"w{int(g.integers(0,V))}\" for _ in range(sent_len)]\n",
    "            H1 = M7_build_segment_state_strict(s, Lex, pi_pows, SM)\n",
    "            # Clé suivante corrélée à la précédente\n",
    "            SM.nextKey_correlated(rho)\n",
    "            sp = [f\"w{int(g.integers(0,V))}\" for _ in range(sent_len)]\n",
    "            H2 = M7_build_segment_state_strict(sp, Lex, pi_pows, SM)\n",
    "            sims.append(float(np.dot(H1.astype(np.int32), H2.astype(np.int32)) / D))\n",
    "        means.append(float(np.mean(sims)))\n",
    "        # reset propre\n",
    "        M7_onBoundary(SM)\n",
    "    # Biais croissant avec rho\n",
    "    assert means[0] <= means[1] + 1e-3 <= means[2] + 2e-3, f\"{means}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3c6e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_M7_reset_vs_intra()\n",
    "test_M7_leak_vs_delta_monotone()\n",
    "test_M7_correlated_keys_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce3e93",
   "metadata": {},
   "source": [
    "## M8 . Chaîne ENC (construction empilée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eafd6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M8_ENC(tokens: Iterable[str],\n",
    "           pi: np.ndarray,\n",
    "           n: int,\n",
    "           LexEN: Any,\n",
    "           D: int,\n",
    "           segmgr: Optional[Any] = None,\n",
    "           acc_S: Optional[np.ndarray] = None,\n",
    "           return_bound: bool = False,\n",
    "           pi_pows: Optional[List[np.ndarray]] = None,\n",
    "           majority_mode: str = \"unbiased\",\n",
    "           m5_variant: str = \"auto\"\n",
    "           ) -> Tuple[List[np.ndarray], List[np.ndarray], np.ndarray, np.ndarray] | \\\n",
    "                  Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], np.ndarray, np.ndarray]:\n",
    "    # --- garde-fous ---\n",
    "    assert isinstance(D, int) and D > 0\n",
    "    assert isinstance(n, int) and n >= 1\n",
    "    assert isinstance(pi, np.ndarray) and pi.ndim == 1 and len(pi) == D\n",
    "\n",
    "    # --- gestionnaire / clé ---\n",
    "    if segmgr is None:\n",
    "        seg_seed = int(LexEN.rng.integers(1, 2**31 - 1))\n",
    "        segmgr = M7_SegMgr_new(seed=seg_seed, D=D)\n",
    "    K_s = M7_curKey(segmgr)  # (D,) int8\n",
    "\n",
    "    # --- accumulateur ---\n",
    "    S = acc_S if acc_S is not None else M6_SegAcc_init(D)\n",
    "\n",
    "    # --- pi^j (indices) minimal si absent ---\n",
    "    if pi_pows is None:\n",
    "        pi_pows = [np.arange(D, dtype=np.int64)]\n",
    "        for _ in range(n + 2):\n",
    "            pi_pows.append(pi[pi_pows[-1]])\n",
    "\n",
    "    # --- seuillage ---\n",
    "    rng = getattr(LexEN, \"rng\", np.random.default_rng(0))\n",
    "    def _sign_unbiased_int8(x: np.ndarray, rng: np.random.Generator) -> np.ndarray:\n",
    "        gt = (x > 0); lt = (x < 0); eq = ~(gt | lt)\n",
    "        out = np.empty_like(x, dtype=np.int8)\n",
    "        out[gt] = 1; out[lt] = -1\n",
    "        if np.any(eq):\n",
    "            toss = (rng.integers(0, 2, size=int(eq.sum())) * 2 - 1).astype(np.int8)\n",
    "            out[eq] = toss\n",
    "        return out\n",
    "    def _sign_strict_int8(x: np.ndarray) -> np.ndarray:\n",
    "        return np.where(x > 0, 1, -1).astype(np.int8, copy=False)\n",
    "\n",
    "    # --- choix M5 ---\n",
    "    def _pick_m5(window: Sequence[str]) -> np.ndarray:\n",
    "        if m5_variant in {\"unbiased\",\"auto\"}:\n",
    "            fn_u = globals().get(\"M5_ngram_cached_unbiased\", None)\n",
    "            if callable(fn_u):\n",
    "                return fn_u(LexEN, pi_pows, list(window), rng=rng)\n",
    "            if m5_variant == \"unbiased\":\n",
    "                raise NameError(\"M5_ngram_cached_unbiased indisponible.\")\n",
    "        if m5_variant in {\"strict\",\"auto\"}:\n",
    "            fn_s = globals().get(\"M5_ngram_cached_strict\", None)\n",
    "            if callable(fn_s):\n",
    "                return fn_s(LexEN, pi_pows, list(window))\n",
    "            if m5_variant == \"strict\":\n",
    "                raise NameError(\"M5_ngram_cached_strict indisponible.\")\n",
    "        return M5_ngram(LexEN, pi, list(window))\n",
    "\n",
    "    # --- push robuste ---\n",
    "    def _safe_pushE(S_: np.ndarray,\n",
    "                    E_t: np.ndarray,\n",
    "                    Delta: int,\n",
    "                    K_s_: np.ndarray) -> np.ndarray:\n",
    "        fn_pushE = globals().get(\"M6_SegAcc_pushE\", None)\n",
    "        if callable(fn_pushE):\n",
    "            try:\n",
    "                return fn_pushE(S_, E_t, Delta, K_s_, pi_pows)\n",
    "            except TypeError:\n",
    "                pass\n",
    "        # fallback explicite\n",
    "        idx = pi_pows[Delta] if Delta < len(pi_pows) else np.arange(D, dtype=np.int64)\n",
    "        X_t = E_t[idx].astype(np.int8, copy=False)\n",
    "        X_b = M3_bind(X_t, K_s_)\n",
    "        fn_push = globals().get(\"M6_SegAcc_push\", None)\n",
    "        if not callable(fn_push):\n",
    "            raise NameError(\"M6_SegAcc_push indisponible.\")\n",
    "        import inspect\n",
    "        n_params = len(inspect.signature(fn_push).parameters)\n",
    "        if n_params == 2:\n",
    "            return fn_push(S_, X_b)\n",
    "        elif n_params == 3:\n",
    "            return fn_push(S_, X_t, K_s_)\n",
    "        else:\n",
    "            raise TypeError(\"Signature inattendue pour M6_SegAcc_push (2 ou 3 paramètres attendus).\")\n",
    "\n",
    "    # --- sorties ---\n",
    "    E_seq: List[np.ndarray] = []\n",
    "    X_seq: List[np.ndarray] = []\n",
    "    Xb_seq: Optional[List[np.ndarray]] = [] if return_bound else None\n",
    "\n",
    "    # --- encodage glissant ---\n",
    "    tok_list: List[str] = list(tokens)\n",
    "    for t in range(len(tok_list)):\n",
    "        left   = max(0, t - n + 1)\n",
    "        window = tok_list[left : t + 1]\n",
    "        E_t    = _pick_m5(window)  # (D,) int8\n",
    "        Delta  = t - left\n",
    "        idx    = pi_pows[Delta] if Delta < len(pi_pows) else np.arange(D, dtype=np.int64)\n",
    "        X_t    = E_t[idx].astype(np.int8, copy=False)\n",
    "        # *** Correctif: passer le bon nom de paramètre 'K_s_' ***\n",
    "        S      = _safe_pushE(S, E_t, Delta=Delta, K_s_=K_s)\n",
    "        if return_bound:\n",
    "            Xb = M3_bind(X_t, K_s)\n",
    "            Xb_seq.append(Xb)\n",
    "        E_seq.append(E_t)\n",
    "        X_seq.append(X_t)\n",
    "\n",
    "    # seuillage final\n",
    "    if majority_mode == \"unbiased\":\n",
    "        H_s = _sign_unbiased_int8(S, rng=rng)\n",
    "    elif majority_mode == \"strict\":\n",
    "        H_s = _sign_strict_int8(S)\n",
    "    else:\n",
    "        raise ValueError(\"majority_mode ∈ {'unbiased','strict'}\")\n",
    "\n",
    "    if return_bound:\n",
    "        return E_seq, X_seq, Xb_seq, S, H_s\n",
    "    return E_seq, X_seq, S, H_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ed594c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_pi_pows_with_neg(pi: np.ndarray, max_abs_k: int) -> dict[int, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Pré-calcul des puissances de permutation Pi^k pour k ∈ [-max_abs_k, ..., +max_abs_k].\n",
    "    - pi: permutation de [0..D-1] (np.ndarray (D,) d'ints)\n",
    "    - Retour: dict k -> indices (np.ndarray (D,))\n",
    "    \"\"\"\n",
    "    D = pi.size\n",
    "    # Puissances positives\n",
    "    pows_pos = [np.arange(D, dtype=np.int64)]\n",
    "    for _ in range(max_abs_k):\n",
    "        pows_pos.append(pi[pows_pos[-1]])\n",
    "    # Inverse pour puissances négatives\n",
    "    pi_inv = np.empty_like(pi)\n",
    "    pi_inv[pi] = np.arange(D, dtype=np.int64)\n",
    "    pows_neg = [np.arange(D, dtype=np.int64)]\n",
    "    for _ in range(max_abs_k):\n",
    "        pows_neg.append(pi_inv[pows_neg[-1]])\n",
    "    # Fusion dans un dict\n",
    "    out: dict[int, np.ndarray] = {}\n",
    "    for k in range(0, max_abs_k+1):\n",
    "        out[ k] = pows_pos[k]\n",
    "        out[-k] = pows_neg[k]\n",
    "    return out\n",
    "\n",
    "def test_M8_integration(seed: int = 202, D: int = 16384, n: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Corrigé théorique & pratique.\n",
    "    - Théorie: pour des décalages Δ_a, Δ_b (position relative), \n",
    "      <Pi^{Δ_a}E_a, Pi^{Δ_b}E_b> = <E_a, Pi^{Δ_b-Δ_a}E_b>.\n",
    "      Donc on NE DOIT PAS comparer Gram(E) à Gram(X) directement si Δ diffèrent.\n",
    "      On doit comparer Gram(X) à un Gram(E) 'aligné' par la différence de décalages.\n",
    "    - Pratique: on calcule G_X puis G_E^align avec Pi^{Δ_b-Δ_a}, et on vérifie l'égalité bit-exacte.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # dépendances\n",
    "    Lex = M4_LexEN_new(seed+1, D)\n",
    "    pi  = M2_plan_perm(D, seed+2)\n",
    "\n",
    "    # Deux segments jouets\n",
    "    sent1 = [\"i\",\"love\",\"cats\",\"very\",\"much\"]\n",
    "    sent2 = [\"we\",\"like\",\"music\"]\n",
    "\n",
    "    # Encode chaque segment séparément (on a seulement besoin des séquences {E_t} et {X_t})\n",
    "    n_local = n\n",
    "    E1, X1, S1, H1 = M8_ENC(sent1, pi, n_local, Lex, D)   # mode standard\n",
    "    E2, X2, S2, H2 = M8_ENC(sent2, pi, n_local, Lex, D)\n",
    "\n",
    "    # ---------- (i) Isométrie intra-segment avec alignement correct ----------\n",
    "    # Définir les décalages Δ_t = t - left, où left = max(0, t-n+1)\n",
    "    def deltas_for_len(T: int, n_: int) -> list[int]:\n",
    "        Δ = []\n",
    "        for t in range(T):\n",
    "            left = max(0, t - n_ + 1)\n",
    "            Δ.append(t - left)\n",
    "        return Δ\n",
    "\n",
    "    Δ1 = deltas_for_len(len(E1), n_local)  # ∈ {0,1,...,n-1} (saturation à n-1)\n",
    "    Δ2 = deltas_for_len(len(E2), n_local)\n",
    "\n",
    "    # Pré-calcul Pi^k pour k ∈ [-(n-1), ..., +(n-1)]\n",
    "    pows = _build_pi_pows_with_neg(pi, max_abs_k=n_local-1)\n",
    "\n",
    "    # Empilement en matrices (len x D) en int16 pour produits stables\n",
    "    A1 = np.stack(X1).astype(np.int16, copy=False)  # (T1, D)\n",
    "    B1 = np.stack(E1).astype(np.int16, copy=False)  # (T1, D)\n",
    "\n",
    "    # Gram(X1)\n",
    "    G_X1 = (A1 @ A1.T) / D  # (T1,T1) float64\n",
    "\n",
    "    # Gram(E1) aligné: [a,b] = <E_a, Pi^{Δ_b-Δ_a} E_b> / D\n",
    "    T1 = len(E1)\n",
    "    G_E1_aligned = np.empty((T1, T1), dtype=np.float64)\n",
    "    for a in range(T1):\n",
    "        Ea = B1[a]\n",
    "        for b in range(T1):\n",
    "            k = Δ1[b] - Δ1[a]            # différence de décalages\n",
    "            Eb_shift = B1[b, pows[k]]    # Pi^{k} E_b\n",
    "            G_E1_aligned[a, b] = float(np.dot(Ea, Eb_shift) / D)\n",
    "\n",
    "    # Tolérance: égalité exacte attendue (arithmétique entière / D), donc 0.0\n",
    "    tol_iso = 0.0\n",
    "    max_abs_diff_1 = float(np.max(np.abs(G_E1_aligned - G_X1)))\n",
    "    assert max_abs_diff_1 <= tol_iso + 1e-12, (\n",
    "        f\"Isométrie violée (segment 1): max|Δ|={max_abs_diff_1:.3g} > {tol_iso}\"\n",
    "    )\n",
    "\n",
    "    # Répéter pour le 2e segment (symétrie du raisonnement)\n",
    "    A2 = np.stack(X2).astype(np.int16, copy=False)\n",
    "    B2 = np.stack(E2).astype(np.int16, copy=False)\n",
    "    G_X2 = (A2 @ A2.T) / D\n",
    "    T2 = len(E2)\n",
    "    G_E2_aligned = np.empty((T2, T2), dtype=np.float64)\n",
    "    for a in range(T2):\n",
    "        Ea = B2[a]\n",
    "        for b in range(T2):\n",
    "            k = Δ2[b] - Δ2[a]\n",
    "            Eb_shift = B2[b, pows[k]]\n",
    "            G_E2_aligned[a, b] = float(np.dot(Ea, Eb_shift) / D)\n",
    "\n",
    "    max_abs_diff_2 = float(np.max(np.abs(G_E2_aligned - G_X2)))\n",
    "    assert max_abs_diff_2 <= tol_iso + 1e-12, (\n",
    "        f\"Isométrie violée (segment 2): max|Δ|={max_abs_diff_2:.3g} > {tol_iso}\"\n",
    "    )\n",
    "\n",
    "    # ---------- (ii) Tests de bords utiles ----------\n",
    "    # sim(X_t, X_t) == 1 et dH == 0, idem pour E_t\n",
    "    diag_X1 = np.diag(G_X1)\n",
    "    diag_E1 = np.diag((B1 @ B1.T) / D)\n",
    "    assert np.allclose(diag_X1, 1.0), \"norme de X_t non préservée\"\n",
    "    assert np.allclose(diag_E1, 1.0), \"norme de E_t non préservée\"\n",
    "\n",
    "    # ---------- (iii) Commentaire ----------\n",
    "    # L'ancien test comparait Gram(E) à Gram(X) sans aligner les différences de décalages.\n",
    "    # Or, si Δ_a != Δ_b, on n'a pas <Pi^{Δ_a}E_a, Pi^{Δ_b}E_b> = <E_a, E_b>, \n",
    "    # mais bien <E_a, Pi^{Δ_b-Δ_a}E_b>. Le présent test restitue cette équivariance,\n",
    "    # rendant l'isométrie exacte (tolérance 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7324c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_M8_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e7205",
   "metadata": {},
   "source": [
    "# Encoder HDC (application concrète EN→FR sur OPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "051f3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opus_load_subset(name: str = \"opus_books\", config: str = \"en-fr\",\n",
    "                     split: str = \"train\", N: int = 10_000,\n",
    "                     seed: int = 123) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Charge un sous-ensemble de N paires (EN, FR) depuis huggingface/datasets.\n",
    "    Ne duplique aucune primitive HDC.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(name, config, split=split)\n",
    "    ds = ds.shuffle(seed=seed).select(range(min(N, len(ds))))\n",
    "    ens = [ex[\"translation\"][\"en\"] for ex in ds]\n",
    "    frs = [ex[\"translation\"][\"fr\"] for ex in ds]\n",
    "    return ens, frs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a9a9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en(s: str) -> List[str]:\n",
    "    # cohérent avec _canon_token(w) de M4 : lower+strip, split espaces\n",
    "    return [w.strip().lower() for w in s.split() if w.strip()]\n",
    "\n",
    "def build_vocab_EN(ens: Iterable[str], V: int = 5_000) -> set:\n",
    "    cnt = Counter()\n",
    "    for s in ens:\n",
    "        cnt.update(tokenize_en(s))\n",
    "    # top-V\n",
    "    return set(w for w, _ in cnt.most_common(V))\n",
    "\n",
    "def sentence_to_tokens_EN(s: str, vocab: set) -> List[str]:\n",
    "    toks = tokenize_en(s)\n",
    "    # on garde tous les tokens; M4 gère OOV via pool si demandé\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e86734e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\o'\n",
      "/var/folders/zg/w26r16bd5c11s4xr1f6lb4mh0000gn/T/ipykernel_38343/2942300705.py:18: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  (signature (S, X_b) \\ou\\ (S, X, K_s)).\n"
     ]
    }
   ],
   "source": [
    "# --- utilitaires dtypes ---\n",
    "def _as_int16(x: np.ndarray) -> np.ndarray:\n",
    "    return np.asarray(x, dtype=np.int16)\n",
    "\n",
    "def _as_int8_pm1(x: np.ndarray) -> np.ndarray:\n",
    "    x8 = np.asarray(x, dtype=np.int8)\n",
    "    # (option) vérifs: assert np.all((x8 == -1) | (x8 == 1))\n",
    "    return x8\n",
    "\n",
    "# --- adaptateur robuste vers M6 (avec fallback sans dépendance externe) ---\n",
    "def _safe_push(S: np.ndarray,\n",
    "               X_t: np.ndarray,      # (D,) int8  : vecteur positionné (non lié)\n",
    "               Xb_t: np.ndarray,     # (D,) int8  : X_t bindé avec K_s\n",
    "               K_s: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pousse un pas d'accumulation dans S en couvrant les deux cas:\n",
    "      - \\textbf{M6 présent}: on route vers \\texttt{M6_SegAcc_push} si défini\n",
    "        (signature (S, X_b) \\ou\\ (S, X, K_s)).\n",
    "      - \\textbf{M6 absent}: \\emph{fallback} local \\textbf{sans double-binding}:\n",
    "        S <- S + Xb_t, avec accumulation en int16 et retour int16.\n",
    "    \"\"\"\n",
    "    # 1) Si la primitive M6 existe, on s'y raccorde proprement.\n",
    "    fn_push = globals().get(\"M6_SegAcc_push\", None)\n",
    "    if callable(fn_push):\n",
    "        n_params = len(inspect.signature(fn_push).parameters)\n",
    "        if n_params == 2:      # (S, X_b)\n",
    "            return fn_push(S, _as_int8_pm1(Xb_t))\n",
    "        if n_params == 3:      # (S, X, K_s)\n",
    "            return fn_push(S, _as_int8_pm1(X_t), _as_int8_pm1(K_s))\n",
    "        raise TypeError(\"Signature inattendue pour M6_SegAcc_push (attendu 2 ou 3 paramètres).\")\n",
    "\n",
    "    # 2) Fallback local (aucune API M6 de push disponible):\n",
    "    #    on accumule \\emph{uniquement} le vecteur déjà bindé (évite double-binding).\n",
    "    S16  = _as_int16(S)\n",
    "    Xb16 = _as_int16(Xb_t)\n",
    "    S16 += Xb16\n",
    "    return S16\n",
    "\n",
    "def enc_sentence_ENC(sentence_tokens: List[str],\n",
    "                     n: int,\n",
    "                     pi: np.ndarray,           # plan M2 (permutation de [0..D-1])\n",
    "                     LexEN: Any,               # instance M4_LexEN (.D, .get)\n",
    "                     D: int,\n",
    "                     seg_seed: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Applique la chaîne ENC à une phrase EN (un segment) en respectant:\n",
    "      - position RELATIVE (Δ = t - left),\n",
    "      - pas de double-binding (push robuste),\n",
    "      - dtypes: int8 pour hypervecteurs, int16 pour l'accumulateur.\n",
    "\n",
    "    Retour: dict(E_seq, X_seq, S, H, len, seg_seed)\n",
    "    \"\"\"\n",
    "    # --- gestionnaire de segment (M7) + accumulateur (M6) ---\n",
    "    SegMgr = M7_SegMgr_new(seed=seg_seed, D=D)\n",
    "    K_s    = M7_curKey(SegMgr)                    # (D,) int8 (readonly)\n",
    "    S      = M6_SegAcc_init(D)                    # (D,) int16\n",
    "\n",
    "    E_seq: List[np.ndarray] = []\n",
    "    X_seq: List[np.ndarray] = []\n",
    "\n",
    "    toks = list(sentence_tokens)\n",
    "    for t in range(len(toks)):\n",
    "        # fenêtre n-grammes relative au début courant\n",
    "        left   = max(0, t - n + 1)\n",
    "        window = toks[left : t + 1]\n",
    "\n",
    "        # 1) n-gramme positionnel (M5) : E_t ∈ {-1,+1}^D (int8)\n",
    "        E_t = M5_ngram(LexEN, pi, window)\n",
    "\n",
    "        # 2) permutation RELATIVE (M2) : Δ = t - left ∈ {0,...,n-1}\n",
    "        Delta = t - left\n",
    "        X_t   = M2_perm_pow(E_t, pi, Delta)       # (D,) int8\n",
    "\n",
    "        # 3) binding de tranche (M3)\n",
    "        Xb_t  = M3_bind(X_t, K_s)                 # (D,) int8\n",
    "\n",
    "        # 4) accumulation (M6) via adaptateur (ou fallback local)\n",
    "        S     = _safe_push(S, X_t, Xb_t, K_s)     # (D,) int16\n",
    "\n",
    "        # traces\n",
    "        E_seq.append(E_t); X_seq.append(X_t)\n",
    "\n",
    "    # 5) seuillage majorité (M6)\n",
    "    H_s = M6_SegAcc_sign(S).astype(np.int8, copy=False)\n",
    "\n",
    "    return {\n",
    "        \"E_seq\":   E_seq,\n",
    "        \"X_seq\":   X_seq,\n",
    "        \"S\":       S,\n",
    "        \"H\":       H_s,\n",
    "        \"len\":     len(toks),\n",
    "        \"seg_seed\": seg_seed,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "01d56939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Logging & helpers (communs)\n",
    "# ===========================\n",
    "import logging, inspect\n",
    "from typing import List, Dict, Any, Tuple, Iterable, Sequence, Optional\n",
    "import numpy as np\n",
    "\n",
    "log = logging.getLogger(\"ENC.pipeline\")\n",
    "if not log.handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "def _sign_strict_int8(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Retourne un vecteur int8 dans {-1,+1} avec règle stricte (+1 si x>0, sinon -1).\"\"\"\n",
    "    return np.where(x > 0, 1, -1).astype(np.int8, copy=False)\n",
    "\n",
    "# ===========================================================\n",
    "# Adaptateurs robustes pour l'accumulation (M6) avec fallback\n",
    "# ===========================================================\n",
    "\n",
    "_warned_local_accum = False  # pour ne logger le fallback qu'une seule fois\n",
    "\n",
    "def _safe_push(S: np.ndarray,\n",
    "               X_t: np.ndarray,\n",
    "               X_b: np.ndarray,\n",
    "               K_s: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adaptateur robuste pour l'accumulation M6.\n",
    "    Ordre de préférence:\n",
    "      1) M6_SegAcc_push(S, X_b)                [signature 2 args]\n",
    "      2) M6_SegAcc_push(S, X_t, K_s)           [signature 3 args]\n",
    "      3) Fallback local: S += X_b.astype(int16) (log d'avertissement unique)\n",
    "    Hyp.: S est int16 (shape (D,)); X_t, X_b, K_s sont int8 dans {-1,+1}.\n",
    "    \"\"\"\n",
    "    global _warned_local_accum\n",
    "\n",
    "    fn_push = globals().get(\"M6_SegAcc_push\", None)\n",
    "    if callable(fn_push):\n",
    "        n_params = len(inspect.signature(fn_push).parameters)\n",
    "        if n_params == 2:\n",
    "            # signature (S, X) -> on passe la version déjà bindée\n",
    "            return fn_push(S, X_b)\n",
    "        elif n_params == 3:\n",
    "            # signature (S, X, K) -> on passe X_t non bindé + K_s\n",
    "            return fn_push(S, X_t, K_s)\n",
    "        else:\n",
    "            log.warning(\"Signature inattendue pour M6_SegAcc_push (attendu 2 ou 3 paramètres). \"\n",
    "                        \"Fallback local déclenché.\")\n",
    "\n",
    "    # --- Fallback local: accumulation explicite coordonnée ---\n",
    "    if not _warned_local_accum:\n",
    "        log.warning(\"M6_SegAcc_push introuvable: utilisation d'un accumulateur local \"\n",
    "                    \"S += X_b (int16). Vérifiez l'import de M6.\")\n",
    "        _warned_local_accum = True\n",
    "\n",
    "    # sécurité dtype\n",
    "    if S.dtype != np.int16:\n",
    "        S = S.astype(np.int16, copy=False)\n",
    "    S += X_b.astype(np.int16, copy=False)\n",
    "    return S\n",
    "\n",
    "# ===========================\n",
    "# Encodage corpus (avec logs)\n",
    "# ===========================\n",
    "\n",
    "def encode_corpus_ENC(ens: List[str],\n",
    "                      LexEN,\n",
    "                      pi: np.ndarray,\n",
    "                      D: int, n: int,\n",
    "                      seg_seed0: int = 10_001,\n",
    "                      log_every: int = 1_000) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Encode un corpus de phrases anglaises (liste 'ens') via la chaîne ENC.\n",
    "    - Ajoute des logs de progression toutes les 'log_every' phrases.\n",
    "    - Réutilise strictement les primitives M2..M7 (pas de redondance).\n",
    "    \"\"\"\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    g = np.random.default_rng(seg_seed0)\n",
    "    total = len(ens)\n",
    "    log.info(\"ENC: démarrage encodage %d phrases (n=%d, D=%d)...\", total, n, D)\n",
    "\n",
    "    for i, s in enumerate(ens, 1):\n",
    "        toks = sentence_to_tokens_EN(s, vocab=set())  # le vocab/BPE a déjà été traité côté M4\n",
    "        seg_seed = int(g.integers(1, 2**31-1))\n",
    "        out.append(enc_sentence_ENC(toks, n, pi, LexEN, D, seg_seed))\n",
    "\n",
    "        if i % log_every == 0 or i == total:\n",
    "            log.info(\"ENC: %d/%d phrases encodées (%.1f%%).\", i, total, 100.0 * i / total)\n",
    "\n",
    "    log.info(\"ENC: encodage terminé.\")\n",
    "    return out\n",
    "\n",
    "# ===========================================\n",
    "# Encodage d'une phrase (segment) avec traces\n",
    "# ===========================================\n",
    "\n",
    "def enc_sentence_ENC(sentence_tokens: List[str],\n",
    "                     n: int,\n",
    "                     pi: np.ndarray,           # plan M2\n",
    "                     LexEN,                    # instance M4_LexEN\n",
    "                     D: int,\n",
    "                     seg_seed: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Applique la chaîne ENC à une phrase EN (un segment).\n",
    "    Retourne E_seq, X_seq, S, H et des méta-données utiles (longueur, seed, etc.).\n",
    "    \"\"\"\n",
    "    # 0) gestionnaire de segment + accumulateur\n",
    "    SegMgr = M7_SegMgr_new(seed=seg_seed, D=D)  # M7\n",
    "    K_s    = M7_curKey(SegMgr)                  # (D,) int8 readonly\n",
    "    S      = M6_SegAcc_init(D)                 # (D,) int16\n",
    "\n",
    "    E_seq: List[np.ndarray] = []\n",
    "    X_seq: List[np.ndarray] = []\n",
    "\n",
    "    toks = sentence_tokens\n",
    "    for t in range(len(toks)):\n",
    "        # 1) fenêtre n-grammes (positionnelle) -> E_t (M5)\n",
    "        left   = max(0, t - n + 1)\n",
    "        window = toks[left:t+1]\n",
    "        E_t    = M5_ngram(LexEN, pi, window).astype(np.int8, copy=False)   # (D,) int8\n",
    "\n",
    "        # 2) position relative -> X_t = Pi^{t-left}(E_t) (M2)\n",
    "        Delta  = t - left\n",
    "        X_t    = M2_perm_pow(E_t, pi, Delta).astype(np.int8, copy=False)   # (D,) int8\n",
    "\n",
    "        # 3) binding -> Xb_t (M3)\n",
    "        Xb_t   = M3_bind(X_t, K_s).astype(np.int8, copy=False)             # (D,) int8\n",
    "\n",
    "        # 4) accumulation (M6) via adaptateur (ou fallback local)\n",
    "        S      = _safe_push(S, X_t, Xb_t, K_s)                             # (D,) int16\n",
    "\n",
    "        # 5) traces\n",
    "        E_seq.append(E_t); X_seq.append(X_t)\n",
    "\n",
    "    # 6) seuillage final (majorité). On préfère la règle stricte (évite biais sur 0)\n",
    "    #    si M6_SegAcc_sign n'est pas présent, on applique la version stricte locale.\n",
    "    if \"M6_SegAcc_sign\" in globals() and callable(globals()[\"M6_SegAcc_sign\"]):\n",
    "        H_s = globals()[\"M6_SegAcc_sign\"](S)\n",
    "        if H_s.dtype != np.int8:\n",
    "            H_s = H_s.astype(np.int8, copy=False)\n",
    "    else:\n",
    "        log.warning(\"M6_SegAcc_sign introuvable: seuillage STRICT local appliqué.\")\n",
    "        H_s = _sign_strict_int8(S)\n",
    "\n",
    "    return {\"E_seq\": E_seq, \"X_seq\": X_seq, \"S\": S, \"H\": H_s,\n",
    "            \"len\": len(toks), \"seg_seed\": seg_seed}\n",
    "\n",
    "# =====================================================\n",
    "# Mesures de similarité intra/inter n-grammes (avec log)\n",
    "# =====================================================\n",
    "\n",
    "def intra_inter_ngram_sims(E_seq_list: List[List[np.ndarray]], D: int) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Intra: moyenne des similarités entre n-grammes consécutifs d'une même phrase.\n",
    "    Inter: moyenne des |similarités| entre premiers n-grammes de phrases consécutives.\n",
    "    \"\"\"\n",
    "    log.info(\"Calcul des similarités intra/inter n-grammes...\")\n",
    "    s_intra_vals: List[float] = []\n",
    "    for E_seq in E_seq_list:\n",
    "        for a in range(len(E_seq) - 1):\n",
    "            s_intra_vals.append(M1_sim(E_seq[a], E_seq[a + 1]))\n",
    "    s_intra = float(np.mean(s_intra_vals)) if s_intra_vals else 0.0\n",
    "\n",
    "    s_inter_vals: List[float] = []\n",
    "    for i in range(len(E_seq_list) - 1):\n",
    "        Ei = E_seq_list[i]\n",
    "        Ej = E_seq_list[i + 1]\n",
    "        if Ei and Ej:\n",
    "            s_inter_vals.append(abs(M1_sim(Ei[0], Ej[0])))\n",
    "    s_inter = float(np.mean(s_inter_vals)) if s_inter_vals else 0.0\n",
    "\n",
    "    log.info(\"Similarités n-grammes -> intra=%.4f, inter(abs)=%.4f\", s_intra, s_inter)\n",
    "    return s_intra, s_inter\n",
    "\n",
    "# ======================================\n",
    "# Similarité inter-segments H (avec logs)\n",
    "# ======================================\n",
    "\n",
    "def inter_segment_similarity(H_list: List[np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    |sim(H^{(s)}, H^{(s+1)})| moyen sur segments consécutifs (≈ 0 à grande D).\n",
    "    \"\"\"\n",
    "    log.info(\"Calcul des similarités inter-segments H...\")\n",
    "    vals: List[float] = []\n",
    "    for i in range(len(H_list) - 1):\n",
    "        vals.append(abs(M1_sim(H_list[i].astype(np.int8), H_list[i + 1].astype(np.int8))))\n",
    "    mean_inter = float(np.mean(vals)) if vals else 0.0\n",
    "    log.info(\"Similarité inter-segments (abs mean) = %.5f\", mean_inter)\n",
    "    return mean_inter\n",
    "\n",
    "# =========================================================\n",
    "# Courbes d'erreur de majorité (strict, robuste, avec logs)\n",
    "# =========================================================\n",
    "\n",
    "# -- Adaptateur push 'classique' (M6_SegAcc_push) si pushE indispo --\n",
    "def _safe_push_with_X(S: np.ndarray, X_t: np.ndarray, K_s: np.ndarray) -> np.ndarray:\n",
    "    fn_push = globals().get(\"M6_SegAcc_push\", None)\n",
    "    if not callable(fn_push):\n",
    "        raise NameError(\"Ni M6_SegAcc_pushE ni M6_SegAcc_push n'est disponible dans le namespace.\")\n",
    "    n_params = len(inspect.signature(fn_push).parameters)\n",
    "    if n_params == 2:\n",
    "        # signature (S, X_b): on fournit déjà le binding\n",
    "        X_b = M3_bind(X_t, K_s)\n",
    "        return fn_push(S, X_b)\n",
    "    elif n_params == 3:\n",
    "        # signature (S, X, K_s): on laisse M6 faire le binding\n",
    "        return fn_push(S, X_t, K_s)\n",
    "    else:\n",
    "        raise TypeError(\"Signature inattendue pour M6_SegAcc_push (attendu 2 ou 3 paramètres).\")\n",
    "\n",
    "\n",
    "def majority_error_curve(E_seq_list: List[List[np.ndarray]],\n",
    "                         pi: np.ndarray,\n",
    "                         D: int,\n",
    "                         eta_list: Tuple[float, ...] = (0.0, 0.05, 0.10),\n",
    "                         seed_noise: int = 303) -> Dict[float, List[Tuple[int, float]]]:\n",
    "    \"\"\"\n",
    "    Courbes d'erreur de majorité avec *même clé K_s* pour clean & noisy,\n",
    "    et *même pipeline* (toujours via _safe_push_with_X).\n",
    "    Retourne: {eta: [(m, err_emp_moyenne), ...]}.\n",
    "    \"\"\"\n",
    "    g = np.random.default_rng(seed_noise)\n",
    "\n",
    "    def _flip_noise_pm1(X: np.ndarray, eta: float) -> np.ndarray:\n",
    "        if eta <= 0.0:\n",
    "            return X.astype(np.int8, copy=False)\n",
    "        mask  = (g.random(X.shape[0]) < eta).astype(np.int8)\n",
    "        flips = (1 - 2*mask).astype(np.int8)   # 0->+1, 1->-1\n",
    "        return (X.astype(np.int8, copy=False) * flips).astype(np.int8, copy=False)\n",
    "\n",
    "    per_eta: Dict[float, List[Tuple[int, float]]] = {eta: [] for eta in eta_list}\n",
    "\n",
    "    for E_seq in E_seq_list:\n",
    "        m = len(E_seq)\n",
    "        if m == 0:\n",
    "            continue\n",
    "\n",
    "        # --- 1) même clé pour clean & noisy ---\n",
    "        SM = M7_SegMgr_new(seed=int(g.integers(1, 2**31-1)), D=D)\n",
    "        K  = M7_curKey(SM)  # (D,) int8\n",
    "\n",
    "        # --- 2) baseline clean (même chemin) ---\n",
    "        S_clean = M6_SegAcc_init(D)\n",
    "        for t, E_t in enumerate(E_seq):\n",
    "            # position relative: X_t = Pi^t(E_t)\n",
    "            X_t = M2_perm_pow(E_t, pi, t).astype(np.int8, copy=False)\n",
    "            S_clean = _safe_push_with_X(S_clean, X_t, K)\n",
    "        H_clean = M6_SegAcc_sign(S_clean)\n",
    "\n",
    "        # --- 3) variantes bruitées (même clé & même chemin) ---\n",
    "        for eta in eta_list:\n",
    "            S_noisy = M6_SegAcc_init(D)\n",
    "            for t, E_t in enumerate(E_seq):\n",
    "                X_t = M2_perm_pow(E_t, pi, t).astype(np.int8, copy=False)\n",
    "                X_t = _flip_noise_pm1(X_t, eta)          # seul changement côté noisy\n",
    "                S_noisy = _safe_push_with_X(S_noisy, X_t, K)\n",
    "            H_noisy = M6_SegAcc_sign(S_noisy)\n",
    "\n",
    "            # fraction de bits différents\n",
    "            diff = (H_clean.astype(np.int8) * H_noisy.astype(np.int8) == -1).mean()\n",
    "            per_eta[eta].append((m, float(diff)))\n",
    "\n",
    "    # Agrégation: moyenne par longueur m\n",
    "    reduced: Dict[float, List[Tuple[int, float]]] = {}\n",
    "    for eta, pairs in per_eta.items():\n",
    "        by_m: Dict[int, List[float]] = {}\n",
    "        for m, e in pairs:\n",
    "            by_m.setdefault(m, []).append(e)\n",
    "        reduced[eta] = sorted((m, float(np.mean(es))) for m, es in by_m.items())\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11f7664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- utilitaires locaux pour M6 si les primitives ne sont pas importées ---\n",
    "def _local_segacc_init(D: int) -> np.ndarray:\n",
    "    return np.zeros(D, dtype=np.int16)\n",
    "\n",
    "def _local_bind(X_t: np.ndarray, K_s: np.ndarray) -> np.ndarray:\n",
    "    # retourne X_b en int8 dans {-1,+1}\n",
    "    return (X_t.astype(np.int8, copy=False) * K_s.astype(np.int8, copy=False)).astype(np.int8, copy=False)\n",
    "\n",
    "def _local_segacc_push(S: np.ndarray, X_t: np.ndarray, K_s: np.ndarray) -> np.ndarray:\n",
    "    # accumulation locale: bind + somme en int16\n",
    "    if S.dtype != np.int16:\n",
    "        S = S.astype(np.int16, copy=False)\n",
    "    X_b = _local_bind(X_t, K_s).astype(np.int16, copy=False)\n",
    "    S  += X_b\n",
    "    return S\n",
    "\n",
    "def _local_segacc_sign(S: np.ndarray) -> np.ndarray:\n",
    "    # seuillage STRICT (évite biais sur 0): +1 si S>0, sinon -1\n",
    "    return np.where(S > 0, 1, -1).astype(np.int8, copy=False)\n",
    "\n",
    "# --- adaptateurs génériques vers M6 (avec fallback local si absent) ---\n",
    "def _segacc_init(D: int) -> np.ndarray:\n",
    "    fn = globals().get(\"M6_SegAcc_init\", None)\n",
    "    return fn(D) if callable(fn) else _local_segacc_init(D)\n",
    "\n",
    "def _segacc_sign(S: np.ndarray) -> np.ndarray:\n",
    "    fn = globals().get(\"M6_SegAcc_sign\", None)\n",
    "    return fn(S) if callable(fn) else _local_segacc_sign(S)\n",
    "\n",
    "def _safe_push_with_X(S: np.ndarray, X_t: np.ndarray, K_s: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Essaie M6_SegAcc_push puis retombe sur le push local si absent.\n",
    "    - (S, X_b)  : fournir X_b déjà bindé,\n",
    "    - (S, X, K) : fournir X_t non bindé + K_s,\n",
    "    - sinon     : fallback local (bind + somme int16).\n",
    "    \"\"\"\n",
    "    fn_push = globals().get(\"M6_SegAcc_push\", None)\n",
    "    if callable(fn_push):\n",
    "        try:\n",
    "            n_params = len(inspect.signature(fn_push).parameters)\n",
    "        except (TypeError, ValueError):\n",
    "            n_params = 0\n",
    "        if n_params == 2:\n",
    "            return fn_push(S, _local_bind(X_t, K_s))\n",
    "        if n_params == 3:\n",
    "            return fn_push(S, X_t, K_s)\n",
    "        # signature inattendue -> fallback local\n",
    "    # pas de push M6: fallback local\n",
    "    return _local_segacc_push(S, X_t, K_s)\n",
    "\n",
    "def _safe_push_with_E(S: np.ndarray,\n",
    "                      E_t: np.ndarray,\n",
    "                      Delta: int,\n",
    "                      K_s: np.ndarray,\n",
    "                      pi_pows: List[np.ndarray],\n",
    "                      pi: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Essaie M6_SegAcc_pushE(S, E_t, Delta, K_s, pi_pows); sinon calcule X_t=Pi^Delta(E_t)\n",
    "    et utilise _safe_push_with_X.\n",
    "    \"\"\"\n",
    "    fn_pushE = globals().get(\"M6_SegAcc_pushE\", None)\n",
    "    if callable(fn_pushE):\n",
    "        try:\n",
    "            return fn_pushE(S, E_t, Delta, K_s, pi_pows)\n",
    "        except TypeError:\n",
    "            # mauvaise arité -> fallback X\n",
    "            pass\n",
    "    # fallback: utiliser Pi^Delta côté M2 puis push classique\n",
    "    X_t = M2_perm_pow(E_t, pi, Delta)\n",
    "    return _safe_push_with_X(S, X_t, K_s)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Remplacement sécurisé de majority_error_curve (plus d'erreur)\n",
    "# ------------------------------------------------------------\n",
    "def majority_error_curve(E_seq_list: List[List[np.ndarray]],\n",
    "                         pi: np.ndarray,\n",
    "                         D: int,\n",
    "                         eta_list: Tuple[float, ...] = (0.0, 0.05, 0.10),\n",
    "                         seed_noise: int = 303) -> Dict[float, List[Tuple[int, float]]]:\n",
    "    \"\"\"\n",
    "    Rejoue l'accumulation 'clean' puis 'noisy' (flips sur X_t) pour chaque séquence E_t.\n",
    "    Compatible avec:\n",
    "      - M6_SegAcc_pushE + M6_SegAcc_sign,\n",
    "      - M6_SegAcc_push (2 ou 3 args) + M6_SegAcc_sign,\n",
    "      - ou *aucune* primitive M6 (fallback local sans dépendance).\n",
    "    \"\"\"\n",
    "    g = np.random.default_rng(seed_noise)\n",
    "\n",
    "    use_pushE = callable(globals().get(\"M6_SegAcc_pushE\", None))\n",
    "    pi_pows = None\n",
    "    if use_pushE:\n",
    "        max_m = max((len(seq) for seq in E_seq_list), default=0)\n",
    "        pi_pows = [np.arange(D, dtype=np.int64)]\n",
    "        for _ in range(max_m + 2):\n",
    "            pi_pows.append(pi[pi_pows[-1]])\n",
    "\n",
    "    def _flip_noise_pm1(X: np.ndarray, eta: float) -> np.ndarray:\n",
    "        if eta <= 0.0:\n",
    "            return X.astype(np.int8, copy=False)\n",
    "        mask  = (g.random(X.shape[0]) < eta).astype(np.int8)\n",
    "        flips = (1 - 2*mask).astype(np.int8)  # 0->+1, 1->-1\n",
    "        return (X.astype(np.int8, copy=False) * flips).astype(np.int8, copy=False)\n",
    "\n",
    "    per_eta: Dict[float, List[Tuple[int, float]]] = {eta: [] for eta in eta_list}\n",
    "\n",
    "    for E_seq in E_seq_list:\n",
    "        m = len(E_seq)\n",
    "        if m == 0:\n",
    "            continue\n",
    "\n",
    "        # même clé K_s pour clean/noisy (SegMgr)\n",
    "        SM = M7_SegMgr_new(seed=7_777, D=D)\n",
    "        K  = M7_curKey(SM)\n",
    "\n",
    "        # --- Clean ---\n",
    "        S_clean = _segacc_init(D)\n",
    "        for t, E_t in enumerate(E_seq):\n",
    "            if use_pushE:\n",
    "                S_clean = _safe_push_with_E(S_clean, E_t, Delta=t, K_s=K, pi_pows=pi_pows, pi=pi)\n",
    "            else:\n",
    "                X_t = M2_perm_pow(E_t, pi, t)\n",
    "                S_clean = _safe_push_with_X(S_clean, X_t, K)\n",
    "        H_clean = _segacc_sign(S_clean)\n",
    "\n",
    "        # --- Noisy ---\n",
    "        for eta in eta_list:\n",
    "            S_noisy = _segacc_init(D)\n",
    "            for t, E_t in enumerate(E_seq):\n",
    "                X_t = M2_perm_pow(E_t, pi, t)\n",
    "                X_t = _flip_noise_pm1(X_t, eta)\n",
    "                S_noisy = _safe_push_with_X(S_noisy, X_t, K)\n",
    "            H_noisy = _segacc_sign(S_noisy)\n",
    "\n",
    "            # fraction de bits différents\n",
    "            diff = (H_clean.astype(np.int8) * H_noisy.astype(np.int8) == -1).mean()\n",
    "            per_eta[eta].append((m, float(diff)))\n",
    "\n",
    "    # agrégation: moyenne par longueur m\n",
    "    reduced: Dict[float, List[Tuple[int, float]]] = {}\n",
    "    for eta, pairs in per_eta.items():\n",
    "        by_m: Dict[int, List[float]] = {}\n",
    "        for m, e in pairs:\n",
    "            by_m.setdefault(m, []).append(e)\n",
    "        reduced[eta] = sorted((m, float(np.mean(es))) for m, es in by_m.items())\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "691a2d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "log = logging.getLogger(\"validate_on_opus\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "def validate_on_opus(D: int = 16_384, n: int = 3, N: int = 1000,\n",
    "                     seed_lex: int = 10_123, seed_pi: int = 10_456) -> dict:\n",
    "    log.info(\"Chargement du sous-corpus OPUS (N=%d)...\", N)\n",
    "    ens, frs = opus_load_subset(\"opus_books\", \"en-fr\", \"train\", N=N, seed=2024)\n",
    "\n",
    "    log.info(\"Initialisation du lexique anglais (M4) avec seed=%d et dimension D=%d...\", seed_lex, D)\n",
    "    Lex = M4_LexEN_new(seed_lex, D, reserve_pool=4_096)\n",
    "\n",
    "    log.info(\"Construction du plan de permutation (M2) avec seed=%d...\", seed_pi)\n",
    "    pi  = M2_plan_perm(D, seed_pi)\n",
    "\n",
    "    log.info(\"Encodage des phrases anglaises avec ENC (n=%d, seg_seed0=9001)...\", n)\n",
    "    encoded = encode_corpus_ENC(ens, Lex, pi, D, n, seg_seed0=9_001)\n",
    "\n",
    "    log.info(\"Extraction des séquences E et signatures H...\")\n",
    "    E_list  = [e[\"E_seq\"] for e in encoded]\n",
    "    H_list  = [e[\"H\"] for e in encoded]\n",
    "\n",
    "    log.info(\"Calcul des similarités intra/inter n-grammes...\")\n",
    "    s_intra, s_inter = intra_inter_ngram_sims(E_list, D)\n",
    "\n",
    "    log.info(\"Calcul des similarités inter-segments...\")\n",
    "    s_inter_seg = inter_segment_similarity(H_list)\n",
    "\n",
    "    log.info(\"Calcul des courbes d'erreur de majorité (eta = 0, 0.05, 0.1)...\")\n",
    "    maj_curves = majority_error_curve(E_list, pi, D, eta_list=(0.0, 0.05, 0.1))\n",
    "\n",
    "    log.info(\"Compilation des résultats...\")\n",
    "    results = {\n",
    "        \"D\": D, \"n\": n, \"N_pairs\": N,\n",
    "        \"seed_lex\": seed_lex, \"seed_pi\": seed_pi,\n",
    "        \"intra_ngram_mean_sim\": s_intra,\n",
    "        \"inter_ngram_abs_mean_sim\": s_inter,\n",
    "        \"inter_segment_abs_mean_sim\": s_inter_seg,\n",
    "        \"majority_curves\": maj_curves\n",
    "    }\n",
    "\n",
    "    log.info(\"Validation terminée avec succès.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10812190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 15:13:40,234 [INFO] Chargement du sous-corpus OPUS (N=1000)...\n",
      "2025-09-30 15:13:43,229 [INFO] Initialisation du lexique anglais (M4) avec seed=10123 et dimension D=16384...\n",
      "2025-09-30 15:13:43,293 [INFO] Construction du plan de permutation (M2) avec seed=10456...\n",
      "2025-09-30 15:13:43,295 [INFO] Encodage des phrases anglaises avec ENC (n=3, seg_seed0=9001)...\n",
      "2025-09-30 15:13:43,295 [INFO] ENC: démarrage encodage 1000 phrases (n=3, D=16384)...\n",
      "2025-09-30 15:13:43,300 [WARNING] M6_SegAcc_push introuvable: utilisation d'un accumulateur local S += X_b (int16). Vérifiez l'import de M6.\n",
      "2025-09-30 15:15:05,225 [INFO] ENC: 1000/1000 phrases encodées (100.0%).\n",
      "2025-09-30 15:15:05,225 [INFO] ENC: encodage terminé.\n",
      "2025-09-30 15:15:05,225 [INFO] Extraction des séquences E et signatures H...\n",
      "2025-09-30 15:15:05,225 [INFO] Calcul des similarités intra/inter n-grammes...\n",
      "2025-09-30 15:15:05,226 [INFO] Calcul des similarités intra/inter n-grammes...\n",
      "2025-09-30 15:15:05,468 [INFO] Similarités n-grammes -> intra=0.0004, inter(abs)=0.0212\n",
      "2025-09-30 15:15:05,468 [INFO] Calcul des similarités inter-segments...\n",
      "2025-09-30 15:15:05,469 [INFO] Calcul des similarités inter-segments H...\n",
      "2025-09-30 15:15:05,480 [INFO] Similarité inter-segments (abs mean) = 0.01030\n",
      "2025-09-30 15:15:05,480 [INFO] Calcul des courbes d'erreur de majorité (eta = 0, 0.05, 0.1)...\n",
      "2025-09-30 15:20:17,417 [INFO] Compilation des résultats...\n",
      "2025-09-30 15:20:17,418 [INFO] Validation terminée avec succès.\n"
     ]
    }
   ],
   "source": [
    "res = validate_on_opus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32617cf",
   "metadata": {},
   "source": [
    "### Tester l'hypothèse ENC3 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b419e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Courbes de majorité ENC3 \"pur\" : vecteur répété + flips (simulation)\n",
    "# ===========================================================\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def majority_curve_repeated_vector(E_seq_list: List[List[np.ndarray]],\n",
    "                                   pi: np.ndarray,\n",
    "                                   D: int,\n",
    "                                   eta_list: Tuple[float, ...] = (0.0, 0.05, 0.10),\n",
    "                                   trials_per_m: int = 4000,\n",
    "                                   seed: int = 4242) -> Dict[float, List[Tuple[int, float]]]:\n",
    "    \"\"\"\n",
    "    Calcule des courbes d'erreur pour le vote majoritaire en condition ENC3 \"pur\":\n",
    "      - On répète m fois le *même* hypervecteur de référence X_ref ∈ {-1,+1}^D.\n",
    "      - À chaque répétition, chaque coordonnée subit un flip indépendant de probabilité eta.\n",
    "      - On estime l'erreur bit par bit: err(m, eta) = P[sum_{k=1..m} Y_k <= 0], où Y_k ∈ {-1,+1},\n",
    "        P(Y_k=+1) = 1-eta, P(Y_k=-1) = eta. (Ties comptées comme erreurs ➜ règle *stricte*.)\n",
    "      - L'erreur ne dépend PAS de X_ref ni de D (i.i.d. par coordonnée), mais on renvoie les points\n",
    "        pour un ensemble de longueurs m *observées* dans le corpus (pour rester commensurable avec\n",
    "        vos longueurs de phrases).\n",
    "    Paramètres\n",
    "    ----------\n",
    "    E_seq_list : liste des séquences d'E_t (utilisée uniquement pour produire la grille des m rencontrés).\n",
    "    pi, D      : non utilisés dans ce protocole (présents pour signature homogène).\n",
    "    eta_list   : niveaux de bruit à tester (flips).\n",
    "    trials_per_m : nombre d'essais Monte Carlo par longueur m (plus grand ➜ variance plus faible).\n",
    "    seed       : seed RNG.\n",
    "    Retour\n",
    "    ------\n",
    "    dict: {eta: [(m, err_empirique), ...]} avec m croissants.\n",
    "    \"\"\"\n",
    "    # Grille des longueurs m rencontrées dans le corpus (hors m=0)\n",
    "    m_grid = sorted({len(seq) for seq in E_seq_list if len(seq) > 0})\n",
    "    if not m_grid:\n",
    "        return {eta: [] for eta in eta_list}\n",
    "\n",
    "    g = np.random.default_rng(seed)\n",
    "    out: Dict[float, List[Tuple[int, float]]] = {eta: [] for eta in eta_list}\n",
    "\n",
    "    for eta in eta_list:\n",
    "        # Rappel: p = P(Y=+1) = 1 - eta\n",
    "        p = 1.0 - float(eta)\n",
    "        for m in m_grid:\n",
    "            # On simule 'trials_per_m' expériences indépendantes:\n",
    "            # Pour chaque expérience, on tire m variables de Bernoulli(p) puis on\n",
    "            # mappe {1 -> +1, 0 -> -1}, on somme et on teste (sum > 0 ? ok : erreur).\n",
    "            # Avec règle stricte, ties (sum==0) sont des erreurs.\n",
    "            # NB: on n'a pas besoin de dimension D ici, l'événement est par coordonnée.\n",
    "            B = (g.random(size=(trials_per_m, m)) < p)    # True ~ +1, False ~ -1\n",
    "            Y = np.where(B, 1, -1).astype(np.int16)       # (trials, m)\n",
    "            S = Y.sum(axis=1)                              # (trials,)\n",
    "            err = float((S <= 0).mean())                  # tie inclus comme erreur\n",
    "            out[eta].append((m, err))\n",
    "\n",
    "    # Tri des listes par m (pour un affichage propre)\n",
    "    for eta in eta_list:\n",
    "        out[eta].sort(key=lambda t: t[0])\n",
    "    return out\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# Adaptation: validate_on_opus utilise now majority_curve_repeated_vector\n",
    "# ===========================================================\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(\"validate_on_opus\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "def validate_on_opus_enc3(D: int = 16_384, n: int = 3, N: int = 1000,\n",
    "                     seed_lex: int = 10_123, seed_pi: int = 10_456) -> dict:\n",
    "    log.info(\"Chargement du sous-corpus OPUS (N=%d)...\", N)\n",
    "    ens, frs = opus_load_subset(\"opus_books\", \"en-fr\", \"train\", N=N, seed=2024)\n",
    "\n",
    "    log.info(\"Initialisation du lexique anglais (M4) avec seed=%d et dimension D=%d...\", seed_lex, D)\n",
    "    Lex = M4_LexEN_new(seed_lex, D, reserve_pool=4_096)\n",
    "\n",
    "    log.info(\"Construction du plan de permutation (M2) avec seed=%d...\", seed_pi)\n",
    "    pi  = M2_plan_perm(D, seed_pi)\n",
    "\n",
    "    log.info(\"Encodage des phrases anglaises avec ENC (n=%d, seg_seed0=9001)...\", n)\n",
    "    encoded = encode_corpus_ENC(ens, Lex, pi, D, n, seg_seed0=9_001)\n",
    "\n",
    "    log.info(\"Extraction des séquences E et signatures H...\")\n",
    "    E_list  = [e[\"E_seq\"] for e in encoded]\n",
    "    H_list  = [e[\"H\"] for e in encoded]\n",
    "\n",
    "    log.info(\"Calcul des similarités intra/inter n-grammes...\")\n",
    "    s_intra, s_inter = intra_inter_ngram_sims(E_list, D)\n",
    "\n",
    "    log.info(\"Calcul des similarités inter-segments...\")\n",
    "    s_inter_seg = inter_segment_similarity(H_list)\n",
    "\n",
    "    log.info(\"Calcul des courbes d'erreur de majorité ENC3 (vecteur répété) pour eta = 0, 0.05, 0.1...\")\n",
    "    # ➜ Remplace l’ancien protocole 'majority_error_curve' (pipeline réel)\n",
    "    maj_curves = majority_curve_repeated_vector(E_list, pi, D, eta_list=(0.0, 0.05, 0.1),\n",
    "                                                trials_per_m=4000, seed=4242)\n",
    "\n",
    "    log.info(\"Compilation des résultats...\")\n",
    "    results = {\n",
    "        \"D\": D, \"n\": n, \"N_pairs\": N,\n",
    "        \"seed_lex\": seed_lex, \"seed_pi\": seed_pi,\n",
    "        \"intra_ngram_mean_sim\": s_intra,\n",
    "        \"inter_ngram_abs_mean_sim\": s_inter,\n",
    "        \"inter_segment_abs_mean_sim\": s_inter_seg,\n",
    "        \"majority_curves\": maj_curves\n",
    "    }\n",
    "\n",
    "    log.info(\"Validation terminée avec succès.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed1db86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 15:23:20,995 [INFO] Chargement du sous-corpus OPUS (N=1000)...\n",
      "2025-09-30 15:23:23,189 [INFO] Initialisation du lexique anglais (M4) avec seed=10123 et dimension D=16384...\n",
      "2025-09-30 15:23:23,254 [INFO] Construction du plan de permutation (M2) avec seed=10456...\n",
      "2025-09-30 15:23:23,256 [INFO] Encodage des phrases anglaises avec ENC (n=3, seg_seed0=9001)...\n",
      "2025-09-30 15:23:23,256 [INFO] ENC: démarrage encodage 1000 phrases (n=3, D=16384)...\n",
      "2025-09-30 15:24:44,426 [INFO] ENC: 1000/1000 phrases encodées (100.0%).\n",
      "2025-09-30 15:24:44,426 [INFO] ENC: encodage terminé.\n",
      "2025-09-30 15:24:44,426 [INFO] Extraction des séquences E et signatures H...\n",
      "2025-09-30 15:24:44,427 [INFO] Calcul des similarités intra/inter n-grammes...\n",
      "2025-09-30 15:24:44,427 [INFO] Calcul des similarités intra/inter n-grammes...\n",
      "2025-09-30 15:24:44,670 [INFO] Similarités n-grammes -> intra=0.0004, inter(abs)=0.0212\n",
      "2025-09-30 15:24:44,670 [INFO] Calcul des similarités inter-segments...\n",
      "2025-09-30 15:24:44,670 [INFO] Calcul des similarités inter-segments H...\n",
      "2025-09-30 15:24:44,682 [INFO] Similarité inter-segments (abs mean) = 0.01030\n",
      "2025-09-30 15:24:44,682 [INFO] Calcul des courbes d'erreur de majorité ENC3 (vecteur répété) pour eta = 0, 0.05, 0.1...\n",
      "2025-09-30 15:24:44,902 [INFO] Compilation des résultats...\n",
      "2025-09-30 15:24:44,902 [INFO] Validation terminée avec succès.\n"
     ]
    }
   ],
   "source": [
    "res_enc3 = validate_on_opus_enc3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "251251e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D': 16384,\n",
       " 'n': 3,\n",
       " 'N_pairs': 1000,\n",
       " 'seed_lex': 10123,\n",
       " 'seed_pi': 10456,\n",
       " 'intra_ngram_mean_sim': 0.0003932793581665431,\n",
       " 'inter_ngram_abs_mean_sim': 0.021165819139451952,\n",
       " 'inter_segment_abs_mean_sim': 0.010303027637012012,\n",
       " 'majority_curves': {0.0: [(1, 0.0),\n",
       "   (2, 0.0),\n",
       "   (3, 0.0),\n",
       "   (4, 0.0),\n",
       "   (5, 0.0),\n",
       "   (6, 0.0),\n",
       "   (7, 0.0),\n",
       "   (8, 0.0),\n",
       "   (9, 0.0),\n",
       "   (10, 0.0),\n",
       "   (11, 0.0),\n",
       "   (12, 0.0),\n",
       "   (13, 0.0),\n",
       "   (14, 0.0),\n",
       "   (15, 0.0),\n",
       "   (16, 0.0),\n",
       "   (17, 0.0),\n",
       "   (18, 0.0),\n",
       "   (19, 0.0),\n",
       "   (20, 0.0),\n",
       "   (21, 0.0),\n",
       "   (22, 0.0),\n",
       "   (23, 0.0),\n",
       "   (24, 0.0),\n",
       "   (25, 0.0),\n",
       "   (26, 0.0),\n",
       "   (27, 0.0),\n",
       "   (28, 0.0),\n",
       "   (29, 0.0),\n",
       "   (30, 0.0),\n",
       "   (31, 0.0),\n",
       "   (32, 0.0),\n",
       "   (33, 0.0),\n",
       "   (34, 0.0),\n",
       "   (35, 0.0),\n",
       "   (36, 0.0),\n",
       "   (37, 0.0),\n",
       "   (38, 0.0),\n",
       "   (39, 0.0),\n",
       "   (40, 0.0),\n",
       "   (41, 0.0),\n",
       "   (42, 0.0),\n",
       "   (43, 0.0),\n",
       "   (44, 0.0),\n",
       "   (45, 0.0),\n",
       "   (46, 0.0),\n",
       "   (47, 0.0),\n",
       "   (48, 0.0),\n",
       "   (49, 0.0),\n",
       "   (50, 0.0),\n",
       "   (51, 0.0),\n",
       "   (52, 0.0),\n",
       "   (53, 0.0),\n",
       "   (54, 0.0),\n",
       "   (55, 0.0),\n",
       "   (57, 0.0),\n",
       "   (58, 0.0),\n",
       "   (60, 0.0),\n",
       "   (61, 0.0),\n",
       "   (62, 0.0),\n",
       "   (64, 0.0),\n",
       "   (65, 0.0),\n",
       "   (66, 0.0),\n",
       "   (67, 0.0),\n",
       "   (70, 0.0),\n",
       "   (71, 0.0),\n",
       "   (72, 0.0),\n",
       "   (73, 0.0),\n",
       "   (74, 0.0),\n",
       "   (76, 0.0),\n",
       "   (78, 0.0),\n",
       "   (82, 0.0),\n",
       "   (88, 0.0),\n",
       "   (90, 0.0),\n",
       "   (96, 0.0),\n",
       "   (106, 0.0),\n",
       "   (111, 0.0),\n",
       "   (117, 0.0),\n",
       "   (140, 0.0),\n",
       "   (143, 0.0),\n",
       "   (150, 0.0),\n",
       "   (157, 0.0),\n",
       "   (161, 0.0)],\n",
       "  0.05: [(1, 0.053),\n",
       "   (2, 0.0965),\n",
       "   (3, 0.00825),\n",
       "   (4, 0.01825),\n",
       "   (5, 0.001),\n",
       "   (6, 0.00225),\n",
       "   (7, 0.0),\n",
       "   (8, 0.0),\n",
       "   (9, 0.00025),\n",
       "   (10, 0.00025),\n",
       "   (11, 0.0),\n",
       "   (12, 0.0),\n",
       "   (13, 0.0),\n",
       "   (14, 0.0),\n",
       "   (15, 0.0),\n",
       "   (16, 0.0),\n",
       "   (17, 0.0),\n",
       "   (18, 0.0),\n",
       "   (19, 0.0),\n",
       "   (20, 0.0),\n",
       "   (21, 0.0),\n",
       "   (22, 0.0),\n",
       "   (23, 0.0),\n",
       "   (24, 0.0),\n",
       "   (25, 0.0),\n",
       "   (26, 0.0),\n",
       "   (27, 0.0),\n",
       "   (28, 0.0),\n",
       "   (29, 0.0),\n",
       "   (30, 0.0),\n",
       "   (31, 0.0),\n",
       "   (32, 0.0),\n",
       "   (33, 0.0),\n",
       "   (34, 0.0),\n",
       "   (35, 0.0),\n",
       "   (36, 0.0),\n",
       "   (37, 0.0),\n",
       "   (38, 0.0),\n",
       "   (39, 0.0),\n",
       "   (40, 0.0),\n",
       "   (41, 0.0),\n",
       "   (42, 0.0),\n",
       "   (43, 0.0),\n",
       "   (44, 0.0),\n",
       "   (45, 0.0),\n",
       "   (46, 0.0),\n",
       "   (47, 0.0),\n",
       "   (48, 0.0),\n",
       "   (49, 0.0),\n",
       "   (50, 0.0),\n",
       "   (51, 0.0),\n",
       "   (52, 0.0),\n",
       "   (53, 0.0),\n",
       "   (54, 0.0),\n",
       "   (55, 0.0),\n",
       "   (57, 0.0),\n",
       "   (58, 0.0),\n",
       "   (60, 0.0),\n",
       "   (61, 0.0),\n",
       "   (62, 0.0),\n",
       "   (64, 0.0),\n",
       "   (65, 0.0),\n",
       "   (66, 0.0),\n",
       "   (67, 0.0),\n",
       "   (70, 0.0),\n",
       "   (71, 0.0),\n",
       "   (72, 0.0),\n",
       "   (73, 0.0),\n",
       "   (74, 0.0),\n",
       "   (76, 0.0),\n",
       "   (78, 0.0),\n",
       "   (82, 0.0),\n",
       "   (88, 0.0),\n",
       "   (90, 0.0),\n",
       "   (96, 0.0),\n",
       "   (106, 0.0),\n",
       "   (111, 0.0),\n",
       "   (117, 0.0),\n",
       "   (140, 0.0),\n",
       "   (143, 0.0),\n",
       "   (150, 0.0),\n",
       "   (157, 0.0),\n",
       "   (161, 0.0)],\n",
       "  0.1: [(1, 0.0935),\n",
       "   (2, 0.193),\n",
       "   (3, 0.02775),\n",
       "   (4, 0.04875),\n",
       "   (5, 0.00875),\n",
       "   (6, 0.016),\n",
       "   (7, 0.00225),\n",
       "   (8, 0.0055),\n",
       "   (9, 0.0005),\n",
       "   (10, 0.00225),\n",
       "   (11, 0.00025),\n",
       "   (12, 0.0015),\n",
       "   (13, 0.00025),\n",
       "   (14, 0.0),\n",
       "   (15, 0.00025),\n",
       "   (16, 0.0),\n",
       "   (17, 0.0),\n",
       "   (18, 0.0),\n",
       "   (19, 0.0),\n",
       "   (20, 0.0),\n",
       "   (21, 0.0),\n",
       "   (22, 0.0),\n",
       "   (23, 0.0),\n",
       "   (24, 0.0),\n",
       "   (25, 0.0),\n",
       "   (26, 0.0),\n",
       "   (27, 0.0),\n",
       "   (28, 0.0),\n",
       "   (29, 0.0),\n",
       "   (30, 0.0),\n",
       "   (31, 0.0),\n",
       "   (32, 0.0),\n",
       "   (33, 0.0),\n",
       "   (34, 0.0),\n",
       "   (35, 0.0),\n",
       "   (36, 0.0),\n",
       "   (37, 0.0),\n",
       "   (38, 0.0),\n",
       "   (39, 0.0),\n",
       "   (40, 0.0),\n",
       "   (41, 0.0),\n",
       "   (42, 0.0),\n",
       "   (43, 0.0),\n",
       "   (44, 0.0),\n",
       "   (45, 0.0),\n",
       "   (46, 0.0),\n",
       "   (47, 0.0),\n",
       "   (48, 0.0),\n",
       "   (49, 0.0),\n",
       "   (50, 0.0),\n",
       "   (51, 0.0),\n",
       "   (52, 0.0),\n",
       "   (53, 0.0),\n",
       "   (54, 0.0),\n",
       "   (55, 0.0),\n",
       "   (57, 0.0),\n",
       "   (58, 0.0),\n",
       "   (60, 0.0),\n",
       "   (61, 0.0),\n",
       "   (62, 0.0),\n",
       "   (64, 0.0),\n",
       "   (65, 0.0),\n",
       "   (66, 0.0),\n",
       "   (67, 0.0),\n",
       "   (70, 0.0),\n",
       "   (71, 0.0),\n",
       "   (72, 0.0),\n",
       "   (73, 0.0),\n",
       "   (74, 0.0),\n",
       "   (76, 0.0),\n",
       "   (78, 0.0),\n",
       "   (82, 0.0),\n",
       "   (88, 0.0),\n",
       "   (90, 0.0),\n",
       "   (96, 0.0),\n",
       "   (106, 0.0),\n",
       "   (111, 0.0),\n",
       "   (117, 0.0),\n",
       "   (140, 0.0),\n",
       "   (143, 0.0),\n",
       "   (150, 0.0),\n",
       "   (157, 0.0),\n",
       "   (161, 0.0)]}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_enc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62974c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

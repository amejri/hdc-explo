{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore_bis_v5 — pipeline ENC → MEM → DEC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a426dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using src path: /Users/aymenmejri/Desktop/MyCode/experiments/hdc_v2/hdc_project/src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "SRC = ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "print(f\"Using src path: {SRC}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087c8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hdc_project.encoder import m4, pipeline as enc_pipeline\n",
    "from hdc_project.encoder.mem import pipeline as mem_pipeline\n",
    "from hdc_project.decoder import (\n",
    "    DD1_ctx,\n",
    "    DD2_query,\n",
    "    DD2_query_bin,\n",
    "    DD3_bindToMem,\n",
    "    DD4_search_topK,\n",
    "    DD5_payload,\n",
    "    DD6_vote,\n",
    "    DD7_updateLM,\n",
    "    DecodeOneStep,\n",
    "    DX2_run,\n",
    "    DX3_run,\n",
    "    DX4_run,\n",
    "    DX5_run,\n",
    "    DX6_run,\n",
    "    DX7_run,\n",
    ")\n",
    "from hdc_project.decoder.dec import (\n",
    "    hd_assert_pm1,\n",
    "    hd_bind,\n",
    "    hd_sim,\n",
    "    hd_sim_dot,\n",
    "    build_perm_inverse,\n",
    "    permute_pow,\n",
    "    permute_pow_signed,\n",
    "    rademacher,\n",
    "    _as_vocab_from_buckets,\n",
    ")\n",
    "\n",
    "log = logging.getLogger(\"explore_v5\")\n",
    "if not log.handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e16aba",
   "metadata": {},
   "source": [
    "## 1. Chargement du corpus et encodage ENC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00954696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:28:19,161 [INFO] OPUS subset loaded: 20000 pairs\n",
      "2025-10-07 23:28:19,162 [INFO] Using 5000 sentence pairs\n",
      "2025-10-07 23:28:20,918 [INFO] Loaded cached lexicon lex_en_D8192.npz\n",
      "2025-10-07 23:28:49,664 [INFO] Loaded cached lexicon lex_fr_mem_D8192.npz\n",
      "2025-10-07 23:28:49,672 [INFO] Instantiated new lexicon lex_fr_lm_D8192.npz\n",
      "2025-10-07 23:32:43,994 [INFO] Encoded 5000 EN / 5000 FR sentences\n",
      "2025-10-07 23:33:30,719 [INFO] Encoder signature shape: (8192,)\n",
      "2025-10-07 23:34:04,766 [INFO] ENC stats — intra: 0.0003 | inter(|.|): 0.0255 | inter segments: 0.0088\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCES = 5_000\n",
    "N_OPUS = 20_000\n",
    "\n",
    "try:\n",
    "    ens_raw, frs_raw = enc_pipeline.opus_load_subset(\n",
    "        name=\"opus_books\",\n",
    "        config=\"en-fr\",\n",
    "        split=\"train\",\n",
    "        N=N_OPUS,\n",
    "        seed=2025,\n",
    "    )\n",
    "    log.info(\"OPUS subset loaded: %d pairs\", len(ens_raw))\n",
    "except Exception as exc:\n",
    "    log.warning(\"OPUS download failed (%s); using a local toy corpus\", exc)\n",
    "    ens_raw = [\n",
    "        \"hyperdimensional computing is fun\",\n",
    "        \"vector symbolic architectures are powerful\",\n",
    "        \"encoding words into hyperspace\",\n",
    "        \"memory augmented networks love clean data\",\n",
    "    ]\n",
    "    frs_raw = [\n",
    "        \"le calcul hyperdimensionnel est amusant\",\n",
    "        \"les architectures symboliques vectorielles sont puissantes\",\n",
    "        \"encoder des mots dans l'hyperspace\",\n",
    "        \"les réseaux augmentés de mémoire aiment les données propres\",\n",
    "    ]\n",
    "\n",
    "ens_sample = ens_raw[:MAX_SENTENCES]\n",
    "frs_sample = frs_raw[:MAX_SENTENCES]\n",
    "log.info(\"Using %d sentence pairs\", len(ens_sample))\n",
    "\n",
    "D = 8_192\n",
    "n = 5\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "ARTIFACTS = ROOT / \"artifacts\"\n",
    "ARTIFACTS.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def _load_or_build_lex(path: Path, seed: int) -> m4.M4_LexEN:\n",
    "    if path.exists():\n",
    "        try:\n",
    "            lex = m4.M4_LexEN.load(str(path))\n",
    "            if lex.D != D:\n",
    "                raise ValueError(\"dimension mismatch\")\n",
    "            log.info(\"Loaded cached lexicon %s\", path.name)\n",
    "            return lex\n",
    "        except Exception as exc:  # pragma: no cover - fallback\n",
    "            log.warning(\"Failed to load %s (%s); rebuilding\", path.name, exc)\n",
    "            path.unlink(missing_ok=True)\n",
    "    lex = m4.M4_LexEN_new(seed=seed, D=D)\n",
    "    log.info(\"Instantiated new lexicon %s\", path.name)\n",
    "    return lex\n",
    "\n",
    "\n",
    "lex_en_path = ARTIFACTS / f\"lex_en_D{D}.npz\"\n",
    "lex_fr_mem_path = ARTIFACTS / f\"lex_fr_mem_D{D}.npz\"\n",
    "lex_fr_lm_path = ARTIFACTS / f\"lex_fr_lm_D{D}.npz\"\n",
    "\n",
    "Lex_en = _load_or_build_lex(lex_en_path, seed=1)\n",
    "Lex_fr_mem = _load_or_build_lex(lex_fr_mem_path, seed=2)\n",
    "Lex_fr_lm = _load_or_build_lex(lex_fr_lm_path, seed=202)\n",
    "\n",
    "# MEM/DEC compatibility contract: reuse the exact same callable.\n",
    "L_FR_PAYLOAD = Lex_fr_mem.get  # payload lexicon (MEM & DEC share this object)\n",
    "L_FR_LM = Lex_fr_lm.get  # LM-only lexicon (separate semantic space)\n",
    "\n",
    "pi = rng.permutation(D).astype(np.int64)\n",
    "pi_inv = build_perm_inverse(pi)\n",
    "\n",
    "encoded_en = enc_pipeline.encode_corpus_ENC(ens_sample, Lex_en, pi, D, n, seg_seed0=999)\n",
    "encoded_fr = enc_pipeline.encode_corpus_ENC(frs_sample, Lex_fr_mem, pi, D, n, seg_seed0=1999)\n",
    "log.info(\"Encoded %d EN / %d FR sentences\", len(encoded_en), len(encoded_fr))\n",
    "\n",
    "E_list_en = [segment[\"E_seq\"] for segment in encoded_en]\n",
    "H_list_en = [segment[\"H\"] for segment in encoded_en]\n",
    "if H_list_en:\n",
    "    log.info(\"Encoder signature shape: %s\", H_list_en[0].shape)\n",
    "\n",
    "intra_sim, inter_sim = enc_pipeline.intra_inter_ngram_sims(E_list_en, D)\n",
    "inter_seg = enc_pipeline.inter_segment_similarity(H_list_en)\n",
    "log.info(\n",
    "    \"ENC stats — intra: %.4f | inter(|.|): %.4f | inter segments: %.4f\",\n",
    "    intra_sim,\n",
    "    inter_sim,\n",
    "    inter_seg,\n",
    ")\n",
    "\n",
    "\n",
    "def _maybe_save_lex(path: Path, lex: m4.M4_LexEN) -> None:\n",
    "    if path.exists():\n",
    "        return\n",
    "    table = getattr(lex, '_table', {})\n",
    "    if table:\n",
    "        lex.save(str(path))\n",
    "        log.info('Saved lexicon %s', path.name)\n",
    "\n",
    "\n",
    "for path, lex in ((lex_en_path, Lex_en), (lex_fr_mem_path, Lex_fr_mem), (lex_fr_lm_path, Lex_fr_lm)):\n",
    "    _maybe_save_lex(path, lex)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c8135",
   "metadata": {},
   "source": [
    "## 2. Construction des paires MEM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4cab3",
   "metadata": {},
   "source": [
    "### Analyse de l'alignement EN→FR\n",
    "1. **Proxy linéaire raisonné.** Le calcul `start_fr = round(start_en * ratio)` reste un proxy commode mais imparfait : l'encodeur travaille dans l'espace positionnel imposé par \\(\\Pi\\), et une projection linéaire des bornes EN vers le flux FR casse parfois l'isométrie sémantique (réordonnements, insertions, suppressions).\n",
    "2. **Effet pratique.** La fenêtre FR ainsi projetée peut récupérer des tokens peu pertinents ; ceux-ci diluent la signature lexicale $Z_{\\mathrm{fr,lex}}$ et, in fine, le prototype MEM couplé au segment EN.\n",
    "3. **Correctif minimal (algorithme).**\n",
    "   3.1. Construire $Z_{\\mathrm{fr,lex}}$ sur la fenêtre $[\\texttt{start\\_fr}:\\texttt{stop\\_fr}]$.\n",
    "   3.2. Élaguer itérativement les tokens dont le retrait augmente la cohérence interne $\\langle Z_{\\mathrm{fr,lex}},\\, Z_{\\mathrm{fr,lex}}^{(-t)}\\rangle$ au-dessus d'un seuil fixé.\n",
    "   3.3. (Option) Injecter un bigramme FR issu de l'historique si $|\\texttt{span\\_tokens}| < 2$ afin de stabiliser la majorité.\n",
    "4. **Correctif préférable (séquences FR encodées disponibles).**\n",
    "   - Réutiliser exactement les mêmes fenêtres et strides que côté EN sur $X_{\\mathrm{seq}}^{(\\mathrm{FR})}$.\n",
    "   - Remplacer le contenu projeté par la signature lexicale des tokens FR co-localisés dans ces bornes, ce qui préserve la cohérence temporelle et l'homogénéité des tranches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515331c",
   "metadata": {},
   "source": [
    "### Cas des spans vides\n",
    "1. **Biais du vecteur constant.** Retourner le vecteur $\\mathbf{1}$ (tous +1) pour un segment vide induit un biais global artificiel : le produit scalaire $\\langle \\mathbf{1},\\, \\cdot \\rangle$ reste élevé, mais n'apporte aucune information discriminante et déforme la moyenne des prototypes MEM.\n",
    "2. **Prior lexical neutre.** Remplacer $\\mathbf{1}$ par un prior neutre défini par\n",
    "   $$Z^{\\text{prior}} = \\operatorname{sign}\\Big( \\sum_{v\\in \\mathcal{V}_{\\text{freq}}} w(v)\\,L_{\\mathrm{fr,mem}}(v) \\Big)$$\n",
    "   avec $w(v)\\propto$ la fréquence empirique du token $v$. À défaut de statistiques fiables, utiliser un vecteur de Rademacher ($\\pm 1$) avec seed fixé assure un comportement stationnaire sans corrélation systématique.\n",
    "3. **Interdiction de $\\mathbf{1}$.** Bannir explicitement $\\mathbf{1}$ comme valeur par défaut : elle biaise la densité des prototypes et fausse les similarités inter-mémoire, compromettant les décisions DD6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae0c14b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:38:26,384 [INFO] Prepared 50000 MEM pairs                 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def content_signature_from_Xseq(X_seq: Sequence[np.ndarray], *, majority: str = \"strict\") -> np.ndarray:\n",
    "    if not X_seq:\n",
    "        raise ValueError(\"X_seq vide\")\n",
    "    acc = np.zeros(X_seq[0].shape[0], dtype=np.int32)\n",
    "    for x in X_seq:\n",
    "        acc += x.astype(np.int32, copy=False)\n",
    "    if majority == \"strict\":\n",
    "        return np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    if majority == \"unbiased\":\n",
    "        rng_local = np.random.default_rng(0)\n",
    "        ties = acc == 0\n",
    "        acc[ties] = rng_local.integers(0, 2, size=int(ties.sum()), dtype=np.int32) * 2 - 1\n",
    "        return np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    raise ValueError(\"majority must be 'strict' or 'unbiased'\")\n",
    "\n",
    "\n",
    "def span_signatures_from_trace(\n",
    "    X_seq: Sequence[np.ndarray],\n",
    "    *,\n",
    "    win: int,\n",
    "    stride: int,\n",
    "    majority: str,\n",
    ") -> List[Tuple[np.ndarray, int, int]]:\n",
    "    T = len(X_seq)\n",
    "    if T == 0:\n",
    "        return []\n",
    "    spans: List[Tuple[np.ndarray, int, int]] = []\n",
    "    if T <= win:\n",
    "        spans.append((content_signature_from_Xseq(X_seq, majority=majority), 0, T))\n",
    "        return spans\n",
    "    for start in range(0, T - win + 1, max(1, stride)):\n",
    "        stop = start + win\n",
    "        spans.append((content_signature_from_Xseq(X_seq[start:stop], majority=majority), start, stop))\n",
    "    return spans\n",
    "\n",
    "\n",
    "def lexical_signature_from_tokens(\n",
    "    tokens: Sequence[str],\n",
    "    L_fr_mem: Callable[[str], np.ndarray],\n",
    "    D: int,\n",
    "    *,\n",
    "    prior: np.ndarray | None = None,\n",
    "    rng: np.random.Generator | None = None,\n",
    "    history_tokens: Sequence[str] | None = None,\n",
    "    prune_alignment_threshold: float = 0.0,\n",
    "    min_history_tokens: int = 2,\n",
    ") -> tuple[np.ndarray, list[str]]:\n",
    "    \"\"\"Build a MEM payload signature from tokens using the shared payload lexicon.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : Sequence[str]\n",
    "        Token span under consideration.\n",
    "    L_fr_mem : Callable[[str], np.ndarray]\n",
    "        MUST be `L_FR_PAYLOAD` so that MEM and DEC operate in the same\n",
    "        hyperdimensional space.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(0) if rng is None else rng\n",
    "    tok_list = [tok for tok in tokens if tok]\n",
    "    if len(tok_list) < min_history_tokens and history_tokens:\n",
    "        for tok in list(history_tokens)[-min_history_tokens:]:\n",
    "            if tok not in tok_list:\n",
    "                tok_list.append(tok)\n",
    "            if len(tok_list) >= min_history_tokens:\n",
    "                break\n",
    "    if not tok_list:\n",
    "        if prior is not None:\n",
    "            return prior.astype(np.int8, copy=False), []\n",
    "        noise = 2 * rng.integers(0, 2, size=D, dtype=np.int8) - 1\n",
    "        return noise.astype(np.int8, copy=False), []\n",
    "    acc = np.zeros(D, dtype=np.int32)\n",
    "    vecs: list[np.ndarray] = []\n",
    "    for tok in tok_list:\n",
    "        vec = L_fr_mem(tok).astype(np.int8, copy=False)\n",
    "        hd_assert_pm1(vec, D)\n",
    "        vec_i32 = vec.astype(np.int32, copy=False)\n",
    "        vecs.append(vec)\n",
    "        acc += vec_i32\n",
    "    signature = np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "\n",
    "    if len(vecs) > 2:\n",
    "        sig_i32 = signature.astype(np.int32, copy=False)\n",
    "        filtered_tokens: list[str] = []\n",
    "        filtered_vecs: list[np.ndarray] = []\n",
    "        for tok, vec in zip(tok_list, vecs):\n",
    "            align = float(np.dot(vec.astype(np.int32, copy=False), sig_i32)) / D\n",
    "            if align >= prune_alignment_threshold:\n",
    "                filtered_tokens.append(tok)\n",
    "                filtered_vecs.append(vec)\n",
    "        if filtered_tokens and len(filtered_tokens) < len(tok_list):\n",
    "            tok_list = filtered_tokens\n",
    "            vecs = filtered_vecs\n",
    "            acc = np.zeros(D, dtype=np.int32)\n",
    "            for vec in vecs:\n",
    "                acc += vec.astype(np.int32, copy=False)\n",
    "            signature = np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    return signature, list(tok_list)\n",
    "\n",
    "\n",
    "def build_mem_pairs_with_meta(\n",
    "    encoded_en: Sequence[dict],\n",
    "    encoded_fr: Sequence[dict],\n",
    "    tokens_fr: Sequence[Sequence[str]],\n",
    "    *,\n",
    "    L_fr_mem: Callable[[str], np.ndarray],\n",
    "    win: int = 8,\n",
    "    stride: int = 4,\n",
    "    majority: str = \"strict\",\n",
    "    max_pairs: int | None = None,\n",
    "    prior: np.ndarray | None = None,\n",
    "    rng_empty: np.random.Generator | None = None,\n",
    "    prune_alignment_threshold: float = 0.0,\n",
    "    min_history_tokens: int = 2,\n",
    ") -> Tuple[List[Tuple[np.ndarray, np.ndarray]], List[dict]]:\n",
    "    rng_empty = np.random.default_rng(0) if rng_empty is None else rng_empty\n",
    "    pairs: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "    meta: List[dict] = []\n",
    "    N = min(len(encoded_en), len(encoded_fr))\n",
    "    for idx in tqdm(range(N), desc=\"MEM span extraction\", leave=False):\n",
    "        spans_en = span_signatures_from_trace(encoded_en[idx][\"X_seq\"], win=win, stride=stride, majority=majority)\n",
    "        spans_fr_trace = span_signatures_from_trace(encoded_fr[idx][\"X_seq\"], win=win, stride=stride, majority=majority)\n",
    "        tok_fr = list(tokens_fr[idx]) if idx < len(tokens_fr) else []\n",
    "        len_seq_en = len(encoded_en[idx][\"X_seq\"])\n",
    "        len_seq_fr = len(encoded_fr[idx][\"X_seq\"])\n",
    "        len_tok_fr = len(tok_fr)\n",
    "        if len_seq_en == 0:\n",
    "            continue\n",
    "        ratio_en = len_tok_fr / max(len_seq_en, 1)\n",
    "        ratio_fr = len_tok_fr / max(len_seq_fr, 1)\n",
    "        paired_len = min(len(spans_en), len(spans_fr_trace))\n",
    "\n",
    "        for span_idx, (ze, start_en, stop_en) in enumerate(spans_en):\n",
    "            if len_tok_fr:\n",
    "                if span_idx < paired_len:\n",
    "                    _zf_fr, start_fr_trace, stop_fr_trace = spans_fr_trace[span_idx]\n",
    "                    start_fr = int(round(start_fr_trace * ratio_fr))\n",
    "                    stop_fr = int(round(stop_fr_trace * ratio_fr))\n",
    "                    fr_trace_bounds = (int(start_fr_trace), int(stop_fr_trace))\n",
    "                else:\n",
    "                    start_fr = int(round(start_en * ratio_en))\n",
    "                    stop_fr = int(round(stop_en * ratio_en))\n",
    "                    fr_trace_bounds = None\n",
    "                start_fr = max(0, min(start_fr, len_tok_fr - 1))\n",
    "                stop_fr = max(start_fr + 1, min(len_tok_fr, stop_fr))\n",
    "                span_tokens_raw = tok_fr[start_fr:stop_fr]\n",
    "                history_tokens = tok_fr[max(0, start_fr - stride):start_fr]\n",
    "            else:\n",
    "                start_fr = 0\n",
    "                stop_fr = 0\n",
    "                span_tokens_raw = []\n",
    "                history_tokens = []\n",
    "                fr_trace_bounds = None\n",
    "            zf_lex, span_tokens_filtered = lexical_signature_from_tokens(\n",
    "                span_tokens_raw,\n",
    "                L_fr_mem,\n",
    "                D,\n",
    "                prior=prior,\n",
    "                rng=rng_empty,\n",
    "                history_tokens=history_tokens,\n",
    "                prune_alignment_threshold=prune_alignment_threshold,\n",
    "                min_history_tokens=min_history_tokens,\n",
    "            )\n",
    "            pairs.append((ze, zf_lex))\n",
    "            meta.append(\n",
    "                {\n",
    "                    \"sentence_idx\": idx,\n",
    "                    \"start\": start_en,\n",
    "                    \"stop\": stop_en,\n",
    "                    \"start_token_fr\": start_fr,\n",
    "                    \"stop_token_fr\": stop_fr,\n",
    "                    \"history_tokens\": list(history_tokens),\n",
    "                    \"span_tokens\": span_tokens_filtered,\n",
    "                    \"span_tokens_raw\": span_tokens_raw,\n",
    "                    \"fr_trace_bounds\": fr_trace_bounds,\n",
    "                    \"Z_en\": ze,\n",
    "                    \"Z_fr_lex\": zf_lex,\n",
    "                }\n",
    "            )\n",
    "            if max_pairs is not None and len(pairs) >= max_pairs:\n",
    "                return pairs, meta\n",
    "    return pairs, meta\n",
    "\n",
    "\n",
    "tokens_fr = [enc_pipeline.sentence_to_tokens_EN(sent, vocab=set()) for sent in frs_sample]\n",
    "token_freq = Counter(tok for seq in tokens_fr for tok in seq)\n",
    "if token_freq:\n",
    "    prior_acc = np.zeros(D, dtype=np.int32)\n",
    "    for tok, freq in token_freq.items():\n",
    "        vec = L_FR_PAYLOAD(tok).astype(np.int8, copy=False)\n",
    "        hd_assert_pm1(vec, D)\n",
    "        prior_acc += vec.astype(np.int32, copy=False) * int(freq)\n",
    "    lexical_prior_fr = np.where(prior_acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "else:\n",
    "    lexical_prior_fr = rademacher(D, np.random.default_rng(404))\n",
    "\n",
    "empty_span_rng = np.random.default_rng(2027)\n",
    "pairs_mem, span_meta = build_mem_pairs_with_meta(\n",
    "    encoded_en,\n",
    "    encoded_fr,\n",
    "    tokens_fr,\n",
    "    L_fr_mem=L_FR_PAYLOAD,\n",
    "    win=8,\n",
    "    stride=4,\n",
    "    majority=\"strict\",\n",
    "    max_pairs=50_000,\n",
    "    prior=lexical_prior_fr,\n",
    "    rng_empty=empty_span_rng,\n",
    "    prune_alignment_threshold=0.0,\n",
    "    min_history_tokens=2,\n",
    ")\n",
    "log.info(\"Prepared %d MEM pairs\", len(pairs_mem))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85002792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence_idx': 0,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 6,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': [\"l'idée\",\n",
       "   'brusque',\n",
       "   'du',\n",
       "   'mariage',\n",
       "   \"qu'elle\",\n",
       "   'poursuivait'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 3,\n",
       "  'stop_token_fr': 9,\n",
       "  'history_tokens': [\"l'idée\", 'brusque', 'du'],\n",
       "  'span_tokens': ['mariage',\n",
       "   \"qu'elle\",\n",
       "   'poursuivait',\n",
       "   \"d'un\",\n",
       "   'sourire',\n",
       "   'si'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 6,\n",
       "  'stop_token_fr': 12,\n",
       "  'history_tokens': ['du', 'mariage', \"qu'elle\", 'poursuivait'],\n",
       "  'span_tokens': [\"d'un\", 'sourire', 'si', 'tranquille', 'entre', 'cécile'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 9,\n",
       "  'stop_token_fr': 15,\n",
       "  'history_tokens': ['poursuivait', \"d'un\", 'sourire', 'si'],\n",
       "  'span_tokens': ['tranquille', 'entre', 'cécile', 'et', 'paul,', 'acheva'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 12,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['si', 'tranquille', 'entre', 'cécile'],\n",
       "  'span_tokens': ['et',\n",
       "   'paul,',\n",
       "   'acheva',\n",
       "   '__sent_marker_0',\n",
       "   'de',\n",
       "   \"l'exaspérer.\"],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 15,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['cécile', 'et', 'paul,', 'acheva'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'de',\n",
       "   \"l'exaspérer.\",\n",
       "   \"l'idée_brusque\",\n",
       "   'brusque_du',\n",
       "   'du_mariage'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 18,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['acheva', '__sent_marker_0', 'de', \"l'exaspérer.\"],\n",
       "  'span_tokens': [\"l'idée_brusque\",\n",
       "   'brusque_du',\n",
       "   'du_mariage',\n",
       "   \"mariage_qu'elle\",\n",
       "   \"qu'elle_poursuivait\",\n",
       "   \"poursuivait_d'un\"],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 21,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': [\"l'exaspérer.\",\n",
       "   \"l'idée_brusque\",\n",
       "   'brusque_du',\n",
       "   'du_mariage'],\n",
       "  'span_tokens': [\"mariage_qu'elle\",\n",
       "   \"qu'elle_poursuivait\",\n",
       "   \"poursuivait_d'un\",\n",
       "   \"d'un_sourire\",\n",
       "   'sourire_si',\n",
       "   'si_tranquille'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 24,\n",
       "  'stop_token_fr': 30,\n",
       "  'history_tokens': ['du_mariage',\n",
       "   \"mariage_qu'elle\",\n",
       "   \"qu'elle_poursuivait\",\n",
       "   \"poursuivait_d'un\"],\n",
       "  'span_tokens': [\"d'un_sourire\",\n",
       "   'sourire_si',\n",
       "   'si_tranquille',\n",
       "   'tranquille_entre',\n",
       "   'entre_cécile',\n",
       "   'cécile_et'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 32,\n",
       "  'history_tokens': [\"poursuivait_d'un\",\n",
       "   \"d'un_sourire\",\n",
       "   'sourire_si',\n",
       "   'si_tranquille'],\n",
       "  'span_tokens': ['tranquille_entre',\n",
       "   'entre_cécile',\n",
       "   'cécile_et',\n",
       "   '__sent_marker_1',\n",
       "   'et_paul,'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 30,\n",
       "  'stop_token_fr': 35,\n",
       "  'history_tokens': ['si_tranquille',\n",
       "   'tranquille_entre',\n",
       "   'entre_cécile',\n",
       "   'cécile_et'],\n",
       "  'span_tokens': ['__sent_marker_1',\n",
       "   'et_paul,',\n",
       "   'paul,_acheva',\n",
       "   'acheva_de',\n",
       "   \"de_l'exaspérer.\"],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 32,\n",
       "  'stop_token_fr': 38,\n",
       "  'history_tokens': ['entre_cécile',\n",
       "   'cécile_et',\n",
       "   '__sent_marker_1',\n",
       "   'et_paul,'],\n",
       "  'span_tokens': ['paul,_acheva',\n",
       "   'acheva_de',\n",
       "   \"de_l'exaspérer.\",\n",
       "   \"l'idée_brusque_du\",\n",
       "   'brusque_du_mariage',\n",
       "   \"du_mariage_qu'elle\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 35,\n",
       "  'stop_token_fr': 41,\n",
       "  'history_tokens': ['et_paul,',\n",
       "   'paul,_acheva',\n",
       "   'acheva_de',\n",
       "   \"de_l'exaspérer.\"],\n",
       "  'span_tokens': [\"l'idée_brusque_du\",\n",
       "   'brusque_du_mariage',\n",
       "   \"du_mariage_qu'elle\",\n",
       "   \"mariage_qu'elle_poursuivait\",\n",
       "   \"qu'elle_poursuivait_d'un\",\n",
       "   \"poursuivait_d'un_sourire\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 38,\n",
       "  'stop_token_fr': 44,\n",
       "  'history_tokens': [\"de_l'exaspérer.\",\n",
       "   \"l'idée_brusque_du\",\n",
       "   'brusque_du_mariage',\n",
       "   \"du_mariage_qu'elle\"],\n",
       "  'span_tokens': [\"mariage_qu'elle_poursuivait\",\n",
       "   \"qu'elle_poursuivait_d'un\",\n",
       "   \"poursuivait_d'un_sourire\",\n",
       "   \"d'un_sourire_si\",\n",
       "   'sourire_si_tranquille',\n",
       "   'si_tranquille_entre'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 41,\n",
       "  'stop_token_fr': 47,\n",
       "  'history_tokens': [\"du_mariage_qu'elle\",\n",
       "   \"mariage_qu'elle_poursuivait\",\n",
       "   \"qu'elle_poursuivait_d'un\",\n",
       "   \"poursuivait_d'un_sourire\"],\n",
       "  'span_tokens': [\"d'un_sourire_si\",\n",
       "   'sourire_si_tranquille',\n",
       "   'si_tranquille_entre',\n",
       "   'tranquille_entre_cécile',\n",
       "   '__sent_marker_2',\n",
       "   'entre_cécile_et'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 44,\n",
       "  'stop_token_fr': 50,\n",
       "  'history_tokens': [\"poursuivait_d'un_sourire\",\n",
       "   \"d'un_sourire_si\",\n",
       "   'sourire_si_tranquille',\n",
       "   'si_tranquille_entre'],\n",
       "  'span_tokens': ['tranquille_entre_cécile',\n",
       "   '__sent_marker_2',\n",
       "   'entre_cécile_et',\n",
       "   'cécile_et_paul,',\n",
       "   'et_paul,_acheva',\n",
       "   'paul,_acheva_de'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 47,\n",
       "  'stop_token_fr': 53,\n",
       "  'history_tokens': ['si_tranquille_entre',\n",
       "   'tranquille_entre_cécile',\n",
       "   '__sent_marker_2',\n",
       "   'entre_cécile_et'],\n",
       "  'span_tokens': ['cécile_et_paul,',\n",
       "   'et_paul,_acheva',\n",
       "   'paul,_acheva_de',\n",
       "   \"acheva_de_l'exaspérer.\",\n",
       "   \"l'idée_brusque_du_mariage\",\n",
       "   \"brusque_du_mariage_qu'elle\"],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 50,\n",
       "  'stop_token_fr': 56,\n",
       "  'history_tokens': ['entre_cécile_et',\n",
       "   'cécile_et_paul,',\n",
       "   'et_paul,_acheva',\n",
       "   'paul,_acheva_de'],\n",
       "  'span_tokens': [\"acheva_de_l'exaspérer.\",\n",
       "   \"l'idée_brusque_du_mariage\",\n",
       "   \"brusque_du_mariage_qu'elle\",\n",
       "   \"du_mariage_qu'elle_poursuivait\",\n",
       "   \"mariage_qu'elle_poursuivait_d'un\",\n",
       "   \"qu'elle_poursuivait_d'un_sourire\"],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 53,\n",
       "  'stop_token_fr': 59,\n",
       "  'history_tokens': ['paul,_acheva_de',\n",
       "   \"acheva_de_l'exaspérer.\",\n",
       "   \"l'idée_brusque_du_mariage\",\n",
       "   \"brusque_du_mariage_qu'elle\"],\n",
       "  'span_tokens': [\"du_mariage_qu'elle_poursuivait\",\n",
       "   \"mariage_qu'elle_poursuivait_d'un\",\n",
       "   \"qu'elle_poursuivait_d'un_sourire\",\n",
       "   \"poursuivait_d'un_sourire_si\",\n",
       "   \"d'un_sourire_si_tranquille\",\n",
       "   'sourire_si_tranquille_entre'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 56,\n",
       "  'stop_token_fr': 62,\n",
       "  'history_tokens': [\"brusque_du_mariage_qu'elle\",\n",
       "   \"du_mariage_qu'elle_poursuivait\",\n",
       "   \"mariage_qu'elle_poursuivait_d'un\",\n",
       "   \"qu'elle_poursuivait_d'un_sourire\"],\n",
       "  'span_tokens': [\"poursuivait_d'un_sourire_si\",\n",
       "   \"d'un_sourire_si_tranquille\",\n",
       "   'sourire_si_tranquille_entre',\n",
       "   'si_tranquille_entre_cécile',\n",
       "   'tranquille_entre_cécile_et',\n",
       "   'entre_cécile_et_paul,'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 0,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 59,\n",
       "  'stop_token_fr': 65,\n",
       "  'history_tokens': [\"qu'elle_poursuivait_d'un_sourire\",\n",
       "   \"poursuivait_d'un_sourire_si\",\n",
       "   \"d'un_sourire_si_tranquille\",\n",
       "   'sourire_si_tranquille_entre'],\n",
       "  'span_tokens': ['si_tranquille_entre_cécile',\n",
       "   'tranquille_entre_cécile_et',\n",
       "   'entre_cécile_et_paul,',\n",
       "   'cécile_et_paul,_acheva',\n",
       "   'et_paul,_acheva_de',\n",
       "   \"paul,_acheva_de_l'exaspérer.\"],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 1,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['–',\n",
       "   'a',\n",
       "   'sept',\n",
       "   'heures,',\n",
       "   '__sent_marker_0',\n",
       "   'dit',\n",
       "   'harris.'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 1,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['–', 'a', 'sept', 'heures,'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'dit',\n",
       "   'harris.',\n",
       "   '–_a',\n",
       "   '__sent_marker_1',\n",
       "   'a_sept',\n",
       "   'sept_heures,'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 1,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['heures,', '__sent_marker_0', 'dit', 'harris.'],\n",
       "  'span_tokens': ['–_a',\n",
       "   '__sent_marker_1',\n",
       "   'a_sept',\n",
       "   'sept_heures,',\n",
       "   'heures,_dit',\n",
       "   '__sent_marker_2',\n",
       "   'dit_harris.'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 1,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['–_a', '__sent_marker_1', 'a_sept', 'sept_heures,'],\n",
       "  'span_tokens': ['heures,_dit',\n",
       "   '__sent_marker_2',\n",
       "   'dit_harris.',\n",
       "   '–_a_sept',\n",
       "   'a_sept_heures,',\n",
       "   'sept_heures,_dit',\n",
       "   'heures,_dit_harris.'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 1,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['sept_heures,',\n",
       "   'heures,_dit',\n",
       "   '__sent_marker_2',\n",
       "   'dit_harris.'],\n",
       "  'span_tokens': ['–_a_sept',\n",
       "   'a_sept_heures,',\n",
       "   'sept_heures,_dit',\n",
       "   'heures,_dit_harris.',\n",
       "   '–_a_sept_heures,',\n",
       "   'a_sept_heures,_dit',\n",
       "   'sept_heures,_dit_harris.'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 8,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['sa',\n",
       "   'chevelure',\n",
       "   'était',\n",
       "   'longue',\n",
       "   'et',\n",
       "   'noire,',\n",
       "   'et',\n",
       "   'non'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 12,\n",
       "  'history_tokens': ['sa', 'chevelure', 'était', 'longue'],\n",
       "  'span_tokens': ['et', 'noire,', 'et', 'non', 'pas', 'crépue', 'comme', 'de'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 8,\n",
       "  'stop_token_fr': 16,\n",
       "  'history_tokens': ['et', 'noire,', 'et', 'non'],\n",
       "  'span_tokens': ['pas',\n",
       "   'crépue',\n",
       "   'comme',\n",
       "   'de',\n",
       "   'la',\n",
       "   'laine.',\n",
       "   'son',\n",
       "   'front'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 12,\n",
       "  'stop_token_fr': 20,\n",
       "  'history_tokens': ['pas', 'crépue', 'comme', 'de'],\n",
       "  'span_tokens': ['la',\n",
       "   'laine.',\n",
       "   'son',\n",
       "   'front',\n",
       "   'était',\n",
       "   'haut',\n",
       "   'et',\n",
       "   'large,'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 16,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['la', 'laine.', 'son', 'front'],\n",
       "  'span_tokens': ['était',\n",
       "   'haut',\n",
       "   'et',\n",
       "   'large,',\n",
       "   'ses',\n",
       "   'yeux',\n",
       "   'vifs',\n",
       "   'et'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 20,\n",
       "  'stop_token_fr': 28,\n",
       "  'history_tokens': ['était', 'haut', 'et', 'large,'],\n",
       "  'span_tokens': ['ses',\n",
       "   'yeux',\n",
       "   'vifs',\n",
       "   'et',\n",
       "   'pleins',\n",
       "   '__sent_marker_0',\n",
       "   'de',\n",
       "   'feu.'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 24,\n",
       "  'stop_token_fr': 32,\n",
       "  'history_tokens': ['ses', 'yeux', 'vifs', 'et'],\n",
       "  'span_tokens': ['pleins',\n",
       "   '__sent_marker_0',\n",
       "   'de',\n",
       "   'feu.',\n",
       "   'sa_chevelure',\n",
       "   'chevelure_était',\n",
       "   'était_longue',\n",
       "   'longue_et'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 28,\n",
       "  'stop_token_fr': 36,\n",
       "  'history_tokens': ['pleins', '__sent_marker_0', 'de', 'feu.'],\n",
       "  'span_tokens': ['sa_chevelure',\n",
       "   'chevelure_était',\n",
       "   'était_longue',\n",
       "   'longue_et',\n",
       "   'et_noire,',\n",
       "   'noire,_et',\n",
       "   'et_non',\n",
       "   'non_pas'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 32,\n",
       "  'stop_token_fr': 40,\n",
       "  'history_tokens': ['sa_chevelure',\n",
       "   'chevelure_était',\n",
       "   'était_longue',\n",
       "   'longue_et'],\n",
       "  'span_tokens': ['et_noire,',\n",
       "   'noire,_et',\n",
       "   'et_non',\n",
       "   'non_pas',\n",
       "   'pas_crépue',\n",
       "   'crépue_comme',\n",
       "   'comme_de',\n",
       "   'de_la'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 36,\n",
       "  'stop_token_fr': 44,\n",
       "  'history_tokens': ['et_noire,', 'noire,_et', 'et_non', 'non_pas'],\n",
       "  'span_tokens': ['pas_crépue',\n",
       "   'crépue_comme',\n",
       "   'comme_de',\n",
       "   'de_la',\n",
       "   'la_laine.',\n",
       "   'laine._son',\n",
       "   'son_front',\n",
       "   'front_était'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 40,\n",
       "  'stop_token_fr': 48,\n",
       "  'history_tokens': ['pas_crépue', 'crépue_comme', 'comme_de', 'de_la'],\n",
       "  'span_tokens': ['la_laine.',\n",
       "   'laine._son',\n",
       "   'son_front',\n",
       "   'front_était',\n",
       "   'était_haut',\n",
       "   'haut_et',\n",
       "   'et_large,',\n",
       "   'large,_ses'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 44,\n",
       "  'stop_token_fr': 52,\n",
       "  'history_tokens': ['la_laine.', 'laine._son', 'son_front', 'front_était'],\n",
       "  'span_tokens': ['était_haut',\n",
       "   'haut_et',\n",
       "   'et_large,',\n",
       "   'large,_ses',\n",
       "   'ses_yeux',\n",
       "   'yeux_vifs',\n",
       "   '__sent_marker_1',\n",
       "   'vifs_et'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 48,\n",
       "  'stop_token_fr': 57,\n",
       "  'history_tokens': ['était_haut', 'haut_et', 'et_large,', 'large,_ses'],\n",
       "  'span_tokens': ['ses_yeux',\n",
       "   'yeux_vifs',\n",
       "   '__sent_marker_1',\n",
       "   'vifs_et',\n",
       "   'et_pleins',\n",
       "   'pleins_de',\n",
       "   'de_feu.',\n",
       "   'sa_chevelure_était',\n",
       "   'chevelure_était_longue'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 52,\n",
       "  'stop_token_fr': 61,\n",
       "  'history_tokens': ['ses_yeux', 'yeux_vifs', '__sent_marker_1', 'vifs_et'],\n",
       "  'span_tokens': ['et_pleins',\n",
       "   'pleins_de',\n",
       "   'de_feu.',\n",
       "   'sa_chevelure_était',\n",
       "   'chevelure_était_longue',\n",
       "   'était_longue_et',\n",
       "   'longue_et_noire,',\n",
       "   'et_noire,_et',\n",
       "   'noire,_et_non'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 57,\n",
       "  'stop_token_fr': 65,\n",
       "  'history_tokens': ['pleins_de',\n",
       "   'de_feu.',\n",
       "   'sa_chevelure_était',\n",
       "   'chevelure_était_longue'],\n",
       "  'span_tokens': ['était_longue_et',\n",
       "   'longue_et_noire,',\n",
       "   'et_noire,_et',\n",
       "   'noire,_et_non',\n",
       "   'et_non_pas',\n",
       "   'non_pas_crépue',\n",
       "   'pas_crépue_comme',\n",
       "   'crépue_comme_de'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 61,\n",
       "  'stop_token_fr': 69,\n",
       "  'history_tokens': ['était_longue_et',\n",
       "   'longue_et_noire,',\n",
       "   'et_noire,_et',\n",
       "   'noire,_et_non'],\n",
       "  'span_tokens': ['et_non_pas',\n",
       "   'non_pas_crépue',\n",
       "   'pas_crépue_comme',\n",
       "   'crépue_comme_de',\n",
       "   'comme_de_la',\n",
       "   'de_la_laine.',\n",
       "   'la_laine._son',\n",
       "   'laine._son_front'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 65,\n",
       "  'stop_token_fr': 73,\n",
       "  'history_tokens': ['et_non_pas',\n",
       "   'non_pas_crépue',\n",
       "   'pas_crépue_comme',\n",
       "   'crépue_comme_de'],\n",
       "  'span_tokens': ['comme_de_la',\n",
       "   'de_la_laine.',\n",
       "   'la_laine._son',\n",
       "   'laine._son_front',\n",
       "   'son_front_était',\n",
       "   'front_était_haut',\n",
       "   'était_haut_et',\n",
       "   'haut_et_large,'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 69,\n",
       "  'stop_token_fr': 77,\n",
       "  'history_tokens': ['comme_de_la',\n",
       "   'de_la_laine.',\n",
       "   'la_laine._son',\n",
       "   'laine._son_front'],\n",
       "  'span_tokens': ['son_front_était',\n",
       "   'front_était_haut',\n",
       "   'était_haut_et',\n",
       "   'haut_et_large,',\n",
       "   'et_large,_ses',\n",
       "   'large,_ses_yeux',\n",
       "   '__sent_marker_2',\n",
       "   'ses_yeux_vifs'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 73,\n",
       "  'stop_token_fr': 81,\n",
       "  'history_tokens': ['son_front_était',\n",
       "   'front_était_haut',\n",
       "   'était_haut_et',\n",
       "   'haut_et_large,'],\n",
       "  'span_tokens': ['et_large,_ses',\n",
       "   'large,_ses_yeux',\n",
       "   '__sent_marker_2',\n",
       "   'ses_yeux_vifs',\n",
       "   'yeux_vifs_et',\n",
       "   'vifs_et_pleins',\n",
       "   'et_pleins_de',\n",
       "   'pleins_de_feu.'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 77,\n",
       "  'stop_token_fr': 85,\n",
       "  'history_tokens': ['et_large,_ses',\n",
       "   'large,_ses_yeux',\n",
       "   '__sent_marker_2',\n",
       "   'ses_yeux_vifs'],\n",
       "  'span_tokens': ['yeux_vifs_et',\n",
       "   'vifs_et_pleins',\n",
       "   'et_pleins_de',\n",
       "   'pleins_de_feu.',\n",
       "   'sa_chevelure_était_longue',\n",
       "   'chevelure_était_longue_et',\n",
       "   'était_longue_et_noire,',\n",
       "   'longue_et_noire,_et'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 81,\n",
       "  'stop_token_fr': 89,\n",
       "  'history_tokens': ['yeux_vifs_et',\n",
       "   'vifs_et_pleins',\n",
       "   'et_pleins_de',\n",
       "   'pleins_de_feu.'],\n",
       "  'span_tokens': ['sa_chevelure_était_longue',\n",
       "   'chevelure_était_longue_et',\n",
       "   'était_longue_et_noire,',\n",
       "   'longue_et_noire,_et',\n",
       "   'et_noire,_et_non',\n",
       "   'noire,_et_non_pas',\n",
       "   'et_non_pas_crépue',\n",
       "   'non_pas_crépue_comme'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 85,\n",
       "  'stop_token_fr': 93,\n",
       "  'history_tokens': ['sa_chevelure_était_longue',\n",
       "   'chevelure_était_longue_et',\n",
       "   'était_longue_et_noire,',\n",
       "   'longue_et_noire,_et'],\n",
       "  'span_tokens': ['et_noire,_et_non',\n",
       "   'noire,_et_non_pas',\n",
       "   'et_non_pas_crépue',\n",
       "   'non_pas_crépue_comme',\n",
       "   'pas_crépue_comme_de',\n",
       "   'crépue_comme_de_la',\n",
       "   'comme_de_la_laine.',\n",
       "   'de_la_laine._son'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 89,\n",
       "  'stop_token_fr': 97,\n",
       "  'history_tokens': ['et_noire,_et_non',\n",
       "   'noire,_et_non_pas',\n",
       "   'et_non_pas_crépue',\n",
       "   'non_pas_crépue_comme'],\n",
       "  'span_tokens': ['pas_crépue_comme_de',\n",
       "   'crépue_comme_de_la',\n",
       "   'comme_de_la_laine.',\n",
       "   'de_la_laine._son',\n",
       "   'la_laine._son_front',\n",
       "   'laine._son_front_était',\n",
       "   'son_front_était_haut',\n",
       "   'front_était_haut_et'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 93,\n",
       "  'stop_token_fr': 101,\n",
       "  'history_tokens': ['pas_crépue_comme_de',\n",
       "   'crépue_comme_de_la',\n",
       "   'comme_de_la_laine.',\n",
       "   'de_la_laine._son'],\n",
       "  'span_tokens': ['la_laine._son_front',\n",
       "   'laine._son_front_était',\n",
       "   'son_front_était_haut',\n",
       "   'front_était_haut_et',\n",
       "   'était_haut_et_large,',\n",
       "   'haut_et_large,_ses',\n",
       "   'et_large,_ses_yeux',\n",
       "   'large,_ses_yeux_vifs'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 2,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 97,\n",
       "  'stop_token_fr': 105,\n",
       "  'history_tokens': ['la_laine._son_front',\n",
       "   'laine._son_front_était',\n",
       "   'son_front_était_haut',\n",
       "   'front_était_haut_et'],\n",
       "  'span_tokens': ['était_haut_et_large,',\n",
       "   'haut_et_large,_ses',\n",
       "   'et_large,_ses_yeux',\n",
       "   'large,_ses_yeux_vifs',\n",
       "   'ses_yeux_vifs_et',\n",
       "   'yeux_vifs_et_pleins',\n",
       "   'vifs_et_pleins_de',\n",
       "   'et_pleins_de_feu.'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 8,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['ce',\n",
       "   'fut',\n",
       "   'des',\n",
       "   'roches',\n",
       "   'le',\n",
       "   'masle,',\n",
       "   'chanoine',\n",
       "   'à'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 12,\n",
       "  'history_tokens': ['ce', 'fut', 'des', 'roches'],\n",
       "  'span_tokens': ['le',\n",
       "   'masle,',\n",
       "   'chanoine',\n",
       "   'à',\n",
       "   'notre-dame,',\n",
       "   'et',\n",
       "   'qui',\n",
       "   'avait'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 8,\n",
       "  'stop_token_fr': 16,\n",
       "  'history_tokens': ['le', 'masle,', 'chanoine', 'à'],\n",
       "  'span_tokens': ['notre-dame,',\n",
       "   'et',\n",
       "   'qui',\n",
       "   'avait',\n",
       "   'été',\n",
       "   'autrefois',\n",
       "   'valet',\n",
       "   'de'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 12,\n",
       "  'stop_token_fr': 20,\n",
       "  'history_tokens': ['notre-dame,', 'et', 'qui', 'avait'],\n",
       "  'span_tokens': ['été',\n",
       "   'autrefois',\n",
       "   'valet',\n",
       "   'de',\n",
       "   'chambre',\n",
       "   'du',\n",
       "   'cardinal,',\n",
       "   'qui'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 16,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['été', 'autrefois', 'valet', 'de'],\n",
       "  'span_tokens': ['chambre',\n",
       "   'du',\n",
       "   'cardinal,',\n",
       "   'qui',\n",
       "   'le',\n",
       "   'proposa',\n",
       "   'à',\n",
       "   'son'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 20,\n",
       "  'stop_token_fr': 28,\n",
       "  'history_tokens': ['chambre', 'du', 'cardinal,', 'qui'],\n",
       "  'span_tokens': ['le',\n",
       "   'proposa',\n",
       "   'à',\n",
       "   'son',\n",
       "   'éminence',\n",
       "   'comme',\n",
       "   'un',\n",
       "   'homme'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 24,\n",
       "  'stop_token_fr': 32,\n",
       "  'history_tokens': ['le', 'proposa', 'à', 'son'],\n",
       "  'span_tokens': ['éminence',\n",
       "   'comme',\n",
       "   'un',\n",
       "   'homme',\n",
       "   '__sent_marker_0',\n",
       "   'tout',\n",
       "   'dévoué.',\n",
       "   'ce_fut'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 28,\n",
       "  'stop_token_fr': 36,\n",
       "  'history_tokens': ['éminence', 'comme', 'un', 'homme'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'tout',\n",
       "   'dévoué.',\n",
       "   'ce_fut',\n",
       "   'fut_des',\n",
       "   'des_roches',\n",
       "   'roches_le',\n",
       "   'le_masle,'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 32,\n",
       "  'stop_token_fr': 40,\n",
       "  'history_tokens': ['__sent_marker_0', 'tout', 'dévoué.', 'ce_fut'],\n",
       "  'span_tokens': ['fut_des',\n",
       "   'des_roches',\n",
       "   'roches_le',\n",
       "   'le_masle,',\n",
       "   'masle,_chanoine',\n",
       "   'chanoine_à',\n",
       "   'à_notre-dame,',\n",
       "   'notre-dame,_et'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 36,\n",
       "  'stop_token_fr': 44,\n",
       "  'history_tokens': ['fut_des', 'des_roches', 'roches_le', 'le_masle,'],\n",
       "  'span_tokens': ['masle,_chanoine',\n",
       "   'chanoine_à',\n",
       "   'à_notre-dame,',\n",
       "   'notre-dame,_et',\n",
       "   'et_qui',\n",
       "   'qui_avait',\n",
       "   'avait_été',\n",
       "   'été_autrefois'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 40,\n",
       "  'stop_token_fr': 48,\n",
       "  'history_tokens': ['masle,_chanoine',\n",
       "   'chanoine_à',\n",
       "   'à_notre-dame,',\n",
       "   'notre-dame,_et'],\n",
       "  'span_tokens': ['et_qui',\n",
       "   'qui_avait',\n",
       "   'avait_été',\n",
       "   'été_autrefois',\n",
       "   'autrefois_valet',\n",
       "   'valet_de',\n",
       "   'de_chambre',\n",
       "   'chambre_du'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 44,\n",
       "  'stop_token_fr': 52,\n",
       "  'history_tokens': ['et_qui', 'qui_avait', 'avait_été', 'été_autrefois'],\n",
       "  'span_tokens': ['autrefois_valet',\n",
       "   'valet_de',\n",
       "   'de_chambre',\n",
       "   'chambre_du',\n",
       "   'du_cardinal,',\n",
       "   'cardinal,_qui',\n",
       "   'qui_le',\n",
       "   'le_proposa'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 48,\n",
       "  'stop_token_fr': 56,\n",
       "  'history_tokens': ['autrefois_valet',\n",
       "   'valet_de',\n",
       "   'de_chambre',\n",
       "   'chambre_du'],\n",
       "  'span_tokens': ['du_cardinal,',\n",
       "   'cardinal,_qui',\n",
       "   'qui_le',\n",
       "   'le_proposa',\n",
       "   'proposa_à',\n",
       "   'à_son',\n",
       "   'son_éminence',\n",
       "   'éminence_comme'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 52,\n",
       "  'stop_token_fr': 61,\n",
       "  'history_tokens': ['du_cardinal,', 'cardinal,_qui', 'qui_le', 'le_proposa'],\n",
       "  'span_tokens': ['proposa_à',\n",
       "   'à_son',\n",
       "   'son_éminence',\n",
       "   'éminence_comme',\n",
       "   '__sent_marker_1',\n",
       "   'comme_un',\n",
       "   'un_homme',\n",
       "   'homme_tout',\n",
       "   'tout_dévoué.'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 56,\n",
       "  'stop_token_fr': 65,\n",
       "  'history_tokens': ['proposa_à', 'à_son', 'son_éminence', 'éminence_comme'],\n",
       "  'span_tokens': ['__sent_marker_1',\n",
       "   'comme_un',\n",
       "   'un_homme',\n",
       "   'homme_tout',\n",
       "   'tout_dévoué.',\n",
       "   'ce_fut_des',\n",
       "   'fut_des_roches',\n",
       "   'des_roches_le',\n",
       "   'roches_le_masle,'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 61,\n",
       "  'stop_token_fr': 69,\n",
       "  'history_tokens': ['comme_un', 'un_homme', 'homme_tout', 'tout_dévoué.'],\n",
       "  'span_tokens': ['ce_fut_des',\n",
       "   'fut_des_roches',\n",
       "   'des_roches_le',\n",
       "   'roches_le_masle,',\n",
       "   'le_masle,_chanoine',\n",
       "   'masle,_chanoine_à',\n",
       "   'chanoine_à_notre-dame,',\n",
       "   'à_notre-dame,_et'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 65,\n",
       "  'stop_token_fr': 73,\n",
       "  'history_tokens': ['ce_fut_des',\n",
       "   'fut_des_roches',\n",
       "   'des_roches_le',\n",
       "   'roches_le_masle,'],\n",
       "  'span_tokens': ['le_masle,_chanoine',\n",
       "   'masle,_chanoine_à',\n",
       "   'chanoine_à_notre-dame,',\n",
       "   'à_notre-dame,_et',\n",
       "   'notre-dame,_et_qui',\n",
       "   'et_qui_avait',\n",
       "   'qui_avait_été',\n",
       "   'avait_été_autrefois'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 69,\n",
       "  'stop_token_fr': 77,\n",
       "  'history_tokens': ['le_masle,_chanoine',\n",
       "   'masle,_chanoine_à',\n",
       "   'chanoine_à_notre-dame,',\n",
       "   'à_notre-dame,_et'],\n",
       "  'span_tokens': ['notre-dame,_et_qui',\n",
       "   'et_qui_avait',\n",
       "   'qui_avait_été',\n",
       "   'avait_été_autrefois',\n",
       "   'été_autrefois_valet',\n",
       "   'autrefois_valet_de',\n",
       "   'valet_de_chambre',\n",
       "   'de_chambre_du'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 73,\n",
       "  'stop_token_fr': 81,\n",
       "  'history_tokens': ['notre-dame,_et_qui',\n",
       "   'et_qui_avait',\n",
       "   'qui_avait_été',\n",
       "   'avait_été_autrefois'],\n",
       "  'span_tokens': ['été_autrefois_valet',\n",
       "   'autrefois_valet_de',\n",
       "   'valet_de_chambre',\n",
       "   'de_chambre_du',\n",
       "   'chambre_du_cardinal,',\n",
       "   'du_cardinal,_qui',\n",
       "   'cardinal,_qui_le',\n",
       "   'qui_le_proposa'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 77,\n",
       "  'stop_token_fr': 85,\n",
       "  'history_tokens': ['été_autrefois_valet',\n",
       "   'autrefois_valet_de',\n",
       "   'valet_de_chambre',\n",
       "   'de_chambre_du'],\n",
       "  'span_tokens': ['chambre_du_cardinal,',\n",
       "   'du_cardinal,_qui',\n",
       "   'cardinal,_qui_le',\n",
       "   'qui_le_proposa',\n",
       "   'le_proposa_à',\n",
       "   'proposa_à_son',\n",
       "   'à_son_éminence',\n",
       "   '__sent_marker_2'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 81,\n",
       "  'stop_token_fr': 89,\n",
       "  'history_tokens': ['chambre_du_cardinal,',\n",
       "   'du_cardinal,_qui',\n",
       "   'cardinal,_qui_le',\n",
       "   'qui_le_proposa'],\n",
       "  'span_tokens': ['le_proposa_à',\n",
       "   'proposa_à_son',\n",
       "   'à_son_éminence',\n",
       "   '__sent_marker_2',\n",
       "   'son_éminence_comme',\n",
       "   'éminence_comme_un',\n",
       "   'comme_un_homme',\n",
       "   'un_homme_tout'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 85,\n",
       "  'stop_token_fr': 93,\n",
       "  'history_tokens': ['le_proposa_à',\n",
       "   'proposa_à_son',\n",
       "   'à_son_éminence',\n",
       "   '__sent_marker_2'],\n",
       "  'span_tokens': ['son_éminence_comme',\n",
       "   'éminence_comme_un',\n",
       "   'comme_un_homme',\n",
       "   'un_homme_tout',\n",
       "   'homme_tout_dévoué.',\n",
       "   'ce_fut_des_roches',\n",
       "   'fut_des_roches_le',\n",
       "   'des_roches_le_masle,'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 89,\n",
       "  'stop_token_fr': 97,\n",
       "  'history_tokens': ['son_éminence_comme',\n",
       "   'éminence_comme_un',\n",
       "   'comme_un_homme',\n",
       "   'un_homme_tout'],\n",
       "  'span_tokens': ['homme_tout_dévoué.',\n",
       "   'ce_fut_des_roches',\n",
       "   'fut_des_roches_le',\n",
       "   'des_roches_le_masle,',\n",
       "   'roches_le_masle,_chanoine',\n",
       "   'le_masle,_chanoine_à',\n",
       "   'masle,_chanoine_à_notre-dame,',\n",
       "   'chanoine_à_notre-dame,_et'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 93,\n",
       "  'stop_token_fr': 101,\n",
       "  'history_tokens': ['homme_tout_dévoué.',\n",
       "   'ce_fut_des_roches',\n",
       "   'fut_des_roches_le',\n",
       "   'des_roches_le_masle,'],\n",
       "  'span_tokens': ['roches_le_masle,_chanoine',\n",
       "   'le_masle,_chanoine_à',\n",
       "   'masle,_chanoine_à_notre-dame,',\n",
       "   'chanoine_à_notre-dame,_et',\n",
       "   'à_notre-dame,_et_qui',\n",
       "   'notre-dame,_et_qui_avait',\n",
       "   'et_qui_avait_été',\n",
       "   'qui_avait_été_autrefois'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 97,\n",
       "  'stop_token_fr': 105,\n",
       "  'history_tokens': ['roches_le_masle,_chanoine',\n",
       "   'le_masle,_chanoine_à',\n",
       "   'masle,_chanoine_à_notre-dame,',\n",
       "   'chanoine_à_notre-dame,_et'],\n",
       "  'span_tokens': ['à_notre-dame,_et_qui',\n",
       "   'notre-dame,_et_qui_avait',\n",
       "   'et_qui_avait_été',\n",
       "   'qui_avait_été_autrefois',\n",
       "   'avait_été_autrefois_valet',\n",
       "   'été_autrefois_valet_de',\n",
       "   'autrefois_valet_de_chambre',\n",
       "   'valet_de_chambre_du'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 101,\n",
       "  'stop_token_fr': 109,\n",
       "  'history_tokens': ['à_notre-dame,_et_qui',\n",
       "   'notre-dame,_et_qui_avait',\n",
       "   'et_qui_avait_été',\n",
       "   'qui_avait_été_autrefois'],\n",
       "  'span_tokens': ['avait_été_autrefois_valet',\n",
       "   'été_autrefois_valet_de',\n",
       "   'autrefois_valet_de_chambre',\n",
       "   'valet_de_chambre_du',\n",
       "   'de_chambre_du_cardinal,',\n",
       "   'chambre_du_cardinal,_qui',\n",
       "   'du_cardinal,_qui_le',\n",
       "   'cardinal,_qui_le_proposa'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 105,\n",
       "  'stop_token_fr': 113,\n",
       "  'history_tokens': ['avait_été_autrefois_valet',\n",
       "   'été_autrefois_valet_de',\n",
       "   'autrefois_valet_de_chambre',\n",
       "   'valet_de_chambre_du'],\n",
       "  'span_tokens': ['de_chambre_du_cardinal,',\n",
       "   'chambre_du_cardinal,_qui',\n",
       "   'du_cardinal,_qui_le',\n",
       "   'cardinal,_qui_le_proposa',\n",
       "   'qui_le_proposa_à',\n",
       "   'le_proposa_à_son',\n",
       "   'proposa_à_son_éminence',\n",
       "   'à_son_éminence_comme'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 3,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 109,\n",
       "  'stop_token_fr': 117,\n",
       "  'history_tokens': ['de_chambre_du_cardinal,',\n",
       "   'chambre_du_cardinal,_qui',\n",
       "   'du_cardinal,_qui_le',\n",
       "   'cardinal,_qui_le_proposa'],\n",
       "  'span_tokens': ['qui_le_proposa_à',\n",
       "   'le_proposa_à_son',\n",
       "   'proposa_à_son_éminence',\n",
       "   'à_son_éminence_comme',\n",
       "   'son_éminence_comme_un',\n",
       "   'éminence_comme_un_homme',\n",
       "   'comme_un_homme_tout',\n",
       "   'un_homme_tout_dévoué.'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 8,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['peut-être',\n",
       "   'même,',\n",
       "   'suivant',\n",
       "   \"l'exemple\",\n",
       "   'des',\n",
       "   'chinois',\n",
       "   'et',\n",
       "   'des'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 12,\n",
       "  'history_tokens': ['peut-être', 'même,', 'suivant', \"l'exemple\"],\n",
       "  'span_tokens': ['des',\n",
       "   'chinois',\n",
       "   'et',\n",
       "   'des',\n",
       "   'indiens,',\n",
       "   'avait-il',\n",
       "   'déterminé',\n",
       "   'la'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 8,\n",
       "  'stop_token_fr': 17,\n",
       "  'history_tokens': ['des', 'chinois', 'et', 'des'],\n",
       "  'span_tokens': ['indiens,',\n",
       "   'avait-il',\n",
       "   'déterminé',\n",
       "   'la',\n",
       "   'production',\n",
       "   'de',\n",
       "   'cette',\n",
       "   'perle',\n",
       "   'en'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 12,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['indiens,', 'avait-il', 'déterminé', 'la'],\n",
       "  'span_tokens': ['production',\n",
       "   'de',\n",
       "   'cette',\n",
       "   'perle',\n",
       "   'en',\n",
       "   'introduisant',\n",
       "   'sous',\n",
       "   'les',\n",
       "   'plis'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 17,\n",
       "  'stop_token_fr': 25,\n",
       "  'history_tokens': ['de', 'cette', 'perle', 'en'],\n",
       "  'span_tokens': ['introduisant',\n",
       "   'sous',\n",
       "   'les',\n",
       "   'plis',\n",
       "   'du',\n",
       "   'mollusque',\n",
       "   'quelque',\n",
       "   'morceau'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 21,\n",
       "  'stop_token_fr': 29,\n",
       "  'history_tokens': ['introduisant', 'sous', 'les', 'plis'],\n",
       "  'span_tokens': ['du',\n",
       "   'mollusque',\n",
       "   'quelque',\n",
       "   'morceau',\n",
       "   'de',\n",
       "   'verre',\n",
       "   'et',\n",
       "   'de'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 25,\n",
       "  'stop_token_fr': 33,\n",
       "  'history_tokens': ['du', 'mollusque', 'quelque', 'morceau'],\n",
       "  'span_tokens': ['de',\n",
       "   'verre',\n",
       "   'et',\n",
       "   'de',\n",
       "   'métal,',\n",
       "   'qui',\n",
       "   \"s'était\",\n",
       "   'peu'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 29,\n",
       "  'stop_token_fr': 37,\n",
       "  'history_tokens': ['de', 'verre', 'et', 'de'],\n",
       "  'span_tokens': ['métal,',\n",
       "   'qui',\n",
       "   \"s'était\",\n",
       "   'peu',\n",
       "   'à',\n",
       "   'peu',\n",
       "   'recouvert',\n",
       "   'de'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 33,\n",
       "  'stop_token_fr': 41,\n",
       "  'history_tokens': ['métal,', 'qui', \"s'était\", 'peu'],\n",
       "  'span_tokens': ['à',\n",
       "   'peu',\n",
       "   'recouvert',\n",
       "   'de',\n",
       "   'la',\n",
       "   '__sent_marker_0',\n",
       "   'matière',\n",
       "   'nacrée.'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 37,\n",
       "  'stop_token_fr': 45,\n",
       "  'history_tokens': ['à', 'peu', 'recouvert', 'de'],\n",
       "  'span_tokens': ['la',\n",
       "   '__sent_marker_0',\n",
       "   'matière',\n",
       "   'nacrée.',\n",
       "   'peut-être_même,',\n",
       "   'même,_suivant',\n",
       "   \"suivant_l'exemple\",\n",
       "   \"l'exemple_des\"],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 41,\n",
       "  'stop_token_fr': 50,\n",
       "  'history_tokens': ['la', '__sent_marker_0', 'matière', 'nacrée.'],\n",
       "  'span_tokens': ['peut-être_même,',\n",
       "   'même,_suivant',\n",
       "   \"suivant_l'exemple\",\n",
       "   \"l'exemple_des\",\n",
       "   'des_chinois',\n",
       "   'chinois_et',\n",
       "   'et_des',\n",
       "   'des_indiens,',\n",
       "   'indiens,_avait-il'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 45,\n",
       "  'stop_token_fr': 54,\n",
       "  'history_tokens': ['peut-être_même,',\n",
       "   'même,_suivant',\n",
       "   \"suivant_l'exemple\",\n",
       "   \"l'exemple_des\"],\n",
       "  'span_tokens': ['des_chinois',\n",
       "   'chinois_et',\n",
       "   'et_des',\n",
       "   'des_indiens,',\n",
       "   'indiens,_avait-il',\n",
       "   'avait-il_déterminé',\n",
       "   'déterminé_la',\n",
       "   'la_production',\n",
       "   'production_de'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 50,\n",
       "  'stop_token_fr': 58,\n",
       "  'history_tokens': ['chinois_et',\n",
       "   'et_des',\n",
       "   'des_indiens,',\n",
       "   'indiens,_avait-il'],\n",
       "  'span_tokens': ['avait-il_déterminé',\n",
       "   'déterminé_la',\n",
       "   'la_production',\n",
       "   'production_de',\n",
       "   'de_cette',\n",
       "   'cette_perle',\n",
       "   'perle_en',\n",
       "   'en_introduisant'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 54,\n",
       "  'stop_token_fr': 62,\n",
       "  'history_tokens': ['avait-il_déterminé',\n",
       "   'déterminé_la',\n",
       "   'la_production',\n",
       "   'production_de'],\n",
       "  'span_tokens': ['de_cette',\n",
       "   'cette_perle',\n",
       "   'perle_en',\n",
       "   'en_introduisant',\n",
       "   'introduisant_sous',\n",
       "   'sous_les',\n",
       "   'les_plis',\n",
       "   'plis_du'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 58,\n",
       "  'stop_token_fr': 66,\n",
       "  'history_tokens': ['de_cette', 'cette_perle', 'perle_en', 'en_introduisant'],\n",
       "  'span_tokens': ['introduisant_sous',\n",
       "   'sous_les',\n",
       "   'les_plis',\n",
       "   'plis_du',\n",
       "   'du_mollusque',\n",
       "   'mollusque_quelque',\n",
       "   'quelque_morceau',\n",
       "   'morceau_de'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 62,\n",
       "  'stop_token_fr': 70,\n",
       "  'history_tokens': ['introduisant_sous', 'sous_les', 'les_plis', 'plis_du'],\n",
       "  'span_tokens': ['du_mollusque',\n",
       "   'mollusque_quelque',\n",
       "   'quelque_morceau',\n",
       "   'morceau_de',\n",
       "   'de_verre',\n",
       "   'verre_et',\n",
       "   'et_de',\n",
       "   'de_métal,'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 66,\n",
       "  'stop_token_fr': 74,\n",
       "  'history_tokens': ['du_mollusque',\n",
       "   'mollusque_quelque',\n",
       "   'quelque_morceau',\n",
       "   'morceau_de'],\n",
       "  'span_tokens': ['de_verre',\n",
       "   'verre_et',\n",
       "   'et_de',\n",
       "   'de_métal,',\n",
       "   'métal,_qui',\n",
       "   \"qui_s'était\",\n",
       "   \"s'était_peu\",\n",
       "   'peu_à'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 70,\n",
       "  'stop_token_fr': 78,\n",
       "  'history_tokens': ['de_verre', 'verre_et', 'et_de', 'de_métal,'],\n",
       "  'span_tokens': ['métal,_qui',\n",
       "   \"qui_s'était\",\n",
       "   \"s'était_peu\",\n",
       "   'peu_à',\n",
       "   'à_peu',\n",
       "   'peu_recouvert',\n",
       "   '__sent_marker_1',\n",
       "   'recouvert_de'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 74,\n",
       "  'stop_token_fr': 83,\n",
       "  'history_tokens': ['métal,_qui', \"qui_s'était\", \"s'était_peu\", 'peu_à'],\n",
       "  'span_tokens': ['à_peu',\n",
       "   'peu_recouvert',\n",
       "   '__sent_marker_1',\n",
       "   'recouvert_de',\n",
       "   'de_la',\n",
       "   'la_matière',\n",
       "   'matière_nacrée.',\n",
       "   'peut-être_même,_suivant',\n",
       "   \"même,_suivant_l'exemple\"],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 78,\n",
       "  'stop_token_fr': 87,\n",
       "  'history_tokens': ['à_peu',\n",
       "   'peu_recouvert',\n",
       "   '__sent_marker_1',\n",
       "   'recouvert_de'],\n",
       "  'span_tokens': ['de_la',\n",
       "   'la_matière',\n",
       "   'matière_nacrée.',\n",
       "   'peut-être_même,_suivant',\n",
       "   \"même,_suivant_l'exemple\",\n",
       "   \"suivant_l'exemple_des\",\n",
       "   \"l'exemple_des_chinois\",\n",
       "   'des_chinois_et',\n",
       "   'chinois_et_des'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 83,\n",
       "  'stop_token_fr': 91,\n",
       "  'history_tokens': ['la_matière',\n",
       "   'matière_nacrée.',\n",
       "   'peut-être_même,_suivant',\n",
       "   \"même,_suivant_l'exemple\"],\n",
       "  'span_tokens': [\"suivant_l'exemple_des\",\n",
       "   \"l'exemple_des_chinois\",\n",
       "   'des_chinois_et',\n",
       "   'chinois_et_des',\n",
       "   'et_des_indiens,',\n",
       "   'des_indiens,_avait-il',\n",
       "   'indiens,_avait-il_déterminé',\n",
       "   'avait-il_déterminé_la'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 87,\n",
       "  'stop_token_fr': 95,\n",
       "  'history_tokens': [\"suivant_l'exemple_des\",\n",
       "   \"l'exemple_des_chinois\",\n",
       "   'des_chinois_et',\n",
       "   'chinois_et_des'],\n",
       "  'span_tokens': ['et_des_indiens,',\n",
       "   'des_indiens,_avait-il',\n",
       "   'indiens,_avait-il_déterminé',\n",
       "   'avait-il_déterminé_la',\n",
       "   'déterminé_la_production',\n",
       "   'la_production_de',\n",
       "   'production_de_cette',\n",
       "   'de_cette_perle'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 91,\n",
       "  'stop_token_fr': 99,\n",
       "  'history_tokens': ['et_des_indiens,',\n",
       "   'des_indiens,_avait-il',\n",
       "   'indiens,_avait-il_déterminé',\n",
       "   'avait-il_déterminé_la'],\n",
       "  'span_tokens': ['déterminé_la_production',\n",
       "   'la_production_de',\n",
       "   'production_de_cette',\n",
       "   'de_cette_perle',\n",
       "   'cette_perle_en',\n",
       "   'perle_en_introduisant',\n",
       "   'en_introduisant_sous',\n",
       "   'introduisant_sous_les'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 95,\n",
       "  'stop_token_fr': 103,\n",
       "  'history_tokens': ['déterminé_la_production',\n",
       "   'la_production_de',\n",
       "   'production_de_cette',\n",
       "   'de_cette_perle'],\n",
       "  'span_tokens': ['cette_perle_en',\n",
       "   'perle_en_introduisant',\n",
       "   'en_introduisant_sous',\n",
       "   'introduisant_sous_les',\n",
       "   'sous_les_plis',\n",
       "   'les_plis_du',\n",
       "   'plis_du_mollusque',\n",
       "   'du_mollusque_quelque'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 99,\n",
       "  'stop_token_fr': 107,\n",
       "  'history_tokens': ['cette_perle_en',\n",
       "   'perle_en_introduisant',\n",
       "   'en_introduisant_sous',\n",
       "   'introduisant_sous_les'],\n",
       "  'span_tokens': ['sous_les_plis',\n",
       "   'les_plis_du',\n",
       "   'plis_du_mollusque',\n",
       "   'du_mollusque_quelque',\n",
       "   'mollusque_quelque_morceau',\n",
       "   'quelque_morceau_de',\n",
       "   'morceau_de_verre',\n",
       "   'de_verre_et'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 103,\n",
       "  'stop_token_fr': 112,\n",
       "  'history_tokens': ['sous_les_plis',\n",
       "   'les_plis_du',\n",
       "   'plis_du_mollusque',\n",
       "   'du_mollusque_quelque'],\n",
       "  'span_tokens': ['mollusque_quelque_morceau',\n",
       "   'quelque_morceau_de',\n",
       "   'morceau_de_verre',\n",
       "   'de_verre_et',\n",
       "   'verre_et_de',\n",
       "   'et_de_métal,',\n",
       "   'de_métal,_qui',\n",
       "   \"métal,_qui_s'était\",\n",
       "   \"qui_s'était_peu\"],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 107,\n",
       "  'stop_token_fr': 116,\n",
       "  'history_tokens': ['mollusque_quelque_morceau',\n",
       "   'quelque_morceau_de',\n",
       "   'morceau_de_verre',\n",
       "   'de_verre_et'],\n",
       "  'span_tokens': ['verre_et_de',\n",
       "   'et_de_métal,',\n",
       "   'de_métal,_qui',\n",
       "   \"métal,_qui_s'était\",\n",
       "   \"qui_s'était_peu\",\n",
       "   \"s'était_peu_à\",\n",
       "   'peu_à_peu',\n",
       "   '__sent_marker_2',\n",
       "   'à_peu_recouvert'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 112,\n",
       "  'stop_token_fr': 120,\n",
       "  'history_tokens': ['et_de_métal,',\n",
       "   'de_métal,_qui',\n",
       "   \"métal,_qui_s'était\",\n",
       "   \"qui_s'était_peu\"],\n",
       "  'span_tokens': [\"s'était_peu_à\",\n",
       "   'peu_à_peu',\n",
       "   '__sent_marker_2',\n",
       "   'à_peu_recouvert',\n",
       "   'peu_recouvert_de',\n",
       "   'recouvert_de_la',\n",
       "   'de_la_matière',\n",
       "   'la_matière_nacrée.'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 116,\n",
       "  'stop_token_fr': 124,\n",
       "  'history_tokens': [\"s'était_peu_à\",\n",
       "   'peu_à_peu',\n",
       "   '__sent_marker_2',\n",
       "   'à_peu_recouvert'],\n",
       "  'span_tokens': ['peu_recouvert_de',\n",
       "   'recouvert_de_la',\n",
       "   'de_la_matière',\n",
       "   'la_matière_nacrée.',\n",
       "   \"peut-être_même,_suivant_l'exemple\",\n",
       "   \"même,_suivant_l'exemple_des\",\n",
       "   \"suivant_l'exemple_des_chinois\",\n",
       "   \"l'exemple_des_chinois_et\"],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 120,\n",
       "  'stop_token_fr': 128,\n",
       "  'history_tokens': ['peu_recouvert_de',\n",
       "   'recouvert_de_la',\n",
       "   'de_la_matière',\n",
       "   'la_matière_nacrée.'],\n",
       "  'span_tokens': [\"peut-être_même,_suivant_l'exemple\",\n",
       "   \"même,_suivant_l'exemple_des\",\n",
       "   \"suivant_l'exemple_des_chinois\",\n",
       "   \"l'exemple_des_chinois_et\",\n",
       "   'des_chinois_et_des',\n",
       "   'chinois_et_des_indiens,',\n",
       "   'et_des_indiens,_avait-il',\n",
       "   'des_indiens,_avait-il_déterminé'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 120,\n",
       "  'stop': 128,\n",
       "  'start_token_fr': 124,\n",
       "  'stop_token_fr': 132,\n",
       "  'history_tokens': [\"peut-être_même,_suivant_l'exemple\",\n",
       "   \"même,_suivant_l'exemple_des\",\n",
       "   \"suivant_l'exemple_des_chinois\",\n",
       "   \"l'exemple_des_chinois_et\"],\n",
       "  'span_tokens': ['des_chinois_et_des',\n",
       "   'chinois_et_des_indiens,',\n",
       "   'et_des_indiens,_avait-il',\n",
       "   'des_indiens,_avait-il_déterminé',\n",
       "   'indiens,_avait-il_déterminé_la',\n",
       "   'avait-il_déterminé_la_production',\n",
       "   'déterminé_la_production_de',\n",
       "   'la_production_de_cette'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 124,\n",
       "  'stop': 132,\n",
       "  'start_token_fr': 128,\n",
       "  'stop_token_fr': 136,\n",
       "  'history_tokens': ['des_chinois_et_des',\n",
       "   'chinois_et_des_indiens,',\n",
       "   'et_des_indiens,_avait-il',\n",
       "   'des_indiens,_avait-il_déterminé'],\n",
       "  'span_tokens': ['indiens,_avait-il_déterminé_la',\n",
       "   'avait-il_déterminé_la_production',\n",
       "   'déterminé_la_production_de',\n",
       "   'la_production_de_cette',\n",
       "   'production_de_cette_perle',\n",
       "   'de_cette_perle_en',\n",
       "   'cette_perle_en_introduisant',\n",
       "   'perle_en_introduisant_sous'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 128,\n",
       "  'stop': 136,\n",
       "  'start_token_fr': 132,\n",
       "  'stop_token_fr': 140,\n",
       "  'history_tokens': ['indiens,_avait-il_déterminé_la',\n",
       "   'avait-il_déterminé_la_production',\n",
       "   'déterminé_la_production_de',\n",
       "   'la_production_de_cette'],\n",
       "  'span_tokens': ['production_de_cette_perle',\n",
       "   'de_cette_perle_en',\n",
       "   'cette_perle_en_introduisant',\n",
       "   'perle_en_introduisant_sous',\n",
       "   'en_introduisant_sous_les',\n",
       "   'introduisant_sous_les_plis',\n",
       "   'sous_les_plis_du',\n",
       "   'les_plis_du_mollusque'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 132,\n",
       "  'stop': 140,\n",
       "  'start_token_fr': 136,\n",
       "  'stop_token_fr': 145,\n",
       "  'history_tokens': ['production_de_cette_perle',\n",
       "   'de_cette_perle_en',\n",
       "   'cette_perle_en_introduisant',\n",
       "   'perle_en_introduisant_sous'],\n",
       "  'span_tokens': ['en_introduisant_sous_les',\n",
       "   'introduisant_sous_les_plis',\n",
       "   'sous_les_plis_du',\n",
       "   'les_plis_du_mollusque',\n",
       "   'plis_du_mollusque_quelque',\n",
       "   'du_mollusque_quelque_morceau',\n",
       "   'mollusque_quelque_morceau_de',\n",
       "   'quelque_morceau_de_verre',\n",
       "   'morceau_de_verre_et'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 136,\n",
       "  'stop': 144,\n",
       "  'start_token_fr': 140,\n",
       "  'stop_token_fr': 149,\n",
       "  'history_tokens': ['en_introduisant_sous_les',\n",
       "   'introduisant_sous_les_plis',\n",
       "   'sous_les_plis_du',\n",
       "   'les_plis_du_mollusque'],\n",
       "  'span_tokens': ['plis_du_mollusque_quelque',\n",
       "   'du_mollusque_quelque_morceau',\n",
       "   'mollusque_quelque_morceau_de',\n",
       "   'quelque_morceau_de_verre',\n",
       "   'morceau_de_verre_et',\n",
       "   'de_verre_et_de',\n",
       "   'verre_et_de_métal,',\n",
       "   'et_de_métal,_qui',\n",
       "   \"de_métal,_qui_s'était\"],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 140,\n",
       "  'stop': 148,\n",
       "  'start_token_fr': 145,\n",
       "  'stop_token_fr': 153,\n",
       "  'history_tokens': ['du_mollusque_quelque_morceau',\n",
       "   'mollusque_quelque_morceau_de',\n",
       "   'quelque_morceau_de_verre',\n",
       "   'morceau_de_verre_et'],\n",
       "  'span_tokens': ['de_verre_et_de',\n",
       "   'verre_et_de_métal,',\n",
       "   'et_de_métal,_qui',\n",
       "   \"de_métal,_qui_s'était\",\n",
       "   \"métal,_qui_s'était_peu\",\n",
       "   \"qui_s'était_peu_à\",\n",
       "   \"s'était_peu_à_peu\",\n",
       "   'peu_à_peu_recouvert'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 4,\n",
       "  'start': 144,\n",
       "  'stop': 152,\n",
       "  'start_token_fr': 149,\n",
       "  'stop_token_fr': 157,\n",
       "  'history_tokens': ['de_verre_et_de',\n",
       "   'verre_et_de_métal,',\n",
       "   'et_de_métal,_qui',\n",
       "   \"de_métal,_qui_s'était\"],\n",
       "  'span_tokens': [\"métal,_qui_s'était_peu\",\n",
       "   \"qui_s'était_peu_à\",\n",
       "   \"s'était_peu_à_peu\",\n",
       "   'peu_à_peu_recouvert',\n",
       "   'à_peu_recouvert_de',\n",
       "   'peu_recouvert_de_la',\n",
       "   'recouvert_de_la_matière',\n",
       "   'de_la_matière_nacrée.'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 5,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 9,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['et',\n",
       "   'le',\n",
       "   'marin',\n",
       "   'fit',\n",
       "   'le',\n",
       "   'récit',\n",
       "   'de',\n",
       "   'ce',\n",
       "   'qui'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 5,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 13,\n",
       "  'history_tokens': ['et', 'le', 'marin', 'fit'],\n",
       "  'span_tokens': ['le',\n",
       "   'récit',\n",
       "   'de',\n",
       "   'ce',\n",
       "   'qui',\n",
       "   \"s'était\",\n",
       "   'passé',\n",
       "   '__sent_marker_0',\n",
       "   'la'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 5,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 9,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['récit', 'de', 'ce', 'qui'],\n",
       "  'span_tokens': [\"s'était\",\n",
       "   'passé',\n",
       "   '__sent_marker_0',\n",
       "   'la',\n",
       "   'veille.',\n",
       "   'et_le',\n",
       "   'le_marin',\n",
       "   'marin_fit',\n",
       "   'fit_le'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 5,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 13,\n",
       "  'stop_token_fr': 22,\n",
       "  'history_tokens': [\"s'était\", 'passé', '__sent_marker_0', 'la'],\n",
       "  'span_tokens': ['veille.',\n",
       "   'et_le',\n",
       "   'le_marin',\n",
       "   'marin_fit',\n",
       "   'fit_le',\n",
       "   'le_récit',\n",
       "   'récit_de',\n",
       "   'de_ce',\n",
       "   'ce_qui'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 5,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 18,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['et_le', 'le_marin', 'marin_fit', 'fit_le'],\n",
       "  'span_tokens': ['le_récit',\n",
       "   'récit_de',\n",
       "   'de_ce',\n",
       "   'ce_qui',\n",
       "   '__sent_marker_1',\n",
       "   \"qui_s'était\",\n",
       "   \"s'était_passé\",\n",
       "   'passé_la',\n",
       "   'la_veille.'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 5,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 22,\n",
       "  'stop_token_fr': 31,\n",
       "  'history_tokens': ['le_récit', 'récit_de', 'de_ce', 'ce_qui'],\n",
       "  'span_tokens': ['__sent_marker_1',\n",
       "   \"qui_s'était\",\n",
       "   \"s'était_passé\",\n",
       "   'passé_la',\n",
       "   'la_veille.',\n",
       "   'et_le_marin',\n",
       "   'le_marin_fit',\n",
       "   'marin_fit_le',\n",
       "   'fit_le_récit'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 5,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 36,\n",
       "  'history_tokens': [\"qui_s'était\", \"s'était_passé\", 'passé_la', 'la_veille.'],\n",
       "  'span_tokens': ['et_le_marin',\n",
       "   'le_marin_fit',\n",
       "   'marin_fit_le',\n",
       "   'fit_le_récit',\n",
       "   'le_récit_de',\n",
       "   'récit_de_ce',\n",
       "   '__sent_marker_2',\n",
       "   'de_ce_qui',\n",
       "   \"ce_qui_s'était\"],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 5,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 31,\n",
       "  'stop_token_fr': 40,\n",
       "  'history_tokens': ['et_le_marin',\n",
       "   'le_marin_fit',\n",
       "   'marin_fit_le',\n",
       "   'fit_le_récit'],\n",
       "  'span_tokens': ['le_récit_de',\n",
       "   'récit_de_ce',\n",
       "   '__sent_marker_2',\n",
       "   'de_ce_qui',\n",
       "   \"ce_qui_s'était\",\n",
       "   \"qui_s'était_passé\",\n",
       "   \"s'était_passé_la\",\n",
       "   'passé_la_veille.',\n",
       "   'et_le_marin_fit'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 5,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 36,\n",
       "  'stop_token_fr': 45,\n",
       "  'history_tokens': ['récit_de_ce',\n",
       "   '__sent_marker_2',\n",
       "   'de_ce_qui',\n",
       "   \"ce_qui_s'était\"],\n",
       "  'span_tokens': [\"qui_s'était_passé\",\n",
       "   \"s'était_passé_la\",\n",
       "   'passé_la_veille.',\n",
       "   'et_le_marin_fit',\n",
       "   'le_marin_fit_le',\n",
       "   'marin_fit_le_récit',\n",
       "   'fit_le_récit_de',\n",
       "   'le_récit_de_ce',\n",
       "   'récit_de_ce_qui'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 5,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 40,\n",
       "  'stop_token_fr': 49,\n",
       "  'history_tokens': [\"qui_s'était_passé\",\n",
       "   \"s'était_passé_la\",\n",
       "   'passé_la_veille.',\n",
       "   'et_le_marin_fit'],\n",
       "  'span_tokens': ['le_marin_fit_le',\n",
       "   'marin_fit_le_récit',\n",
       "   'fit_le_récit_de',\n",
       "   'le_récit_de_ce',\n",
       "   'récit_de_ce_qui',\n",
       "   \"de_ce_qui_s'était\",\n",
       "   \"ce_qui_s'était_passé\",\n",
       "   \"qui_s'était_passé_la\",\n",
       "   \"s'était_passé_la_veille.\"],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['«vous',\n",
       "   'vous',\n",
       "   'trompez,',\n",
       "   'messieurs,',\n",
       "   'dit-il,',\n",
       "   'ce',\n",
       "   'mouchoir'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 3,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': ['«vous', 'vous', 'trompez,'],\n",
       "  'span_tokens': ['messieurs,',\n",
       "   'dit-il,',\n",
       "   'ce',\n",
       "   'mouchoir',\n",
       "   \"n'est\",\n",
       "   'pas',\n",
       "   'à'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['messieurs,', 'dit-il,', 'ce', 'mouchoir'],\n",
       "  'span_tokens': [\"n'est\", 'pas', 'à', 'moi,', 'et', 'je', 'ne'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 17,\n",
       "  'history_tokens': ['mouchoir', \"n'est\", 'pas', 'à'],\n",
       "  'span_tokens': ['moi,', 'et', 'je', 'ne', 'sais', 'pourquoi', 'monsieur'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 20,\n",
       "  'history_tokens': ['moi,', 'et', 'je', 'ne'],\n",
       "  'span_tokens': ['sais', 'pourquoi', 'monsieur', 'a', 'eu', 'la'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 17,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['ne', 'sais', 'pourquoi', 'monsieur'],\n",
       "  'span_tokens': ['a', 'eu', 'la', 'fantaisie', 'de', 'me', 'le'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 20,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['monsieur', 'a', 'eu', 'la'],\n",
       "  'span_tokens': ['fantaisie', 'de', 'me', 'le', 'remettre', 'plutôt', \"qu'à\"],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 24,\n",
       "  'stop_token_fr': 31,\n",
       "  'history_tokens': ['fantaisie', 'de', 'me', 'le'],\n",
       "  'span_tokens': ['remettre', 'plutôt', \"qu'à\", \"l'un\", 'de', 'vous,', 'et'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 34,\n",
       "  'history_tokens': ['le', 'remettre', 'plutôt', \"qu'à\"],\n",
       "  'span_tokens': [\"l'un\", 'de', 'vous,', 'et', 'la', 'preuve', 'de'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 31,\n",
       "  'stop_token_fr': 38,\n",
       "  'history_tokens': [\"l'un\", 'de', 'vous,', 'et'],\n",
       "  'span_tokens': ['la', 'preuve', 'de', 'ce', 'que', 'je', 'dis,'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 34,\n",
       "  'stop_token_fr': 41,\n",
       "  'history_tokens': ['et', 'la', 'preuve', 'de'],\n",
       "  'span_tokens': ['ce', 'que', 'je', 'dis,', \"c'est\", 'que', 'voici'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 38,\n",
       "  'stop_token_fr': 44,\n",
       "  'history_tokens': ['ce', 'que', 'je', 'dis,'],\n",
       "  'span_tokens': [\"c'est\", 'que', 'voici', 'le', 'mien', 'dans'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 41,\n",
       "  'stop_token_fr': 48,\n",
       "  'history_tokens': ['dis,', \"c'est\", 'que', 'voici'],\n",
       "  'span_tokens': ['le',\n",
       "   'mien',\n",
       "   'dans',\n",
       "   '__sent_marker_0',\n",
       "   'ma',\n",
       "   'poche.»',\n",
       "   '«vous_vous'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 44,\n",
       "  'stop_token_fr': 51,\n",
       "  'history_tokens': ['voici', 'le', 'mien', 'dans'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'ma',\n",
       "   'poche.»',\n",
       "   '«vous_vous',\n",
       "   'vous_trompez,',\n",
       "   'trompez,_messieurs,',\n",
       "   'messieurs,_dit-il,'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 48,\n",
       "  'stop_token_fr': 55,\n",
       "  'history_tokens': ['__sent_marker_0', 'ma', 'poche.»', '«vous_vous'],\n",
       "  'span_tokens': ['vous_trompez,',\n",
       "   'trompez,_messieurs,',\n",
       "   'messieurs,_dit-il,',\n",
       "   'dit-il,_ce',\n",
       "   'ce_mouchoir',\n",
       "   \"mouchoir_n'est\",\n",
       "   \"n'est_pas\"],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 51,\n",
       "  'stop_token_fr': 58,\n",
       "  'history_tokens': ['«vous_vous',\n",
       "   'vous_trompez,',\n",
       "   'trompez,_messieurs,',\n",
       "   'messieurs,_dit-il,'],\n",
       "  'span_tokens': ['dit-il,_ce',\n",
       "   'ce_mouchoir',\n",
       "   \"mouchoir_n'est\",\n",
       "   \"n'est_pas\",\n",
       "   'pas_à',\n",
       "   'à_moi,',\n",
       "   'moi,_et'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 55,\n",
       "  'stop_token_fr': 61,\n",
       "  'history_tokens': ['dit-il,_ce',\n",
       "   'ce_mouchoir',\n",
       "   \"mouchoir_n'est\",\n",
       "   \"n'est_pas\"],\n",
       "  'span_tokens': ['pas_à', 'à_moi,', 'moi,_et', 'et_je', 'je_ne', 'ne_sais'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 58,\n",
       "  'stop_token_fr': 65,\n",
       "  'history_tokens': [\"n'est_pas\", 'pas_à', 'à_moi,', 'moi,_et'],\n",
       "  'span_tokens': ['et_je',\n",
       "   'je_ne',\n",
       "   'ne_sais',\n",
       "   'sais_pourquoi',\n",
       "   'pourquoi_monsieur',\n",
       "   'monsieur_a',\n",
       "   'a_eu'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 61,\n",
       "  'stop_token_fr': 68,\n",
       "  'history_tokens': ['moi,_et', 'et_je', 'je_ne', 'ne_sais'],\n",
       "  'span_tokens': ['sais_pourquoi',\n",
       "   'pourquoi_monsieur',\n",
       "   'monsieur_a',\n",
       "   'a_eu',\n",
       "   'eu_la',\n",
       "   'la_fantaisie',\n",
       "   'fantaisie_de'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 65,\n",
       "  'stop_token_fr': 72,\n",
       "  'history_tokens': ['sais_pourquoi',\n",
       "   'pourquoi_monsieur',\n",
       "   'monsieur_a',\n",
       "   'a_eu'],\n",
       "  'span_tokens': ['eu_la',\n",
       "   'la_fantaisie',\n",
       "   'fantaisie_de',\n",
       "   'de_me',\n",
       "   'me_le',\n",
       "   'le_remettre',\n",
       "   'remettre_plutôt'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 68,\n",
       "  'stop_token_fr': 75,\n",
       "  'history_tokens': ['a_eu', 'eu_la', 'la_fantaisie', 'fantaisie_de'],\n",
       "  'span_tokens': ['de_me',\n",
       "   'me_le',\n",
       "   'le_remettre',\n",
       "   'remettre_plutôt',\n",
       "   \"plutôt_qu'à\",\n",
       "   \"qu'à_l'un\",\n",
       "   \"l'un_de\"],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 72,\n",
       "  'stop_token_fr': 79,\n",
       "  'history_tokens': ['de_me', 'me_le', 'le_remettre', 'remettre_plutôt'],\n",
       "  'span_tokens': [\"plutôt_qu'à\",\n",
       "   \"qu'à_l'un\",\n",
       "   \"l'un_de\",\n",
       "   'de_vous,',\n",
       "   'vous,_et',\n",
       "   'et_la',\n",
       "   'la_preuve'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 75,\n",
       "  'stop_token_fr': 82,\n",
       "  'history_tokens': ['remettre_plutôt', \"plutôt_qu'à\", \"qu'à_l'un\", \"l'un_de\"],\n",
       "  'span_tokens': ['de_vous,',\n",
       "   'vous,_et',\n",
       "   'et_la',\n",
       "   'la_preuve',\n",
       "   'preuve_de',\n",
       "   'de_ce',\n",
       "   'ce_que'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 79,\n",
       "  'stop_token_fr': 85,\n",
       "  'history_tokens': ['de_vous,', 'vous,_et', 'et_la', 'la_preuve'],\n",
       "  'span_tokens': ['preuve_de',\n",
       "   'de_ce',\n",
       "   'ce_que',\n",
       "   'que_je',\n",
       "   'je_dis,',\n",
       "   \"dis,_c'est\"],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 82,\n",
       "  'stop_token_fr': 89,\n",
       "  'history_tokens': ['la_preuve', 'preuve_de', 'de_ce', 'ce_que'],\n",
       "  'span_tokens': ['que_je',\n",
       "   'je_dis,',\n",
       "   \"dis,_c'est\",\n",
       "   \"c'est_que\",\n",
       "   'que_voici',\n",
       "   'voici_le',\n",
       "   '__sent_marker_1'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 85,\n",
       "  'stop_token_fr': 92,\n",
       "  'history_tokens': ['ce_que', 'que_je', 'je_dis,', \"dis,_c'est\"],\n",
       "  'span_tokens': [\"c'est_que\",\n",
       "   'que_voici',\n",
       "   'voici_le',\n",
       "   '__sent_marker_1',\n",
       "   'le_mien',\n",
       "   'mien_dans',\n",
       "   'dans_ma'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 89,\n",
       "  'stop_token_fr': 96,\n",
       "  'history_tokens': [\"c'est_que\", 'que_voici', 'voici_le', '__sent_marker_1'],\n",
       "  'span_tokens': ['le_mien',\n",
       "   'mien_dans',\n",
       "   'dans_ma',\n",
       "   'ma_poche.»',\n",
       "   '«vous_vous_trompez,',\n",
       "   'vous_trompez,_messieurs,',\n",
       "   'trompez,_messieurs,_dit-il,'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 92,\n",
       "  'stop_token_fr': 99,\n",
       "  'history_tokens': ['__sent_marker_1', 'le_mien', 'mien_dans', 'dans_ma'],\n",
       "  'span_tokens': ['ma_poche.»',\n",
       "   '«vous_vous_trompez,',\n",
       "   'vous_trompez,_messieurs,',\n",
       "   'trompez,_messieurs,_dit-il,',\n",
       "   'messieurs,_dit-il,_ce',\n",
       "   'dit-il,_ce_mouchoir',\n",
       "   \"ce_mouchoir_n'est\"],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 96,\n",
       "  'stop_token_fr': 102,\n",
       "  'history_tokens': ['ma_poche.»',\n",
       "   '«vous_vous_trompez,',\n",
       "   'vous_trompez,_messieurs,',\n",
       "   'trompez,_messieurs,_dit-il,'],\n",
       "  'span_tokens': ['messieurs,_dit-il,_ce',\n",
       "   'dit-il,_ce_mouchoir',\n",
       "   \"ce_mouchoir_n'est\",\n",
       "   \"mouchoir_n'est_pas\",\n",
       "   \"n'est_pas_à\",\n",
       "   'pas_à_moi,'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 99,\n",
       "  'stop_token_fr': 106,\n",
       "  'history_tokens': ['trompez,_messieurs,_dit-il,',\n",
       "   'messieurs,_dit-il,_ce',\n",
       "   'dit-il,_ce_mouchoir',\n",
       "   \"ce_mouchoir_n'est\"],\n",
       "  'span_tokens': [\"mouchoir_n'est_pas\",\n",
       "   \"n'est_pas_à\",\n",
       "   'pas_à_moi,',\n",
       "   'à_moi,_et',\n",
       "   'moi,_et_je',\n",
       "   'et_je_ne',\n",
       "   'je_ne_sais'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 120,\n",
       "  'stop': 128,\n",
       "  'start_token_fr': 102,\n",
       "  'stop_token_fr': 109,\n",
       "  'history_tokens': [\"ce_mouchoir_n'est\",\n",
       "   \"mouchoir_n'est_pas\",\n",
       "   \"n'est_pas_à\",\n",
       "   'pas_à_moi,'],\n",
       "  'span_tokens': ['à_moi,_et',\n",
       "   'moi,_et_je',\n",
       "   'et_je_ne',\n",
       "   'je_ne_sais',\n",
       "   'ne_sais_pourquoi',\n",
       "   'sais_pourquoi_monsieur',\n",
       "   'pourquoi_monsieur_a'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 124,\n",
       "  'stop': 132,\n",
       "  'start_token_fr': 106,\n",
       "  'stop_token_fr': 113,\n",
       "  'history_tokens': ['à_moi,_et', 'moi,_et_je', 'et_je_ne', 'je_ne_sais'],\n",
       "  'span_tokens': ['ne_sais_pourquoi',\n",
       "   'sais_pourquoi_monsieur',\n",
       "   'pourquoi_monsieur_a',\n",
       "   'monsieur_a_eu',\n",
       "   'a_eu_la',\n",
       "   'eu_la_fantaisie',\n",
       "   'la_fantaisie_de'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 128,\n",
       "  'stop': 136,\n",
       "  'start_token_fr': 109,\n",
       "  'stop_token_fr': 116,\n",
       "  'history_tokens': ['je_ne_sais',\n",
       "   'ne_sais_pourquoi',\n",
       "   'sais_pourquoi_monsieur',\n",
       "   'pourquoi_monsieur_a'],\n",
       "  'span_tokens': ['monsieur_a_eu',\n",
       "   'a_eu_la',\n",
       "   'eu_la_fantaisie',\n",
       "   'la_fantaisie_de',\n",
       "   'fantaisie_de_me',\n",
       "   'de_me_le',\n",
       "   'me_le_remettre'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 132,\n",
       "  'stop': 140,\n",
       "  'start_token_fr': 113,\n",
       "  'stop_token_fr': 120,\n",
       "  'history_tokens': ['monsieur_a_eu',\n",
       "   'a_eu_la',\n",
       "   'eu_la_fantaisie',\n",
       "   'la_fantaisie_de'],\n",
       "  'span_tokens': ['fantaisie_de_me',\n",
       "   'de_me_le',\n",
       "   'me_le_remettre',\n",
       "   'le_remettre_plutôt',\n",
       "   \"remettre_plutôt_qu'à\",\n",
       "   \"plutôt_qu'à_l'un\",\n",
       "   \"qu'à_l'un_de\"],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 136,\n",
       "  'stop': 144,\n",
       "  'start_token_fr': 116,\n",
       "  'stop_token_fr': 123,\n",
       "  'history_tokens': ['la_fantaisie_de',\n",
       "   'fantaisie_de_me',\n",
       "   'de_me_le',\n",
       "   'me_le_remettre'],\n",
       "  'span_tokens': ['le_remettre_plutôt',\n",
       "   \"remettre_plutôt_qu'à\",\n",
       "   \"plutôt_qu'à_l'un\",\n",
       "   \"qu'à_l'un_de\",\n",
       "   \"l'un_de_vous,\",\n",
       "   'de_vous,_et',\n",
       "   'vous,_et_la'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 140,\n",
       "  'stop': 148,\n",
       "  'start_token_fr': 120,\n",
       "  'stop_token_fr': 126,\n",
       "  'history_tokens': ['le_remettre_plutôt',\n",
       "   \"remettre_plutôt_qu'à\",\n",
       "   \"plutôt_qu'à_l'un\",\n",
       "   \"qu'à_l'un_de\"],\n",
       "  'span_tokens': [\"l'un_de_vous,\",\n",
       "   'de_vous,_et',\n",
       "   'vous,_et_la',\n",
       "   'et_la_preuve',\n",
       "   'la_preuve_de',\n",
       "   'preuve_de_ce'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 144,\n",
       "  'stop': 152,\n",
       "  'start_token_fr': 123,\n",
       "  'stop_token_fr': 130,\n",
       "  'history_tokens': [\"qu'à_l'un_de\",\n",
       "   \"l'un_de_vous,\",\n",
       "   'de_vous,_et',\n",
       "   'vous,_et_la'],\n",
       "  'span_tokens': ['et_la_preuve',\n",
       "   'la_preuve_de',\n",
       "   'preuve_de_ce',\n",
       "   'de_ce_que',\n",
       "   'ce_que_je',\n",
       "   'que_je_dis,',\n",
       "   \"je_dis,_c'est\"],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 148,\n",
       "  'stop': 156,\n",
       "  'start_token_fr': 126,\n",
       "  'stop_token_fr': 133,\n",
       "  'history_tokens': ['vous,_et_la',\n",
       "   'et_la_preuve',\n",
       "   'la_preuve_de',\n",
       "   'preuve_de_ce'],\n",
       "  'span_tokens': ['de_ce_que',\n",
       "   'ce_que_je',\n",
       "   'que_je_dis,',\n",
       "   \"je_dis,_c'est\",\n",
       "   \"dis,_c'est_que\",\n",
       "   \"c'est_que_voici\",\n",
       "   '__sent_marker_2'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 152,\n",
       "  'stop': 160,\n",
       "  'start_token_fr': 130,\n",
       "  'stop_token_fr': 137,\n",
       "  'history_tokens': ['de_ce_que', 'ce_que_je', 'que_je_dis,', \"je_dis,_c'est\"],\n",
       "  'span_tokens': [\"dis,_c'est_que\",\n",
       "   \"c'est_que_voici\",\n",
       "   '__sent_marker_2',\n",
       "   'que_voici_le',\n",
       "   'voici_le_mien',\n",
       "   'le_mien_dans',\n",
       "   'mien_dans_ma'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 156,\n",
       "  'stop': 164,\n",
       "  'start_token_fr': 133,\n",
       "  'stop_token_fr': 140,\n",
       "  'history_tokens': [\"je_dis,_c'est\",\n",
       "   \"dis,_c'est_que\",\n",
       "   \"c'est_que_voici\",\n",
       "   '__sent_marker_2'],\n",
       "  'span_tokens': ['que_voici_le',\n",
       "   'voici_le_mien',\n",
       "   'le_mien_dans',\n",
       "   'mien_dans_ma',\n",
       "   'dans_ma_poche.»',\n",
       "   '«vous_vous_trompez,_messieurs,',\n",
       "   'vous_trompez,_messieurs,_dit-il,'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 160,\n",
       "  'stop': 168,\n",
       "  'start_token_fr': 137,\n",
       "  'stop_token_fr': 143,\n",
       "  'history_tokens': ['que_voici_le',\n",
       "   'voici_le_mien',\n",
       "   'le_mien_dans',\n",
       "   'mien_dans_ma'],\n",
       "  'span_tokens': ['dans_ma_poche.»',\n",
       "   '«vous_vous_trompez,_messieurs,',\n",
       "   'vous_trompez,_messieurs,_dit-il,',\n",
       "   'trompez,_messieurs,_dit-il,_ce',\n",
       "   'messieurs,_dit-il,_ce_mouchoir',\n",
       "   \"dit-il,_ce_mouchoir_n'est\"],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 164,\n",
       "  'stop': 172,\n",
       "  'start_token_fr': 140,\n",
       "  'stop_token_fr': 147,\n",
       "  'history_tokens': ['mien_dans_ma',\n",
       "   'dans_ma_poche.»',\n",
       "   '«vous_vous_trompez,_messieurs,',\n",
       "   'vous_trompez,_messieurs,_dit-il,'],\n",
       "  'span_tokens': ['trompez,_messieurs,_dit-il,_ce',\n",
       "   'messieurs,_dit-il,_ce_mouchoir',\n",
       "   \"dit-il,_ce_mouchoir_n'est\",\n",
       "   \"ce_mouchoir_n'est_pas\",\n",
       "   \"mouchoir_n'est_pas_à\",\n",
       "   \"n'est_pas_à_moi,\",\n",
       "   'pas_à_moi,_et'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 168,\n",
       "  'stop': 176,\n",
       "  'start_token_fr': 143,\n",
       "  'stop_token_fr': 150,\n",
       "  'history_tokens': ['vous_trompez,_messieurs,_dit-il,',\n",
       "   'trompez,_messieurs,_dit-il,_ce',\n",
       "   'messieurs,_dit-il,_ce_mouchoir',\n",
       "   \"dit-il,_ce_mouchoir_n'est\"],\n",
       "  'span_tokens': [\"ce_mouchoir_n'est_pas\",\n",
       "   \"mouchoir_n'est_pas_à\",\n",
       "   \"n'est_pas_à_moi,\",\n",
       "   'pas_à_moi,_et',\n",
       "   'à_moi,_et_je',\n",
       "   'moi,_et_je_ne',\n",
       "   'et_je_ne_sais'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 172,\n",
       "  'stop': 180,\n",
       "  'start_token_fr': 147,\n",
       "  'stop_token_fr': 154,\n",
       "  'history_tokens': [\"ce_mouchoir_n'est_pas\",\n",
       "   \"mouchoir_n'est_pas_à\",\n",
       "   \"n'est_pas_à_moi,\",\n",
       "   'pas_à_moi,_et'],\n",
       "  'span_tokens': ['à_moi,_et_je',\n",
       "   'moi,_et_je_ne',\n",
       "   'et_je_ne_sais',\n",
       "   'je_ne_sais_pourquoi',\n",
       "   'ne_sais_pourquoi_monsieur',\n",
       "   'sais_pourquoi_monsieur_a',\n",
       "   'pourquoi_monsieur_a_eu'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 176,\n",
       "  'stop': 184,\n",
       "  'start_token_fr': 150,\n",
       "  'stop_token_fr': 157,\n",
       "  'history_tokens': ['pas_à_moi,_et',\n",
       "   'à_moi,_et_je',\n",
       "   'moi,_et_je_ne',\n",
       "   'et_je_ne_sais'],\n",
       "  'span_tokens': ['je_ne_sais_pourquoi',\n",
       "   'ne_sais_pourquoi_monsieur',\n",
       "   'sais_pourquoi_monsieur_a',\n",
       "   'pourquoi_monsieur_a_eu',\n",
       "   'monsieur_a_eu_la',\n",
       "   'a_eu_la_fantaisie',\n",
       "   'eu_la_fantaisie_de'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 180,\n",
       "  'stop': 188,\n",
       "  'start_token_fr': 154,\n",
       "  'stop_token_fr': 161,\n",
       "  'history_tokens': ['je_ne_sais_pourquoi',\n",
       "   'ne_sais_pourquoi_monsieur',\n",
       "   'sais_pourquoi_monsieur_a',\n",
       "   'pourquoi_monsieur_a_eu'],\n",
       "  'span_tokens': ['monsieur_a_eu_la',\n",
       "   'a_eu_la_fantaisie',\n",
       "   'eu_la_fantaisie_de',\n",
       "   'la_fantaisie_de_me',\n",
       "   'fantaisie_de_me_le',\n",
       "   'de_me_le_remettre',\n",
       "   'me_le_remettre_plutôt'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 184,\n",
       "  'stop': 192,\n",
       "  'start_token_fr': 157,\n",
       "  'stop_token_fr': 164,\n",
       "  'history_tokens': ['pourquoi_monsieur_a_eu',\n",
       "   'monsieur_a_eu_la',\n",
       "   'a_eu_la_fantaisie',\n",
       "   'eu_la_fantaisie_de'],\n",
       "  'span_tokens': ['la_fantaisie_de_me',\n",
       "   'fantaisie_de_me_le',\n",
       "   'de_me_le_remettre',\n",
       "   'me_le_remettre_plutôt',\n",
       "   \"le_remettre_plutôt_qu'à\",\n",
       "   \"remettre_plutôt_qu'à_l'un\",\n",
       "   \"plutôt_qu'à_l'un_de\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 188,\n",
       "  'stop': 196,\n",
       "  'start_token_fr': 161,\n",
       "  'stop_token_fr': 167,\n",
       "  'history_tokens': ['la_fantaisie_de_me',\n",
       "   'fantaisie_de_me_le',\n",
       "   'de_me_le_remettre',\n",
       "   'me_le_remettre_plutôt'],\n",
       "  'span_tokens': [\"le_remettre_plutôt_qu'à\",\n",
       "   \"remettre_plutôt_qu'à_l'un\",\n",
       "   \"plutôt_qu'à_l'un_de\",\n",
       "   \"qu'à_l'un_de_vous,\",\n",
       "   \"l'un_de_vous,_et\",\n",
       "   'de_vous,_et_la'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 192,\n",
       "  'stop': 200,\n",
       "  'start_token_fr': 164,\n",
       "  'stop_token_fr': 171,\n",
       "  'history_tokens': ['me_le_remettre_plutôt',\n",
       "   \"le_remettre_plutôt_qu'à\",\n",
       "   \"remettre_plutôt_qu'à_l'un\",\n",
       "   \"plutôt_qu'à_l'un_de\"],\n",
       "  'span_tokens': [\"qu'à_l'un_de_vous,\",\n",
       "   \"l'un_de_vous,_et\",\n",
       "   'de_vous,_et_la',\n",
       "   'vous,_et_la_preuve',\n",
       "   'et_la_preuve_de',\n",
       "   'la_preuve_de_ce',\n",
       "   'preuve_de_ce_que'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 196,\n",
       "  'stop': 204,\n",
       "  'start_token_fr': 167,\n",
       "  'stop_token_fr': 174,\n",
       "  'history_tokens': [\"plutôt_qu'à_l'un_de\",\n",
       "   \"qu'à_l'un_de_vous,\",\n",
       "   \"l'un_de_vous,_et\",\n",
       "   'de_vous,_et_la'],\n",
       "  'span_tokens': ['vous,_et_la_preuve',\n",
       "   'et_la_preuve_de',\n",
       "   'la_preuve_de_ce',\n",
       "   'preuve_de_ce_que',\n",
       "   'de_ce_que_je',\n",
       "   'ce_que_je_dis,',\n",
       "   \"que_je_dis,_c'est\"],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 200,\n",
       "  'stop': 208,\n",
       "  'start_token_fr': 171,\n",
       "  'stop_token_fr': 178,\n",
       "  'history_tokens': ['vous,_et_la_preuve',\n",
       "   'et_la_preuve_de',\n",
       "   'la_preuve_de_ce',\n",
       "   'preuve_de_ce_que'],\n",
       "  'span_tokens': ['de_ce_que_je',\n",
       "   'ce_que_je_dis,',\n",
       "   \"que_je_dis,_c'est\",\n",
       "   \"je_dis,_c'est_que\",\n",
       "   \"dis,_c'est_que_voici\",\n",
       "   \"c'est_que_voici_le\",\n",
       "   'que_voici_le_mien'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 6,\n",
       "  'start': 204,\n",
       "  'stop': 212,\n",
       "  'start_token_fr': 174,\n",
       "  'stop_token_fr': 181,\n",
       "  'history_tokens': ['preuve_de_ce_que',\n",
       "   'de_ce_que_je',\n",
       "   'ce_que_je_dis,',\n",
       "   \"que_je_dis,_c'est\"],\n",
       "  'span_tokens': [\"je_dis,_c'est_que\",\n",
       "   \"dis,_c'est_que_voici\",\n",
       "   \"c'est_que_voici_le\",\n",
       "   'que_voici_le_mien',\n",
       "   'voici_le_mien_dans',\n",
       "   'le_mien_dans_ma',\n",
       "   'mien_dans_ma_poche.»'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 6,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['il', 'n’avait', 'pas', 'été', 'aussi', 'agité'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 3,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': ['il', 'n’avait', 'pas'],\n",
       "  'span_tokens': ['été', 'aussi', 'agité', 'depuis', 'un', 'duel', 'qu’il'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 6,\n",
       "  'stop_token_fr': 13,\n",
       "  'history_tokens': ['pas', 'été', 'aussi', 'agité'],\n",
       "  'span_tokens': ['depuis', 'un', 'duel', 'qu’il', 'avait', 'failli', 'avoir'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 16,\n",
       "  'history_tokens': ['depuis', 'un', 'duel', 'qu’il'],\n",
       "  'span_tokens': ['avait', 'failli', 'avoir', 'en', '1816,', 'et,'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 13,\n",
       "  'stop_token_fr': 19,\n",
       "  'history_tokens': ['qu’il', 'avait', 'failli', 'avoir'],\n",
       "  'span_tokens': ['en', '1816,', 'et,', 'pour', 'lui', 'rendre'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 16,\n",
       "  'stop_token_fr': 22,\n",
       "  'history_tokens': ['avoir', 'en', '1816,', 'et,'],\n",
       "  'span_tokens': ['pour', 'lui', 'rendre', 'justice,', 'alors', 'la'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 19,\n",
       "  'stop_token_fr': 25,\n",
       "  'history_tokens': ['et,', 'pour', 'lui', 'rendre'],\n",
       "  'span_tokens': ['justice,', 'alors', 'la', 'perspective', 'de', 'recevoir'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 22,\n",
       "  'stop_token_fr': 29,\n",
       "  'history_tokens': ['rendre', 'justice,', 'alors', 'la'],\n",
       "  'span_tokens': ['perspective',\n",
       "   'de',\n",
       "   'recevoir',\n",
       "   'une',\n",
       "   'balle',\n",
       "   'l’avait',\n",
       "   'rendu'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 25,\n",
       "  'stop_token_fr': 32,\n",
       "  'history_tokens': ['la', 'perspective', 'de', 'recevoir'],\n",
       "  'span_tokens': ['une',\n",
       "   'balle',\n",
       "   'l’avait',\n",
       "   'rendu',\n",
       "   '__sent_marker_0',\n",
       "   'moins',\n",
       "   'malheureux.'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 29,\n",
       "  'stop_token_fr': 35,\n",
       "  'history_tokens': ['une', 'balle', 'l’avait', 'rendu'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'moins',\n",
       "   'malheureux.',\n",
       "   'il_n’avait',\n",
       "   'n’avait_pas',\n",
       "   'pas_été'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 32,\n",
       "  'stop_token_fr': 38,\n",
       "  'history_tokens': ['rendu', '__sent_marker_0', 'moins', 'malheureux.'],\n",
       "  'span_tokens': ['il_n’avait',\n",
       "   'n’avait_pas',\n",
       "   'pas_été',\n",
       "   'été_aussi',\n",
       "   'aussi_agité',\n",
       "   'agité_depuis'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 35,\n",
       "  'stop_token_fr': 41,\n",
       "  'history_tokens': ['malheureux.', 'il_n’avait', 'n’avait_pas', 'pas_été'],\n",
       "  'span_tokens': ['été_aussi',\n",
       "   'aussi_agité',\n",
       "   'agité_depuis',\n",
       "   'depuis_un',\n",
       "   'un_duel',\n",
       "   'duel_qu’il'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 38,\n",
       "  'stop_token_fr': 45,\n",
       "  'history_tokens': ['pas_été', 'été_aussi', 'aussi_agité', 'agité_depuis'],\n",
       "  'span_tokens': ['depuis_un',\n",
       "   'un_duel',\n",
       "   'duel_qu’il',\n",
       "   'qu’il_avait',\n",
       "   'avait_failli',\n",
       "   'failli_avoir',\n",
       "   'avoir_en'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 41,\n",
       "  'stop_token_fr': 48,\n",
       "  'history_tokens': ['agité_depuis', 'depuis_un', 'un_duel', 'duel_qu’il'],\n",
       "  'span_tokens': ['qu’il_avait',\n",
       "   'avait_failli',\n",
       "   'failli_avoir',\n",
       "   'avoir_en',\n",
       "   'en_1816,',\n",
       "   '1816,_et,',\n",
       "   'et,_pour'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 45,\n",
       "  'stop_token_fr': 51,\n",
       "  'history_tokens': ['qu’il_avait',\n",
       "   'avait_failli',\n",
       "   'failli_avoir',\n",
       "   'avoir_en'],\n",
       "  'span_tokens': ['en_1816,',\n",
       "   '1816,_et,',\n",
       "   'et,_pour',\n",
       "   'pour_lui',\n",
       "   'lui_rendre',\n",
       "   'rendre_justice,'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 48,\n",
       "  'stop_token_fr': 54,\n",
       "  'history_tokens': ['avoir_en', 'en_1816,', '1816,_et,', 'et,_pour'],\n",
       "  'span_tokens': ['pour_lui',\n",
       "   'lui_rendre',\n",
       "   'rendre_justice,',\n",
       "   'justice,_alors',\n",
       "   'alors_la',\n",
       "   'la_perspective'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 51,\n",
       "  'stop_token_fr': 57,\n",
       "  'history_tokens': ['et,_pour', 'pour_lui', 'lui_rendre', 'rendre_justice,'],\n",
       "  'span_tokens': ['justice,_alors',\n",
       "   'alors_la',\n",
       "   'la_perspective',\n",
       "   'perspective_de',\n",
       "   'de_recevoir',\n",
       "   'recevoir_une'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 54,\n",
       "  'stop_token_fr': 60,\n",
       "  'history_tokens': ['rendre_justice,',\n",
       "   'justice,_alors',\n",
       "   'alors_la',\n",
       "   'la_perspective'],\n",
       "  'span_tokens': ['perspective_de',\n",
       "   'de_recevoir',\n",
       "   'recevoir_une',\n",
       "   'une_balle',\n",
       "   '__sent_marker_1',\n",
       "   'balle_l’avait'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 57,\n",
       "  'stop_token_fr': 64,\n",
       "  'history_tokens': ['la_perspective',\n",
       "   'perspective_de',\n",
       "   'de_recevoir',\n",
       "   'recevoir_une'],\n",
       "  'span_tokens': ['une_balle',\n",
       "   '__sent_marker_1',\n",
       "   'balle_l’avait',\n",
       "   'l’avait_rendu',\n",
       "   'rendu_moins',\n",
       "   'moins_malheureux.',\n",
       "   'il_n’avait_pas'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 60,\n",
       "  'stop_token_fr': 67,\n",
       "  'history_tokens': ['recevoir_une',\n",
       "   'une_balle',\n",
       "   '__sent_marker_1',\n",
       "   'balle_l’avait'],\n",
       "  'span_tokens': ['l’avait_rendu',\n",
       "   'rendu_moins',\n",
       "   'moins_malheureux.',\n",
       "   'il_n’avait_pas',\n",
       "   'n’avait_pas_été',\n",
       "   'pas_été_aussi',\n",
       "   'été_aussi_agité'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 64,\n",
       "  'stop_token_fr': 70,\n",
       "  'history_tokens': ['l’avait_rendu',\n",
       "   'rendu_moins',\n",
       "   'moins_malheureux.',\n",
       "   'il_n’avait_pas'],\n",
       "  'span_tokens': ['n’avait_pas_été',\n",
       "   'pas_été_aussi',\n",
       "   'été_aussi_agité',\n",
       "   'aussi_agité_depuis',\n",
       "   'agité_depuis_un',\n",
       "   'depuis_un_duel'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 67,\n",
       "  'stop_token_fr': 73,\n",
       "  'history_tokens': ['il_n’avait_pas',\n",
       "   'n’avait_pas_été',\n",
       "   'pas_été_aussi',\n",
       "   'été_aussi_agité'],\n",
       "  'span_tokens': ['aussi_agité_depuis',\n",
       "   'agité_depuis_un',\n",
       "   'depuis_un_duel',\n",
       "   'un_duel_qu’il',\n",
       "   'duel_qu’il_avait',\n",
       "   'qu’il_avait_failli'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 70,\n",
       "  'stop_token_fr': 76,\n",
       "  'history_tokens': ['été_aussi_agité',\n",
       "   'aussi_agité_depuis',\n",
       "   'agité_depuis_un',\n",
       "   'depuis_un_duel'],\n",
       "  'span_tokens': ['un_duel_qu’il',\n",
       "   'duel_qu’il_avait',\n",
       "   'qu’il_avait_failli',\n",
       "   'avait_failli_avoir',\n",
       "   'failli_avoir_en',\n",
       "   'avoir_en_1816,'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 73,\n",
       "  'stop_token_fr': 80,\n",
       "  'history_tokens': ['depuis_un_duel',\n",
       "   'un_duel_qu’il',\n",
       "   'duel_qu’il_avait',\n",
       "   'qu’il_avait_failli'],\n",
       "  'span_tokens': ['avait_failli_avoir',\n",
       "   'failli_avoir_en',\n",
       "   'avoir_en_1816,',\n",
       "   'en_1816,_et,',\n",
       "   '1816,_et,_pour',\n",
       "   'et,_pour_lui',\n",
       "   'pour_lui_rendre'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 76,\n",
       "  'stop_token_fr': 83,\n",
       "  'history_tokens': ['qu’il_avait_failli',\n",
       "   'avait_failli_avoir',\n",
       "   'failli_avoir_en',\n",
       "   'avoir_en_1816,'],\n",
       "  'span_tokens': ['en_1816,_et,',\n",
       "   '1816,_et,_pour',\n",
       "   'et,_pour_lui',\n",
       "   'pour_lui_rendre',\n",
       "   'lui_rendre_justice,',\n",
       "   'rendre_justice,_alors',\n",
       "   'justice,_alors_la'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 80,\n",
       "  'stop_token_fr': 86,\n",
       "  'history_tokens': ['en_1816,_et,',\n",
       "   '1816,_et,_pour',\n",
       "   'et,_pour_lui',\n",
       "   'pour_lui_rendre'],\n",
       "  'span_tokens': ['lui_rendre_justice,',\n",
       "   'rendre_justice,_alors',\n",
       "   'justice,_alors_la',\n",
       "   'alors_la_perspective',\n",
       "   'la_perspective_de',\n",
       "   'perspective_de_recevoir'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 83,\n",
       "  'stop_token_fr': 89,\n",
       "  'history_tokens': ['pour_lui_rendre',\n",
       "   'lui_rendre_justice,',\n",
       "   'rendre_justice,_alors',\n",
       "   'justice,_alors_la'],\n",
       "  'span_tokens': ['alors_la_perspective',\n",
       "   'la_perspective_de',\n",
       "   'perspective_de_recevoir',\n",
       "   'de_recevoir_une',\n",
       "   '__sent_marker_2',\n",
       "   'recevoir_une_balle'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 86,\n",
       "  'stop_token_fr': 92,\n",
       "  'history_tokens': ['justice,_alors_la',\n",
       "   'alors_la_perspective',\n",
       "   'la_perspective_de',\n",
       "   'perspective_de_recevoir'],\n",
       "  'span_tokens': ['de_recevoir_une',\n",
       "   '__sent_marker_2',\n",
       "   'recevoir_une_balle',\n",
       "   'une_balle_l’avait',\n",
       "   'balle_l’avait_rendu',\n",
       "   'l’avait_rendu_moins'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 89,\n",
       "  'stop_token_fr': 96,\n",
       "  'history_tokens': ['perspective_de_recevoir',\n",
       "   'de_recevoir_une',\n",
       "   '__sent_marker_2',\n",
       "   'recevoir_une_balle'],\n",
       "  'span_tokens': ['une_balle_l’avait',\n",
       "   'balle_l’avait_rendu',\n",
       "   'l’avait_rendu_moins',\n",
       "   'rendu_moins_malheureux.',\n",
       "   'il_n’avait_pas_été',\n",
       "   'n’avait_pas_été_aussi',\n",
       "   'pas_été_aussi_agité'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 92,\n",
       "  'stop_token_fr': 99,\n",
       "  'history_tokens': ['recevoir_une_balle',\n",
       "   'une_balle_l’avait',\n",
       "   'balle_l’avait_rendu',\n",
       "   'l’avait_rendu_moins'],\n",
       "  'span_tokens': ['rendu_moins_malheureux.',\n",
       "   'il_n’avait_pas_été',\n",
       "   'n’avait_pas_été_aussi',\n",
       "   'pas_été_aussi_agité',\n",
       "   'été_aussi_agité_depuis',\n",
       "   'aussi_agité_depuis_un',\n",
       "   'agité_depuis_un_duel'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 120,\n",
       "  'stop': 128,\n",
       "  'start_token_fr': 96,\n",
       "  'stop_token_fr': 102,\n",
       "  'history_tokens': ['rendu_moins_malheureux.',\n",
       "   'il_n’avait_pas_été',\n",
       "   'n’avait_pas_été_aussi',\n",
       "   'pas_été_aussi_agité'],\n",
       "  'span_tokens': ['été_aussi_agité_depuis',\n",
       "   'aussi_agité_depuis_un',\n",
       "   'agité_depuis_un_duel',\n",
       "   'depuis_un_duel_qu’il',\n",
       "   'un_duel_qu’il_avait',\n",
       "   'duel_qu’il_avait_failli'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 124,\n",
       "  'stop': 132,\n",
       "  'start_token_fr': 99,\n",
       "  'stop_token_fr': 105,\n",
       "  'history_tokens': ['pas_été_aussi_agité',\n",
       "   'été_aussi_agité_depuis',\n",
       "   'aussi_agité_depuis_un',\n",
       "   'agité_depuis_un_duel'],\n",
       "  'span_tokens': ['depuis_un_duel_qu’il',\n",
       "   'un_duel_qu’il_avait',\n",
       "   'duel_qu’il_avait_failli',\n",
       "   'qu’il_avait_failli_avoir',\n",
       "   'avait_failli_avoir_en',\n",
       "   'failli_avoir_en_1816,'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 128,\n",
       "  'stop': 136,\n",
       "  'start_token_fr': 102,\n",
       "  'stop_token_fr': 108,\n",
       "  'history_tokens': ['agité_depuis_un_duel',\n",
       "   'depuis_un_duel_qu’il',\n",
       "   'un_duel_qu’il_avait',\n",
       "   'duel_qu’il_avait_failli'],\n",
       "  'span_tokens': ['qu’il_avait_failli_avoir',\n",
       "   'avait_failli_avoir_en',\n",
       "   'failli_avoir_en_1816,',\n",
       "   'avoir_en_1816,_et,',\n",
       "   'en_1816,_et,_pour',\n",
       "   '1816,_et,_pour_lui'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 132,\n",
       "  'stop': 140,\n",
       "  'start_token_fr': 105,\n",
       "  'stop_token_fr': 111,\n",
       "  'history_tokens': ['duel_qu’il_avait_failli',\n",
       "   'qu’il_avait_failli_avoir',\n",
       "   'avait_failli_avoir_en',\n",
       "   'failli_avoir_en_1816,'],\n",
       "  'span_tokens': ['avoir_en_1816,_et,',\n",
       "   'en_1816,_et,_pour',\n",
       "   '1816,_et,_pour_lui',\n",
       "   'et,_pour_lui_rendre',\n",
       "   'pour_lui_rendre_justice,',\n",
       "   'lui_rendre_justice,_alors'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 136,\n",
       "  'stop': 144,\n",
       "  'start_token_fr': 108,\n",
       "  'stop_token_fr': 115,\n",
       "  'history_tokens': ['failli_avoir_en_1816,',\n",
       "   'avoir_en_1816,_et,',\n",
       "   'en_1816,_et,_pour',\n",
       "   '1816,_et,_pour_lui'],\n",
       "  'span_tokens': ['et,_pour_lui_rendre',\n",
       "   'pour_lui_rendre_justice,',\n",
       "   'lui_rendre_justice,_alors',\n",
       "   'rendre_justice,_alors_la',\n",
       "   'justice,_alors_la_perspective',\n",
       "   'alors_la_perspective_de',\n",
       "   'la_perspective_de_recevoir'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 140,\n",
       "  'stop': 148,\n",
       "  'start_token_fr': 111,\n",
       "  'stop_token_fr': 118,\n",
       "  'history_tokens': ['1816,_et,_pour_lui',\n",
       "   'et,_pour_lui_rendre',\n",
       "   'pour_lui_rendre_justice,',\n",
       "   'lui_rendre_justice,_alors'],\n",
       "  'span_tokens': ['rendre_justice,_alors_la',\n",
       "   'justice,_alors_la_perspective',\n",
       "   'alors_la_perspective_de',\n",
       "   'la_perspective_de_recevoir',\n",
       "   'perspective_de_recevoir_une',\n",
       "   'de_recevoir_une_balle',\n",
       "   'recevoir_une_balle_l’avait'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 7,\n",
       "  'start': 144,\n",
       "  'stop': 152,\n",
       "  'start_token_fr': 115,\n",
       "  'stop_token_fr': 121,\n",
       "  'history_tokens': ['rendre_justice,_alors_la',\n",
       "   'justice,_alors_la_perspective',\n",
       "   'alors_la_perspective_de',\n",
       "   'la_perspective_de_recevoir'],\n",
       "  'span_tokens': ['perspective_de_recevoir_une',\n",
       "   'de_recevoir_une_balle',\n",
       "   'recevoir_une_balle_l’avait',\n",
       "   'une_balle_l’avait_rendu',\n",
       "   'balle_l’avait_rendu_moins',\n",
       "   'l’avait_rendu_moins_malheureux.'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 6,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['et', 'pourtant', 'je', 'me', 'mis', 'à'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 3,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': ['et', 'pourtant', 'je'],\n",
       "  'span_tokens': ['me', 'mis', 'à', 'réfléchir.', 'elle', 'avait', 'été'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 6,\n",
       "  'stop_token_fr': 13,\n",
       "  'history_tokens': ['je', 'me', 'mis', 'à'],\n",
       "  'span_tokens': ['réfléchir.', 'elle', 'avait', 'été', 'jeune,', 'et', 'sa'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 16,\n",
       "  'history_tokens': ['réfléchir.', 'elle', 'avait', 'été'],\n",
       "  'span_tokens': ['jeune,', 'et', 'sa', 'jeunesse', 'avait', 'dû'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 13,\n",
       "  'stop_token_fr': 19,\n",
       "  'history_tokens': ['été', 'jeune,', 'et', 'sa'],\n",
       "  'span_tokens': ['jeunesse', 'avait', 'dû', 'correspondre', 'à', 'celle'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 16,\n",
       "  'stop_token_fr': 23,\n",
       "  'history_tokens': ['sa', 'jeunesse', 'avait', 'dû'],\n",
       "  'span_tokens': ['correspondre',\n",
       "   'à',\n",
       "   'celle',\n",
       "   'de',\n",
       "   'm.',\n",
       "   'rochester;',\n",
       "   'mme'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 19,\n",
       "  'stop_token_fr': 26,\n",
       "  'history_tokens': ['dû', 'correspondre', 'à', 'celle'],\n",
       "  'span_tokens': ['de',\n",
       "   'm.',\n",
       "   'rochester;',\n",
       "   'mme',\n",
       "   'farfaix',\n",
       "   'disait',\n",
       "   \"qu'elle\"],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 23,\n",
       "  'stop_token_fr': 29,\n",
       "  'history_tokens': ['de', 'm.', 'rochester;', 'mme'],\n",
       "  'span_tokens': ['farfaix',\n",
       "   'disait',\n",
       "   \"qu'elle\",\n",
       "   'demeurait',\n",
       "   'depuis',\n",
       "   'longtemps'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 26,\n",
       "  'stop_token_fr': 32,\n",
       "  'history_tokens': ['mme', 'farfaix', 'disait', \"qu'elle\"],\n",
       "  'span_tokens': ['demeurait',\n",
       "   'depuis',\n",
       "   'longtemps',\n",
       "   'dans',\n",
       "   'le',\n",
       "   'château;'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 29,\n",
       "  'stop_token_fr': 36,\n",
       "  'history_tokens': [\"qu'elle\", 'demeurait', 'depuis', 'longtemps'],\n",
       "  'span_tokens': ['dans', 'le', 'château;', 'elle', \"n'avait\", 'jamais', 'dû'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 32,\n",
       "  'stop_token_fr': 39,\n",
       "  'history_tokens': ['longtemps', 'dans', 'le', 'château;'],\n",
       "  'span_tokens': ['elle', \"n'avait\", 'jamais', 'dû', 'être', 'jolie,', 'mais'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 36,\n",
       "  'stop_token_fr': 42,\n",
       "  'history_tokens': ['elle', \"n'avait\", 'jamais', 'dû'],\n",
       "  'span_tokens': ['être', 'jolie,', 'mais', 'peut-être', 'avait-elle', 'eu'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 39,\n",
       "  'stop_token_fr': 45,\n",
       "  'history_tokens': ['dû', 'être', 'jolie,', 'mais'],\n",
       "  'span_tokens': ['peut-être',\n",
       "   'avait-elle',\n",
       "   'eu',\n",
       "   'un',\n",
       "   'caractère',\n",
       "   'vigoureux'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 42,\n",
       "  'stop_token_fr': 49,\n",
       "  'history_tokens': ['mais', 'peut-être', 'avait-elle', 'eu'],\n",
       "  'span_tokens': ['un',\n",
       "   'caractère',\n",
       "   'vigoureux',\n",
       "   '__sent_marker_0',\n",
       "   'et',\n",
       "   'original.',\n",
       "   'et_pourtant'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 45,\n",
       "  'stop_token_fr': 52,\n",
       "  'history_tokens': ['eu', 'un', 'caractère', 'vigoureux'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'et',\n",
       "   'original.',\n",
       "   'et_pourtant',\n",
       "   'pourtant_je',\n",
       "   'je_me',\n",
       "   'me_mis'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 49,\n",
       "  'stop_token_fr': 55,\n",
       "  'history_tokens': ['__sent_marker_0', 'et', 'original.', 'et_pourtant'],\n",
       "  'span_tokens': ['pourtant_je',\n",
       "   'je_me',\n",
       "   'me_mis',\n",
       "   'mis_à',\n",
       "   'à_réfléchir.',\n",
       "   'réfléchir._elle'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 52,\n",
       "  'stop_token_fr': 58,\n",
       "  'history_tokens': ['et_pourtant', 'pourtant_je', 'je_me', 'me_mis'],\n",
       "  'span_tokens': ['mis_à',\n",
       "   'à_réfléchir.',\n",
       "   'réfléchir._elle',\n",
       "   'elle_avait',\n",
       "   'avait_été',\n",
       "   'été_jeune,'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 55,\n",
       "  'stop_token_fr': 62,\n",
       "  'history_tokens': ['me_mis', 'mis_à', 'à_réfléchir.', 'réfléchir._elle'],\n",
       "  'span_tokens': ['elle_avait',\n",
       "   'avait_été',\n",
       "   'été_jeune,',\n",
       "   'jeune,_et',\n",
       "   'et_sa',\n",
       "   'sa_jeunesse',\n",
       "   'jeunesse_avait'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 58,\n",
       "  'stop_token_fr': 65,\n",
       "  'history_tokens': ['réfléchir._elle',\n",
       "   'elle_avait',\n",
       "   'avait_été',\n",
       "   'été_jeune,'],\n",
       "  'span_tokens': ['jeune,_et',\n",
       "   'et_sa',\n",
       "   'sa_jeunesse',\n",
       "   'jeunesse_avait',\n",
       "   'avait_dû',\n",
       "   'dû_correspondre',\n",
       "   'correspondre_à'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 62,\n",
       "  'stop_token_fr': 68,\n",
       "  'history_tokens': ['jeune,_et', 'et_sa', 'sa_jeunesse', 'jeunesse_avait'],\n",
       "  'span_tokens': ['avait_dû',\n",
       "   'dû_correspondre',\n",
       "   'correspondre_à',\n",
       "   'à_celle',\n",
       "   'celle_de',\n",
       "   'de_m.'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 65,\n",
       "  'stop_token_fr': 71,\n",
       "  'history_tokens': ['jeunesse_avait',\n",
       "   'avait_dû',\n",
       "   'dû_correspondre',\n",
       "   'correspondre_à'],\n",
       "  'span_tokens': ['à_celle',\n",
       "   'celle_de',\n",
       "   'de_m.',\n",
       "   'm._rochester;',\n",
       "   'rochester;_mme',\n",
       "   'mme_farfaix'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 68,\n",
       "  'stop_token_fr': 75,\n",
       "  'history_tokens': ['correspondre_à', 'à_celle', 'celle_de', 'de_m.'],\n",
       "  'span_tokens': ['m._rochester;',\n",
       "   'rochester;_mme',\n",
       "   'mme_farfaix',\n",
       "   'farfaix_disait',\n",
       "   \"disait_qu'elle\",\n",
       "   \"qu'elle_demeurait\",\n",
       "   'demeurait_depuis'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 71,\n",
       "  'stop_token_fr': 78,\n",
       "  'history_tokens': ['de_m.',\n",
       "   'm._rochester;',\n",
       "   'rochester;_mme',\n",
       "   'mme_farfaix'],\n",
       "  'span_tokens': ['farfaix_disait',\n",
       "   \"disait_qu'elle\",\n",
       "   \"qu'elle_demeurait\",\n",
       "   'demeurait_depuis',\n",
       "   'depuis_longtemps',\n",
       "   'longtemps_dans',\n",
       "   'dans_le'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 75,\n",
       "  'stop_token_fr': 81,\n",
       "  'history_tokens': ['farfaix_disait',\n",
       "   \"disait_qu'elle\",\n",
       "   \"qu'elle_demeurait\",\n",
       "   'demeurait_depuis'],\n",
       "  'span_tokens': ['depuis_longtemps',\n",
       "   'longtemps_dans',\n",
       "   'dans_le',\n",
       "   'le_château;',\n",
       "   'château;_elle',\n",
       "   \"elle_n'avait\"],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 78,\n",
       "  'stop_token_fr': 84,\n",
       "  'history_tokens': ['demeurait_depuis',\n",
       "   'depuis_longtemps',\n",
       "   'longtemps_dans',\n",
       "   'dans_le'],\n",
       "  'span_tokens': ['le_château;',\n",
       "   'château;_elle',\n",
       "   \"elle_n'avait\",\n",
       "   \"n'avait_jamais\",\n",
       "   'jamais_dû',\n",
       "   'dû_être'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 81,\n",
       "  'stop_token_fr': 88,\n",
       "  'history_tokens': ['dans_le',\n",
       "   'le_château;',\n",
       "   'château;_elle',\n",
       "   \"elle_n'avait\"],\n",
       "  'span_tokens': [\"n'avait_jamais\",\n",
       "   'jamais_dû',\n",
       "   'dû_être',\n",
       "   'être_jolie,',\n",
       "   'jolie,_mais',\n",
       "   'mais_peut-être',\n",
       "   'peut-être_avait-elle'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 84,\n",
       "  'stop_token_fr': 91,\n",
       "  'history_tokens': [\"elle_n'avait\", \"n'avait_jamais\", 'jamais_dû', 'dû_être'],\n",
       "  'span_tokens': ['être_jolie,',\n",
       "   'jolie,_mais',\n",
       "   'mais_peut-être',\n",
       "   'peut-être_avait-elle',\n",
       "   'avait-elle_eu',\n",
       "   'eu_un',\n",
       "   '__sent_marker_1'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 88,\n",
       "  'stop_token_fr': 94,\n",
       "  'history_tokens': ['être_jolie,',\n",
       "   'jolie,_mais',\n",
       "   'mais_peut-être',\n",
       "   'peut-être_avait-elle'],\n",
       "  'span_tokens': ['avait-elle_eu',\n",
       "   'eu_un',\n",
       "   '__sent_marker_1',\n",
       "   'un_caractère',\n",
       "   'caractère_vigoureux',\n",
       "   'vigoureux_et'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 91,\n",
       "  'stop_token_fr': 97,\n",
       "  'history_tokens': ['peut-être_avait-elle',\n",
       "   'avait-elle_eu',\n",
       "   'eu_un',\n",
       "   '__sent_marker_1'],\n",
       "  'span_tokens': ['un_caractère',\n",
       "   'caractère_vigoureux',\n",
       "   'vigoureux_et',\n",
       "   'et_original.',\n",
       "   'et_pourtant_je',\n",
       "   'pourtant_je_me'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 94,\n",
       "  'stop_token_fr': 101,\n",
       "  'history_tokens': ['__sent_marker_1',\n",
       "   'un_caractère',\n",
       "   'caractère_vigoureux',\n",
       "   'vigoureux_et'],\n",
       "  'span_tokens': ['et_original.',\n",
       "   'et_pourtant_je',\n",
       "   'pourtant_je_me',\n",
       "   'je_me_mis',\n",
       "   'me_mis_à',\n",
       "   'mis_à_réfléchir.',\n",
       "   'à_réfléchir._elle'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 120,\n",
       "  'stop': 128,\n",
       "  'start_token_fr': 97,\n",
       "  'stop_token_fr': 104,\n",
       "  'history_tokens': ['vigoureux_et',\n",
       "   'et_original.',\n",
       "   'et_pourtant_je',\n",
       "   'pourtant_je_me'],\n",
       "  'span_tokens': ['je_me_mis',\n",
       "   'me_mis_à',\n",
       "   'mis_à_réfléchir.',\n",
       "   'à_réfléchir._elle',\n",
       "   'réfléchir._elle_avait',\n",
       "   'elle_avait_été',\n",
       "   'avait_été_jeune,'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 124,\n",
       "  'stop': 132,\n",
       "  'start_token_fr': 101,\n",
       "  'stop_token_fr': 107,\n",
       "  'history_tokens': ['je_me_mis',\n",
       "   'me_mis_à',\n",
       "   'mis_à_réfléchir.',\n",
       "   'à_réfléchir._elle'],\n",
       "  'span_tokens': ['réfléchir._elle_avait',\n",
       "   'elle_avait_été',\n",
       "   'avait_été_jeune,',\n",
       "   'été_jeune,_et',\n",
       "   'jeune,_et_sa',\n",
       "   'et_sa_jeunesse'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 128,\n",
       "  'stop': 136,\n",
       "  'start_token_fr': 104,\n",
       "  'stop_token_fr': 110,\n",
       "  'history_tokens': ['à_réfléchir._elle',\n",
       "   'réfléchir._elle_avait',\n",
       "   'elle_avait_été',\n",
       "   'avait_été_jeune,'],\n",
       "  'span_tokens': ['été_jeune,_et',\n",
       "   'jeune,_et_sa',\n",
       "   'et_sa_jeunesse',\n",
       "   'sa_jeunesse_avait',\n",
       "   'jeunesse_avait_dû',\n",
       "   'avait_dû_correspondre'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 132,\n",
       "  'stop': 140,\n",
       "  'start_token_fr': 107,\n",
       "  'stop_token_fr': 114,\n",
       "  'history_tokens': ['avait_été_jeune,',\n",
       "   'été_jeune,_et',\n",
       "   'jeune,_et_sa',\n",
       "   'et_sa_jeunesse'],\n",
       "  'span_tokens': ['sa_jeunesse_avait',\n",
       "   'jeunesse_avait_dû',\n",
       "   'avait_dû_correspondre',\n",
       "   'dû_correspondre_à',\n",
       "   'correspondre_à_celle',\n",
       "   'à_celle_de',\n",
       "   'celle_de_m.'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 136,\n",
       "  'stop': 144,\n",
       "  'start_token_fr': 110,\n",
       "  'stop_token_fr': 117,\n",
       "  'history_tokens': ['et_sa_jeunesse',\n",
       "   'sa_jeunesse_avait',\n",
       "   'jeunesse_avait_dû',\n",
       "   'avait_dû_correspondre'],\n",
       "  'span_tokens': ['dû_correspondre_à',\n",
       "   'correspondre_à_celle',\n",
       "   'à_celle_de',\n",
       "   'celle_de_m.',\n",
       "   'de_m._rochester;',\n",
       "   'm._rochester;_mme',\n",
       "   'rochester;_mme_farfaix'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 140,\n",
       "  'stop': 148,\n",
       "  'start_token_fr': 114,\n",
       "  'stop_token_fr': 120,\n",
       "  'history_tokens': ['dû_correspondre_à',\n",
       "   'correspondre_à_celle',\n",
       "   'à_celle_de',\n",
       "   'celle_de_m.'],\n",
       "  'span_tokens': ['de_m._rochester;',\n",
       "   'm._rochester;_mme',\n",
       "   'rochester;_mme_farfaix',\n",
       "   'mme_farfaix_disait',\n",
       "   \"farfaix_disait_qu'elle\",\n",
       "   \"disait_qu'elle_demeurait\"],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 144,\n",
       "  'stop': 152,\n",
       "  'start_token_fr': 117,\n",
       "  'stop_token_fr': 123,\n",
       "  'history_tokens': ['celle_de_m.',\n",
       "   'de_m._rochester;',\n",
       "   'm._rochester;_mme',\n",
       "   'rochester;_mme_farfaix'],\n",
       "  'span_tokens': ['mme_farfaix_disait',\n",
       "   \"farfaix_disait_qu'elle\",\n",
       "   \"disait_qu'elle_demeurait\",\n",
       "   \"qu'elle_demeurait_depuis\",\n",
       "   'demeurait_depuis_longtemps',\n",
       "   'depuis_longtemps_dans'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 148,\n",
       "  'stop': 156,\n",
       "  'start_token_fr': 120,\n",
       "  'stop_token_fr': 127,\n",
       "  'history_tokens': ['rochester;_mme_farfaix',\n",
       "   'mme_farfaix_disait',\n",
       "   \"farfaix_disait_qu'elle\",\n",
       "   \"disait_qu'elle_demeurait\"],\n",
       "  'span_tokens': [\"qu'elle_demeurait_depuis\",\n",
       "   'demeurait_depuis_longtemps',\n",
       "   'depuis_longtemps_dans',\n",
       "   'longtemps_dans_le',\n",
       "   'dans_le_château;',\n",
       "   'le_château;_elle',\n",
       "   \"château;_elle_n'avait\"],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 152,\n",
       "  'stop': 160,\n",
       "  'start_token_fr': 123,\n",
       "  'stop_token_fr': 130,\n",
       "  'history_tokens': [\"disait_qu'elle_demeurait\",\n",
       "   \"qu'elle_demeurait_depuis\",\n",
       "   'demeurait_depuis_longtemps',\n",
       "   'depuis_longtemps_dans'],\n",
       "  'span_tokens': ['longtemps_dans_le',\n",
       "   'dans_le_château;',\n",
       "   'le_château;_elle',\n",
       "   \"château;_elle_n'avait\",\n",
       "   \"elle_n'avait_jamais\",\n",
       "   \"n'avait_jamais_dû\",\n",
       "   'jamais_dû_être'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 156,\n",
       "  'stop': 164,\n",
       "  'start_token_fr': 127,\n",
       "  'stop_token_fr': 133,\n",
       "  'history_tokens': ['longtemps_dans_le',\n",
       "   'dans_le_château;',\n",
       "   'le_château;_elle',\n",
       "   \"château;_elle_n'avait\"],\n",
       "  'span_tokens': [\"elle_n'avait_jamais\",\n",
       "   \"n'avait_jamais_dû\",\n",
       "   'jamais_dû_être',\n",
       "   'dû_être_jolie,',\n",
       "   'être_jolie,_mais',\n",
       "   'jolie,_mais_peut-être'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 160,\n",
       "  'stop': 168,\n",
       "  'start_token_fr': 130,\n",
       "  'stop_token_fr': 136,\n",
       "  'history_tokens': [\"château;_elle_n'avait\",\n",
       "   \"elle_n'avait_jamais\",\n",
       "   \"n'avait_jamais_dû\",\n",
       "   'jamais_dû_être'],\n",
       "  'span_tokens': ['dû_être_jolie,',\n",
       "   'être_jolie,_mais',\n",
       "   'jolie,_mais_peut-être',\n",
       "   'mais_peut-être_avait-elle',\n",
       "   'peut-être_avait-elle_eu',\n",
       "   '__sent_marker_2'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 164,\n",
       "  'stop': 172,\n",
       "  'start_token_fr': 133,\n",
       "  'stop_token_fr': 140,\n",
       "  'history_tokens': ['jamais_dû_être',\n",
       "   'dû_être_jolie,',\n",
       "   'être_jolie,_mais',\n",
       "   'jolie,_mais_peut-être'],\n",
       "  'span_tokens': ['mais_peut-être_avait-elle',\n",
       "   'peut-être_avait-elle_eu',\n",
       "   '__sent_marker_2',\n",
       "   'avait-elle_eu_un',\n",
       "   'eu_un_caractère',\n",
       "   'un_caractère_vigoureux',\n",
       "   'caractère_vigoureux_et'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 168,\n",
       "  'stop': 176,\n",
       "  'start_token_fr': 136,\n",
       "  'stop_token_fr': 143,\n",
       "  'history_tokens': ['jolie,_mais_peut-être',\n",
       "   'mais_peut-être_avait-elle',\n",
       "   'peut-être_avait-elle_eu',\n",
       "   '__sent_marker_2'],\n",
       "  'span_tokens': ['avait-elle_eu_un',\n",
       "   'eu_un_caractère',\n",
       "   'un_caractère_vigoureux',\n",
       "   'caractère_vigoureux_et',\n",
       "   'vigoureux_et_original.',\n",
       "   'et_pourtant_je_me',\n",
       "   'pourtant_je_me_mis'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 172,\n",
       "  'stop': 180,\n",
       "  'start_token_fr': 140,\n",
       "  'stop_token_fr': 146,\n",
       "  'history_tokens': ['avait-elle_eu_un',\n",
       "   'eu_un_caractère',\n",
       "   'un_caractère_vigoureux',\n",
       "   'caractère_vigoureux_et'],\n",
       "  'span_tokens': ['vigoureux_et_original.',\n",
       "   'et_pourtant_je_me',\n",
       "   'pourtant_je_me_mis',\n",
       "   'je_me_mis_à',\n",
       "   'me_mis_à_réfléchir.',\n",
       "   'mis_à_réfléchir._elle'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 176,\n",
       "  'stop': 184,\n",
       "  'start_token_fr': 143,\n",
       "  'stop_token_fr': 149,\n",
       "  'history_tokens': ['caractère_vigoureux_et',\n",
       "   'vigoureux_et_original.',\n",
       "   'et_pourtant_je_me',\n",
       "   'pourtant_je_me_mis'],\n",
       "  'span_tokens': ['je_me_mis_à',\n",
       "   'me_mis_à_réfléchir.',\n",
       "   'mis_à_réfléchir._elle',\n",
       "   'à_réfléchir._elle_avait',\n",
       "   'réfléchir._elle_avait_été',\n",
       "   'elle_avait_été_jeune,'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 180,\n",
       "  'stop': 188,\n",
       "  'start_token_fr': 146,\n",
       "  'stop_token_fr': 153,\n",
       "  'history_tokens': ['pourtant_je_me_mis',\n",
       "   'je_me_mis_à',\n",
       "   'me_mis_à_réfléchir.',\n",
       "   'mis_à_réfléchir._elle'],\n",
       "  'span_tokens': ['à_réfléchir._elle_avait',\n",
       "   'réfléchir._elle_avait_été',\n",
       "   'elle_avait_été_jeune,',\n",
       "   'avait_été_jeune,_et',\n",
       "   'été_jeune,_et_sa',\n",
       "   'jeune,_et_sa_jeunesse',\n",
       "   'et_sa_jeunesse_avait'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 184,\n",
       "  'stop': 192,\n",
       "  'start_token_fr': 149,\n",
       "  'stop_token_fr': 156,\n",
       "  'history_tokens': ['mis_à_réfléchir._elle',\n",
       "   'à_réfléchir._elle_avait',\n",
       "   'réfléchir._elle_avait_été',\n",
       "   'elle_avait_été_jeune,'],\n",
       "  'span_tokens': ['avait_été_jeune,_et',\n",
       "   'été_jeune,_et_sa',\n",
       "   'jeune,_et_sa_jeunesse',\n",
       "   'et_sa_jeunesse_avait',\n",
       "   'sa_jeunesse_avait_dû',\n",
       "   'jeunesse_avait_dû_correspondre',\n",
       "   'avait_dû_correspondre_à'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 188,\n",
       "  'stop': 196,\n",
       "  'start_token_fr': 153,\n",
       "  'stop_token_fr': 159,\n",
       "  'history_tokens': ['avait_été_jeune,_et',\n",
       "   'été_jeune,_et_sa',\n",
       "   'jeune,_et_sa_jeunesse',\n",
       "   'et_sa_jeunesse_avait'],\n",
       "  'span_tokens': ['sa_jeunesse_avait_dû',\n",
       "   'jeunesse_avait_dû_correspondre',\n",
       "   'avait_dû_correspondre_à',\n",
       "   'dû_correspondre_à_celle',\n",
       "   'correspondre_à_celle_de',\n",
       "   'à_celle_de_m.'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 192,\n",
       "  'stop': 200,\n",
       "  'start_token_fr': 156,\n",
       "  'stop_token_fr': 162,\n",
       "  'history_tokens': ['et_sa_jeunesse_avait',\n",
       "   'sa_jeunesse_avait_dû',\n",
       "   'jeunesse_avait_dû_correspondre',\n",
       "   'avait_dû_correspondre_à'],\n",
       "  'span_tokens': ['dû_correspondre_à_celle',\n",
       "   'correspondre_à_celle_de',\n",
       "   'à_celle_de_m.',\n",
       "   'celle_de_m._rochester;',\n",
       "   'de_m._rochester;_mme',\n",
       "   'm._rochester;_mme_farfaix'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 196,\n",
       "  'stop': 204,\n",
       "  'start_token_fr': 159,\n",
       "  'stop_token_fr': 166,\n",
       "  'history_tokens': ['avait_dû_correspondre_à',\n",
       "   'dû_correspondre_à_celle',\n",
       "   'correspondre_à_celle_de',\n",
       "   'à_celle_de_m.'],\n",
       "  'span_tokens': ['celle_de_m._rochester;',\n",
       "   'de_m._rochester;_mme',\n",
       "   'm._rochester;_mme_farfaix',\n",
       "   'rochester;_mme_farfaix_disait',\n",
       "   \"mme_farfaix_disait_qu'elle\",\n",
       "   \"farfaix_disait_qu'elle_demeurait\",\n",
       "   \"disait_qu'elle_demeurait_depuis\"],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 200,\n",
       "  'stop': 208,\n",
       "  'start_token_fr': 162,\n",
       "  'stop_token_fr': 169,\n",
       "  'history_tokens': ['à_celle_de_m.',\n",
       "   'celle_de_m._rochester;',\n",
       "   'de_m._rochester;_mme',\n",
       "   'm._rochester;_mme_farfaix'],\n",
       "  'span_tokens': ['rochester;_mme_farfaix_disait',\n",
       "   \"mme_farfaix_disait_qu'elle\",\n",
       "   \"farfaix_disait_qu'elle_demeurait\",\n",
       "   \"disait_qu'elle_demeurait_depuis\",\n",
       "   \"qu'elle_demeurait_depuis_longtemps\",\n",
       "   'demeurait_depuis_longtemps_dans',\n",
       "   'depuis_longtemps_dans_le'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 204,\n",
       "  'stop': 212,\n",
       "  'start_token_fr': 166,\n",
       "  'stop_token_fr': 172,\n",
       "  'history_tokens': ['rochester;_mme_farfaix_disait',\n",
       "   \"mme_farfaix_disait_qu'elle\",\n",
       "   \"farfaix_disait_qu'elle_demeurait\",\n",
       "   \"disait_qu'elle_demeurait_depuis\"],\n",
       "  'span_tokens': [\"qu'elle_demeurait_depuis_longtemps\",\n",
       "   'demeurait_depuis_longtemps_dans',\n",
       "   'depuis_longtemps_dans_le',\n",
       "   'longtemps_dans_le_château;',\n",
       "   'dans_le_château;_elle',\n",
       "   \"le_château;_elle_n'avait\"],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 208,\n",
       "  'stop': 216,\n",
       "  'start_token_fr': 169,\n",
       "  'stop_token_fr': 175,\n",
       "  'history_tokens': [\"disait_qu'elle_demeurait_depuis\",\n",
       "   \"qu'elle_demeurait_depuis_longtemps\",\n",
       "   'demeurait_depuis_longtemps_dans',\n",
       "   'depuis_longtemps_dans_le'],\n",
       "  'span_tokens': ['longtemps_dans_le_château;',\n",
       "   'dans_le_château;_elle',\n",
       "   \"le_château;_elle_n'avait\",\n",
       "   \"château;_elle_n'avait_jamais\",\n",
       "   \"elle_n'avait_jamais_dû\",\n",
       "   \"n'avait_jamais_dû_être\"],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 212,\n",
       "  'stop': 220,\n",
       "  'start_token_fr': 172,\n",
       "  'stop_token_fr': 179,\n",
       "  'history_tokens': ['depuis_longtemps_dans_le',\n",
       "   'longtemps_dans_le_château;',\n",
       "   'dans_le_château;_elle',\n",
       "   \"le_château;_elle_n'avait\"],\n",
       "  'span_tokens': [\"château;_elle_n'avait_jamais\",\n",
       "   \"elle_n'avait_jamais_dû\",\n",
       "   \"n'avait_jamais_dû_être\",\n",
       "   'jamais_dû_être_jolie,',\n",
       "   'dû_être_jolie,_mais',\n",
       "   'être_jolie,_mais_peut-être',\n",
       "   'jolie,_mais_peut-être_avait-elle'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 216,\n",
       "  'stop': 224,\n",
       "  'start_token_fr': 175,\n",
       "  'stop_token_fr': 182,\n",
       "  'history_tokens': [\"le_château;_elle_n'avait\",\n",
       "   \"château;_elle_n'avait_jamais\",\n",
       "   \"elle_n'avait_jamais_dû\",\n",
       "   \"n'avait_jamais_dû_être\"],\n",
       "  'span_tokens': ['jamais_dû_être_jolie,',\n",
       "   'dû_être_jolie,_mais',\n",
       "   'être_jolie,_mais_peut-être',\n",
       "   'jolie,_mais_peut-être_avait-elle',\n",
       "   'mais_peut-être_avait-elle_eu',\n",
       "   'peut-être_avait-elle_eu_un',\n",
       "   'avait-elle_eu_un_caractère'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 8,\n",
       "  'start': 220,\n",
       "  'stop': 228,\n",
       "  'start_token_fr': 179,\n",
       "  'stop_token_fr': 185,\n",
       "  'history_tokens': ['jamais_dû_être_jolie,',\n",
       "   'dû_être_jolie,_mais',\n",
       "   'être_jolie,_mais_peut-être',\n",
       "   'jolie,_mais_peut-être_avait-elle'],\n",
       "  'span_tokens': ['mais_peut-être_avait-elle_eu',\n",
       "   'peut-être_avait-elle_eu_un',\n",
       "   'avait-elle_eu_un_caractère',\n",
       "   'eu_un_caractère_vigoureux',\n",
       "   'un_caractère_vigoureux_et',\n",
       "   'caractère_vigoureux_et_original.'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 9,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 17,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['quelle',\n",
       "   'que',\n",
       "   'soit',\n",
       "   'la',\n",
       "   'personne',\n",
       "   'qui',\n",
       "   'est',\n",
       "   'venue,',\n",
       "   'elle',\n",
       "   'est',\n",
       "   'arrivée',\n",
       "   'par',\n",
       "   'le',\n",
       "   'dehors',\n",
       "   'et',\n",
       "   'a',\n",
       "   'frappé'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 9,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 9,\n",
       "  'stop_token_fr': 26,\n",
       "  'history_tokens': ['qui', 'est', 'venue,', 'elle'],\n",
       "  'span_tokens': ['est',\n",
       "   'arrivée',\n",
       "   'par',\n",
       "   'le',\n",
       "   'dehors',\n",
       "   'et',\n",
       "   'a',\n",
       "   'frappé',\n",
       "   'à',\n",
       "   '__sent_marker_0',\n",
       "   'la',\n",
       "   'fenêtre.',\n",
       "   'quelle_que',\n",
       "   'que_soit',\n",
       "   'soit_la',\n",
       "   'la_personne',\n",
       "   'personne_qui'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 9,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 17,\n",
       "  'stop_token_fr': 34,\n",
       "  'history_tokens': ['dehors', 'et', 'a', 'frappé'],\n",
       "  'span_tokens': ['à',\n",
       "   '__sent_marker_0',\n",
       "   'la',\n",
       "   'fenêtre.',\n",
       "   'quelle_que',\n",
       "   'que_soit',\n",
       "   'soit_la',\n",
       "   'la_personne',\n",
       "   'personne_qui',\n",
       "   'qui_est',\n",
       "   'est_venue,',\n",
       "   'venue,_elle',\n",
       "   'elle_est',\n",
       "   'est_arrivée',\n",
       "   'arrivée_par',\n",
       "   'par_le',\n",
       "   'le_dehors'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 9,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 26,\n",
       "  'stop_token_fr': 43,\n",
       "  'history_tokens': ['que_soit', 'soit_la', 'la_personne', 'personne_qui'],\n",
       "  'span_tokens': ['qui_est',\n",
       "   'est_venue,',\n",
       "   'venue,_elle',\n",
       "   'elle_est',\n",
       "   'est_arrivée',\n",
       "   'arrivée_par',\n",
       "   'par_le',\n",
       "   'le_dehors',\n",
       "   'dehors_et',\n",
       "   'et_a',\n",
       "   '__sent_marker_1',\n",
       "   'a_frappé',\n",
       "   'frappé_à',\n",
       "   'à_la',\n",
       "   'la_fenêtre.',\n",
       "   'quelle_que_soit',\n",
       "   'que_soit_la'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 9,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 34,\n",
       "  'stop_token_fr': 51,\n",
       "  'history_tokens': ['est_arrivée', 'arrivée_par', 'par_le', 'le_dehors'],\n",
       "  'span_tokens': ['dehors_et',\n",
       "   'et_a',\n",
       "   '__sent_marker_1',\n",
       "   'a_frappé',\n",
       "   'frappé_à',\n",
       "   'à_la',\n",
       "   'la_fenêtre.',\n",
       "   'quelle_que_soit',\n",
       "   'que_soit_la',\n",
       "   'soit_la_personne',\n",
       "   'la_personne_qui',\n",
       "   'personne_qui_est',\n",
       "   'qui_est_venue,',\n",
       "   'est_venue,_elle',\n",
       "   'venue,_elle_est',\n",
       "   'elle_est_arrivée',\n",
       "   'est_arrivée_par'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 9,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 43,\n",
       "  'stop_token_fr': 60,\n",
       "  'history_tokens': ['à_la', 'la_fenêtre.', 'quelle_que_soit', 'que_soit_la'],\n",
       "  'span_tokens': ['soit_la_personne',\n",
       "   'la_personne_qui',\n",
       "   'personne_qui_est',\n",
       "   'qui_est_venue,',\n",
       "   'est_venue,_elle',\n",
       "   'venue,_elle_est',\n",
       "   'elle_est_arrivée',\n",
       "   'est_arrivée_par',\n",
       "   'arrivée_par_le',\n",
       "   'par_le_dehors',\n",
       "   'le_dehors_et',\n",
       "   '__sent_marker_2',\n",
       "   'dehors_et_a',\n",
       "   'et_a_frappé',\n",
       "   'a_frappé_à',\n",
       "   'frappé_à_la',\n",
       "   'à_la_fenêtre.'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 9,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 51,\n",
       "  'stop_token_fr': 68,\n",
       "  'history_tokens': ['est_venue,_elle',\n",
       "   'venue,_elle_est',\n",
       "   'elle_est_arrivée',\n",
       "   'est_arrivée_par'],\n",
       "  'span_tokens': ['arrivée_par_le',\n",
       "   'par_le_dehors',\n",
       "   'le_dehors_et',\n",
       "   '__sent_marker_2',\n",
       "   'dehors_et_a',\n",
       "   'et_a_frappé',\n",
       "   'a_frappé_à',\n",
       "   'frappé_à_la',\n",
       "   'à_la_fenêtre.',\n",
       "   'quelle_que_soit_la',\n",
       "   'que_soit_la_personne',\n",
       "   'soit_la_personne_qui',\n",
       "   'la_personne_qui_est',\n",
       "   'personne_qui_est_venue,',\n",
       "   'qui_est_venue,_elle',\n",
       "   'est_venue,_elle_est',\n",
       "   'venue,_elle_est_arrivée'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 9,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 60,\n",
       "  'stop_token_fr': 77,\n",
       "  'history_tokens': ['et_a_frappé',\n",
       "   'a_frappé_à',\n",
       "   'frappé_à_la',\n",
       "   'à_la_fenêtre.'],\n",
       "  'span_tokens': ['quelle_que_soit_la',\n",
       "   'que_soit_la_personne',\n",
       "   'soit_la_personne_qui',\n",
       "   'la_personne_qui_est',\n",
       "   'personne_qui_est_venue,',\n",
       "   'qui_est_venue,_elle',\n",
       "   'est_venue,_elle_est',\n",
       "   'venue,_elle_est_arrivée',\n",
       "   'elle_est_arrivée_par',\n",
       "   'est_arrivée_par_le',\n",
       "   'arrivée_par_le_dehors',\n",
       "   'par_le_dehors_et',\n",
       "   'le_dehors_et_a',\n",
       "   'dehors_et_a_frappé',\n",
       "   'et_a_frappé_à',\n",
       "   'a_frappé_à_la',\n",
       "   'frappé_à_la_fenêtre.'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 8,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['the',\n",
       "   'marchesa',\n",
       "   'came',\n",
       "   'to',\n",
       "   'attend',\n",
       "   'to',\n",
       "   'her',\n",
       "   'son,'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 12,\n",
       "  'history_tokens': ['the', 'marchesa', 'came', 'to'],\n",
       "  'span_tokens': ['attend',\n",
       "   'to',\n",
       "   'her',\n",
       "   'son,',\n",
       "   'and',\n",
       "   'sometimes',\n",
       "   'fabrizio',\n",
       "   'was'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 8,\n",
       "  'stop_token_fr': 16,\n",
       "  'history_tokens': ['attend', 'to', 'her', 'son,'],\n",
       "  'span_tokens': ['and',\n",
       "   'sometimes',\n",
       "   'fabrizio',\n",
       "   'was',\n",
       "   'obliged',\n",
       "   'to',\n",
       "   'see',\n",
       "   'her'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 12,\n",
       "  'stop_token_fr': 20,\n",
       "  'history_tokens': ['and', 'sometimes', 'fabrizio', 'was'],\n",
       "  'span_tokens': ['obliged',\n",
       "   'to',\n",
       "   'see',\n",
       "   'her',\n",
       "   'by',\n",
       "   'candle-light,',\n",
       "   'which',\n",
       "   'seemed'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 16,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['obliged', 'to', 'see', 'her'],\n",
       "  'span_tokens': ['by',\n",
       "   'candle-light,',\n",
       "   'which',\n",
       "   'seemed',\n",
       "   'to',\n",
       "   'the',\n",
       "   'poor',\n",
       "   'sick'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 20,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['by', 'candle-light,', 'which', 'seemed'],\n",
       "  'span_tokens': ['to', 'the', 'poor', 'sick', 'heart', 'of', 'clelia'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 24,\n",
       "  'stop_token_fr': 31,\n",
       "  'history_tokens': ['to', 'the', 'poor', 'sick'],\n",
       "  'span_tokens': ['heart', 'of', 'clelia', 'a', 'horrible', 'sin', 'and'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 35,\n",
       "  'history_tokens': ['sick', 'heart', 'of', 'clelia'],\n",
       "  'span_tokens': ['a',\n",
       "   'horrible',\n",
       "   'sin',\n",
       "   'and',\n",
       "   'one',\n",
       "   'that',\n",
       "   'foreboded',\n",
       "   'the'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 31,\n",
       "  'stop_token_fr': 39,\n",
       "  'history_tokens': ['a', 'horrible', 'sin', 'and'],\n",
       "  'span_tokens': ['one',\n",
       "   'that',\n",
       "   'foreboded',\n",
       "   'the',\n",
       "   'death',\n",
       "   '__sent_marker_0',\n",
       "   'of',\n",
       "   'sandrino.'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 35,\n",
       "  'stop_token_fr': 43,\n",
       "  'history_tokens': ['one', 'that', 'foreboded', 'the'],\n",
       "  'span_tokens': ['death',\n",
       "   '__sent_marker_0',\n",
       "   'of',\n",
       "   'sandrino.',\n",
       "   'the_marchesa',\n",
       "   'marchesa_came',\n",
       "   'came_to',\n",
       "   'to_attend'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 39,\n",
       "  'stop_token_fr': 47,\n",
       "  'history_tokens': ['death', '__sent_marker_0', 'of', 'sandrino.'],\n",
       "  'span_tokens': ['the_marchesa',\n",
       "   'marchesa_came',\n",
       "   'came_to',\n",
       "   'to_attend',\n",
       "   'attend_to',\n",
       "   'to_her',\n",
       "   'her_son,',\n",
       "   'son,_and'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 43,\n",
       "  'stop_token_fr': 51,\n",
       "  'history_tokens': ['the_marchesa', 'marchesa_came', 'came_to', 'to_attend'],\n",
       "  'span_tokens': ['attend_to',\n",
       "   'to_her',\n",
       "   'her_son,',\n",
       "   'son,_and',\n",
       "   'and_sometimes',\n",
       "   'sometimes_fabrizio',\n",
       "   'fabrizio_was',\n",
       "   'was_obliged'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 47,\n",
       "  'stop_token_fr': 55,\n",
       "  'history_tokens': ['attend_to', 'to_her', 'her_son,', 'son,_and'],\n",
       "  'span_tokens': ['and_sometimes',\n",
       "   'sometimes_fabrizio',\n",
       "   'fabrizio_was',\n",
       "   'was_obliged',\n",
       "   'obliged_to',\n",
       "   'to_see',\n",
       "   'see_her',\n",
       "   'her_by'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 51,\n",
       "  'stop_token_fr': 59,\n",
       "  'history_tokens': ['and_sometimes',\n",
       "   'sometimes_fabrizio',\n",
       "   'fabrizio_was',\n",
       "   'was_obliged'],\n",
       "  'span_tokens': ['obliged_to',\n",
       "   'to_see',\n",
       "   'see_her',\n",
       "   'her_by',\n",
       "   'by_candle-light,',\n",
       "   'candle-light,_which',\n",
       "   'which_seemed',\n",
       "   'seemed_to'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 55,\n",
       "  'stop_token_fr': 63,\n",
       "  'history_tokens': ['obliged_to', 'to_see', 'see_her', 'her_by'],\n",
       "  'span_tokens': ['by_candle-light,',\n",
       "   'candle-light,_which',\n",
       "   'which_seemed',\n",
       "   'seemed_to',\n",
       "   'to_the',\n",
       "   'the_poor',\n",
       "   'poor_sick',\n",
       "   'sick_heart'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 59,\n",
       "  'stop_token_fr': 67,\n",
       "  'history_tokens': ['by_candle-light,',\n",
       "   'candle-light,_which',\n",
       "   'which_seemed',\n",
       "   'seemed_to'],\n",
       "  'span_tokens': ['to_the',\n",
       "   'the_poor',\n",
       "   'poor_sick',\n",
       "   'sick_heart',\n",
       "   'heart_of',\n",
       "   'of_clelia',\n",
       "   'clelia_a',\n",
       "   'a_horrible'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 63,\n",
       "  'stop_token_fr': 71,\n",
       "  'history_tokens': ['to_the', 'the_poor', 'poor_sick', 'sick_heart'],\n",
       "  'span_tokens': ['heart_of',\n",
       "   'of_clelia',\n",
       "   'clelia_a',\n",
       "   'a_horrible',\n",
       "   'horrible_sin',\n",
       "   'sin_and',\n",
       "   'and_one',\n",
       "   'one_that'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 67,\n",
       "  'stop_token_fr': 74,\n",
       "  'history_tokens': ['heart_of', 'of_clelia', 'clelia_a', 'a_horrible'],\n",
       "  'span_tokens': ['horrible_sin',\n",
       "   'sin_and',\n",
       "   'and_one',\n",
       "   'one_that',\n",
       "   'that_foreboded',\n",
       "   '__sent_marker_1',\n",
       "   'foreboded_the'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 71,\n",
       "  'stop_token_fr': 78,\n",
       "  'history_tokens': ['horrible_sin', 'sin_and', 'and_one', 'one_that'],\n",
       "  'span_tokens': ['that_foreboded',\n",
       "   '__sent_marker_1',\n",
       "   'foreboded_the',\n",
       "   'the_death',\n",
       "   'death_of',\n",
       "   'of_sandrino.',\n",
       "   'the_marchesa_came'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 74,\n",
       "  'stop_token_fr': 82,\n",
       "  'history_tokens': ['one_that',\n",
       "   'that_foreboded',\n",
       "   '__sent_marker_1',\n",
       "   'foreboded_the'],\n",
       "  'span_tokens': ['the_death',\n",
       "   'death_of',\n",
       "   'of_sandrino.',\n",
       "   'the_marchesa_came',\n",
       "   'marchesa_came_to',\n",
       "   'came_to_attend',\n",
       "   'to_attend_to',\n",
       "   'attend_to_her'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 78,\n",
       "  'stop_token_fr': 86,\n",
       "  'history_tokens': ['the_death',\n",
       "   'death_of',\n",
       "   'of_sandrino.',\n",
       "   'the_marchesa_came'],\n",
       "  'span_tokens': ['marchesa_came_to',\n",
       "   'came_to_attend',\n",
       "   'to_attend_to',\n",
       "   'attend_to_her',\n",
       "   'to_her_son,',\n",
       "   'her_son,_and',\n",
       "   'son,_and_sometimes',\n",
       "   'and_sometimes_fabrizio'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 82,\n",
       "  'stop_token_fr': 90,\n",
       "  'history_tokens': ['marchesa_came_to',\n",
       "   'came_to_attend',\n",
       "   'to_attend_to',\n",
       "   'attend_to_her'],\n",
       "  'span_tokens': ['to_her_son,',\n",
       "   'her_son,_and',\n",
       "   'son,_and_sometimes',\n",
       "   'and_sometimes_fabrizio',\n",
       "   'sometimes_fabrizio_was',\n",
       "   'fabrizio_was_obliged',\n",
       "   'was_obliged_to',\n",
       "   'obliged_to_see'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 86,\n",
       "  'stop_token_fr': 94,\n",
       "  'history_tokens': ['to_her_son,',\n",
       "   'her_son,_and',\n",
       "   'son,_and_sometimes',\n",
       "   'and_sometimes_fabrizio'],\n",
       "  'span_tokens': ['sometimes_fabrizio_was',\n",
       "   'fabrizio_was_obliged',\n",
       "   'was_obliged_to',\n",
       "   'obliged_to_see',\n",
       "   'to_see_her',\n",
       "   'see_her_by',\n",
       "   'her_by_candle-light,',\n",
       "   'by_candle-light,_which'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 90,\n",
       "  'stop_token_fr': 98,\n",
       "  'history_tokens': ['sometimes_fabrizio_was',\n",
       "   'fabrizio_was_obliged',\n",
       "   'was_obliged_to',\n",
       "   'obliged_to_see'],\n",
       "  'span_tokens': ['to_see_her',\n",
       "   'see_her_by',\n",
       "   'her_by_candle-light,',\n",
       "   'by_candle-light,_which',\n",
       "   'candle-light,_which_seemed',\n",
       "   'which_seemed_to',\n",
       "   'seemed_to_the',\n",
       "   'to_the_poor'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 94,\n",
       "  'stop_token_fr': 102,\n",
       "  'history_tokens': ['to_see_her',\n",
       "   'see_her_by',\n",
       "   'her_by_candle-light,',\n",
       "   'by_candle-light,_which'],\n",
       "  'span_tokens': ['candle-light,_which_seemed',\n",
       "   'which_seemed_to',\n",
       "   'seemed_to_the',\n",
       "   'to_the_poor',\n",
       "   'the_poor_sick',\n",
       "   'poor_sick_heart',\n",
       "   'sick_heart_of',\n",
       "   'heart_of_clelia'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 98,\n",
       "  'stop_token_fr': 106,\n",
       "  'history_tokens': ['candle-light,_which_seemed',\n",
       "   'which_seemed_to',\n",
       "   'seemed_to_the',\n",
       "   'to_the_poor'],\n",
       "  'span_tokens': ['the_poor_sick',\n",
       "   'poor_sick_heart',\n",
       "   'sick_heart_of',\n",
       "   'heart_of_clelia',\n",
       "   'of_clelia_a',\n",
       "   'clelia_a_horrible',\n",
       "   'a_horrible_sin',\n",
       "   'horrible_sin_and'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 102,\n",
       "  'stop_token_fr': 110,\n",
       "  'history_tokens': ['the_poor_sick',\n",
       "   'poor_sick_heart',\n",
       "   'sick_heart_of',\n",
       "   'heart_of_clelia'],\n",
       "  'span_tokens': ['of_clelia_a',\n",
       "   'clelia_a_horrible',\n",
       "   'a_horrible_sin',\n",
       "   'horrible_sin_and',\n",
       "   'sin_and_one',\n",
       "   'and_one_that',\n",
       "   '__sent_marker_2',\n",
       "   'one_that_foreboded'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 106,\n",
       "  'stop_token_fr': 114,\n",
       "  'history_tokens': ['of_clelia_a',\n",
       "   'clelia_a_horrible',\n",
       "   'a_horrible_sin',\n",
       "   'horrible_sin_and'],\n",
       "  'span_tokens': ['sin_and_one',\n",
       "   'and_one_that',\n",
       "   '__sent_marker_2',\n",
       "   'one_that_foreboded',\n",
       "   'that_foreboded_the',\n",
       "   'foreboded_the_death',\n",
       "   'the_death_of',\n",
       "   'death_of_sandrino.'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 110,\n",
       "  'stop_token_fr': 118,\n",
       "  'history_tokens': ['sin_and_one',\n",
       "   'and_one_that',\n",
       "   '__sent_marker_2',\n",
       "   'one_that_foreboded'],\n",
       "  'span_tokens': ['that_foreboded_the',\n",
       "   'foreboded_the_death',\n",
       "   'the_death_of',\n",
       "   'death_of_sandrino.',\n",
       "   'the_marchesa_came_to',\n",
       "   'marchesa_came_to_attend',\n",
       "   'came_to_attend_to',\n",
       "   'to_attend_to_her'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 114,\n",
       "  'stop_token_fr': 122,\n",
       "  'history_tokens': ['that_foreboded_the',\n",
       "   'foreboded_the_death',\n",
       "   'the_death_of',\n",
       "   'death_of_sandrino.'],\n",
       "  'span_tokens': ['the_marchesa_came_to',\n",
       "   'marchesa_came_to_attend',\n",
       "   'came_to_attend_to',\n",
       "   'to_attend_to_her',\n",
       "   'attend_to_her_son,',\n",
       "   'to_her_son,_and',\n",
       "   'her_son,_and_sometimes',\n",
       "   'son,_and_sometimes_fabrizio'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 120,\n",
       "  'stop': 128,\n",
       "  'start_token_fr': 118,\n",
       "  'stop_token_fr': 125,\n",
       "  'history_tokens': ['the_marchesa_came_to',\n",
       "   'marchesa_came_to_attend',\n",
       "   'came_to_attend_to',\n",
       "   'to_attend_to_her'],\n",
       "  'span_tokens': ['attend_to_her_son,',\n",
       "   'to_her_son,_and',\n",
       "   'her_son,_and_sometimes',\n",
       "   'son,_and_sometimes_fabrizio',\n",
       "   'and_sometimes_fabrizio_was',\n",
       "   'sometimes_fabrizio_was_obliged',\n",
       "   'fabrizio_was_obliged_to'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 124,\n",
       "  'stop': 132,\n",
       "  'start_token_fr': 122,\n",
       "  'stop_token_fr': 129,\n",
       "  'history_tokens': ['attend_to_her_son,',\n",
       "   'to_her_son,_and',\n",
       "   'her_son,_and_sometimes',\n",
       "   'son,_and_sometimes_fabrizio'],\n",
       "  'span_tokens': ['and_sometimes_fabrizio_was',\n",
       "   'sometimes_fabrizio_was_obliged',\n",
       "   'fabrizio_was_obliged_to',\n",
       "   'was_obliged_to_see',\n",
       "   'obliged_to_see_her',\n",
       "   'to_see_her_by',\n",
       "   'see_her_by_candle-light,'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 128,\n",
       "  'stop': 136,\n",
       "  'start_token_fr': 125,\n",
       "  'stop_token_fr': 133,\n",
       "  'history_tokens': ['son,_and_sometimes_fabrizio',\n",
       "   'and_sometimes_fabrizio_was',\n",
       "   'sometimes_fabrizio_was_obliged',\n",
       "   'fabrizio_was_obliged_to'],\n",
       "  'span_tokens': ['was_obliged_to_see',\n",
       "   'obliged_to_see_her',\n",
       "   'to_see_her_by',\n",
       "   'see_her_by_candle-light,',\n",
       "   'her_by_candle-light,_which',\n",
       "   'by_candle-light,_which_seemed',\n",
       "   'candle-light,_which_seemed_to',\n",
       "   'which_seemed_to_the'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 132,\n",
       "  'stop': 140,\n",
       "  'start_token_fr': 129,\n",
       "  'stop_token_fr': 137,\n",
       "  'history_tokens': ['was_obliged_to_see',\n",
       "   'obliged_to_see_her',\n",
       "   'to_see_her_by',\n",
       "   'see_her_by_candle-light,'],\n",
       "  'span_tokens': ['her_by_candle-light,_which',\n",
       "   'by_candle-light,_which_seemed',\n",
       "   'candle-light,_which_seemed_to',\n",
       "   'which_seemed_to_the',\n",
       "   'seemed_to_the_poor',\n",
       "   'to_the_poor_sick',\n",
       "   'the_poor_sick_heart',\n",
       "   'poor_sick_heart_of'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 136,\n",
       "  'stop': 144,\n",
       "  'start_token_fr': 133,\n",
       "  'stop_token_fr': 141,\n",
       "  'history_tokens': ['her_by_candle-light,_which',\n",
       "   'by_candle-light,_which_seemed',\n",
       "   'candle-light,_which_seemed_to',\n",
       "   'which_seemed_to_the'],\n",
       "  'span_tokens': ['seemed_to_the_poor',\n",
       "   'to_the_poor_sick',\n",
       "   'the_poor_sick_heart',\n",
       "   'poor_sick_heart_of',\n",
       "   'sick_heart_of_clelia',\n",
       "   'heart_of_clelia_a',\n",
       "   'of_clelia_a_horrible',\n",
       "   'clelia_a_horrible_sin'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 140,\n",
       "  'stop': 148,\n",
       "  'start_token_fr': 137,\n",
       "  'stop_token_fr': 145,\n",
       "  'history_tokens': ['seemed_to_the_poor',\n",
       "   'to_the_poor_sick',\n",
       "   'the_poor_sick_heart',\n",
       "   'poor_sick_heart_of'],\n",
       "  'span_tokens': ['sick_heart_of_clelia',\n",
       "   'heart_of_clelia_a',\n",
       "   'of_clelia_a_horrible',\n",
       "   'clelia_a_horrible_sin',\n",
       "   'a_horrible_sin_and',\n",
       "   'horrible_sin_and_one',\n",
       "   'sin_and_one_that',\n",
       "   'and_one_that_foreboded'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 10,\n",
       "  'start': 144,\n",
       "  'stop': 152,\n",
       "  'start_token_fr': 141,\n",
       "  'stop_token_fr': 149,\n",
       "  'history_tokens': ['sick_heart_of_clelia',\n",
       "   'heart_of_clelia_a',\n",
       "   'of_clelia_a_horrible',\n",
       "   'clelia_a_horrible_sin'],\n",
       "  'span_tokens': ['a_horrible_sin_and',\n",
       "   'horrible_sin_and_one',\n",
       "   'sin_and_one_that',\n",
       "   'and_one_that_foreboded',\n",
       "   'one_that_foreboded_the',\n",
       "   'that_foreboded_the_death',\n",
       "   'foreboded_the_death_of',\n",
       "   'the_death_of_sandrino.'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 8,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['la',\n",
       "   'vieille',\n",
       "   'avait',\n",
       "   'très',\n",
       "   'bien',\n",
       "   'deviné',\n",
       "   'que',\n",
       "   'ce'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['la', 'vieille', 'avait', 'très'],\n",
       "  'span_tokens': ['bien', 'deviné', 'que', 'ce', 'fut', 'un', 'cordelier'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 8,\n",
       "  'stop_token_fr': 15,\n",
       "  'history_tokens': ['bien', 'deviné', 'que', 'ce'],\n",
       "  'span_tokens': ['fut', 'un', 'cordelier', 'à', 'la', 'grande', 'manche'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 19,\n",
       "  'history_tokens': ['ce', 'fut', 'un', 'cordelier'],\n",
       "  'span_tokens': ['à',\n",
       "   'la',\n",
       "   'grande',\n",
       "   'manche',\n",
       "   'qui',\n",
       "   'vola',\n",
       "   \"l'argent\",\n",
       "   'et'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 15,\n",
       "  'stop_token_fr': 23,\n",
       "  'history_tokens': ['à', 'la', 'grande', 'manche'],\n",
       "  'span_tokens': ['qui',\n",
       "   'vola',\n",
       "   \"l'argent\",\n",
       "   'et',\n",
       "   'les',\n",
       "   'bijoux',\n",
       "   'de',\n",
       "   'cunégonde'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 19,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['qui', 'vola', \"l'argent\", 'et'],\n",
       "  'span_tokens': ['les',\n",
       "   'bijoux',\n",
       "   'de',\n",
       "   'cunégonde',\n",
       "   'dans',\n",
       "   'la',\n",
       "   'ville',\n",
       "   'de'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 23,\n",
       "  'stop_token_fr': 30,\n",
       "  'history_tokens': ['les', 'bijoux', 'de', 'cunégonde'],\n",
       "  'span_tokens': ['dans',\n",
       "   'la',\n",
       "   'ville',\n",
       "   'de',\n",
       "   'badajos,',\n",
       "   \"lorsqu'elle\",\n",
       "   'fuyait'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 34,\n",
       "  'history_tokens': ['dans', 'la', 'ville', 'de'],\n",
       "  'span_tokens': ['badajos,',\n",
       "   \"lorsqu'elle\",\n",
       "   'fuyait',\n",
       "   'en',\n",
       "   'hâte',\n",
       "   '__sent_marker_0',\n",
       "   'avec'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 30,\n",
       "  'stop_token_fr': 38,\n",
       "  'history_tokens': ['de', 'badajos,', \"lorsqu'elle\", 'fuyait'],\n",
       "  'span_tokens': ['en',\n",
       "   'hâte',\n",
       "   '__sent_marker_0',\n",
       "   'avec',\n",
       "   'candide.',\n",
       "   'la_vieille',\n",
       "   'vieille_avait',\n",
       "   'avait_très'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 34,\n",
       "  'stop_token_fr': 42,\n",
       "  'history_tokens': ['en', 'hâte', '__sent_marker_0', 'avec'],\n",
       "  'span_tokens': ['candide.',\n",
       "   'la_vieille',\n",
       "   'vieille_avait',\n",
       "   'avait_très',\n",
       "   'très_bien',\n",
       "   'bien_deviné',\n",
       "   'deviné_que',\n",
       "   'que_ce'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 38,\n",
       "  'stop_token_fr': 46,\n",
       "  'history_tokens': ['candide.', 'la_vieille', 'vieille_avait', 'avait_très'],\n",
       "  'span_tokens': ['très_bien',\n",
       "   'bien_deviné',\n",
       "   'deviné_que',\n",
       "   'que_ce',\n",
       "   'ce_fut',\n",
       "   'fut_un',\n",
       "   'un_cordelier',\n",
       "   'cordelier_à'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 42,\n",
       "  'stop_token_fr': 49,\n",
       "  'history_tokens': ['très_bien', 'bien_deviné', 'deviné_que', 'que_ce'],\n",
       "  'span_tokens': ['ce_fut',\n",
       "   'fut_un',\n",
       "   'un_cordelier',\n",
       "   'cordelier_à',\n",
       "   'à_la',\n",
       "   'la_grande',\n",
       "   'grande_manche'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 46,\n",
       "  'stop_token_fr': 53,\n",
       "  'history_tokens': ['ce_fut', 'fut_un', 'un_cordelier', 'cordelier_à'],\n",
       "  'span_tokens': ['à_la',\n",
       "   'la_grande',\n",
       "   'grande_manche',\n",
       "   'manche_qui',\n",
       "   'qui_vola',\n",
       "   \"vola_l'argent\",\n",
       "   \"l'argent_et\"],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 49,\n",
       "  'stop_token_fr': 57,\n",
       "  'history_tokens': ['cordelier_à', 'à_la', 'la_grande', 'grande_manche'],\n",
       "  'span_tokens': ['manche_qui',\n",
       "   'qui_vola',\n",
       "   \"vola_l'argent\",\n",
       "   \"l'argent_et\",\n",
       "   'et_les',\n",
       "   'les_bijoux',\n",
       "   'bijoux_de',\n",
       "   'de_cunégonde'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 53,\n",
       "  'stop_token_fr': 61,\n",
       "  'history_tokens': ['manche_qui', 'qui_vola', \"vola_l'argent\", \"l'argent_et\"],\n",
       "  'span_tokens': ['et_les',\n",
       "   'les_bijoux',\n",
       "   'bijoux_de',\n",
       "   'de_cunégonde',\n",
       "   'cunégonde_dans',\n",
       "   'dans_la',\n",
       "   'la_ville',\n",
       "   'ville_de'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 57,\n",
       "  'stop_token_fr': 65,\n",
       "  'history_tokens': ['et_les', 'les_bijoux', 'bijoux_de', 'de_cunégonde'],\n",
       "  'span_tokens': ['cunégonde_dans',\n",
       "   'dans_la',\n",
       "   'la_ville',\n",
       "   'ville_de',\n",
       "   'de_badajos,',\n",
       "   \"badajos,_lorsqu'elle\",\n",
       "   \"lorsqu'elle_fuyait\",\n",
       "   '__sent_marker_1'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 61,\n",
       "  'stop_token_fr': 68,\n",
       "  'history_tokens': ['cunégonde_dans', 'dans_la', 'la_ville', 'ville_de'],\n",
       "  'span_tokens': ['de_badajos,',\n",
       "   \"badajos,_lorsqu'elle\",\n",
       "   \"lorsqu'elle_fuyait\",\n",
       "   '__sent_marker_1',\n",
       "   'fuyait_en',\n",
       "   'en_hâte',\n",
       "   'hâte_avec'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 65,\n",
       "  'stop_token_fr': 72,\n",
       "  'history_tokens': ['de_badajos,',\n",
       "   \"badajos,_lorsqu'elle\",\n",
       "   \"lorsqu'elle_fuyait\",\n",
       "   '__sent_marker_1'],\n",
       "  'span_tokens': ['fuyait_en',\n",
       "   'en_hâte',\n",
       "   'hâte_avec',\n",
       "   'avec_candide.',\n",
       "   'la_vieille_avait',\n",
       "   'vieille_avait_très',\n",
       "   'avait_très_bien'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 68,\n",
       "  'stop_token_fr': 76,\n",
       "  'history_tokens': ['__sent_marker_1', 'fuyait_en', 'en_hâte', 'hâte_avec'],\n",
       "  'span_tokens': ['avec_candide.',\n",
       "   'la_vieille_avait',\n",
       "   'vieille_avait_très',\n",
       "   'avait_très_bien',\n",
       "   'très_bien_deviné',\n",
       "   'bien_deviné_que',\n",
       "   'deviné_que_ce',\n",
       "   'que_ce_fut'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 72,\n",
       "  'stop_token_fr': 80,\n",
       "  'history_tokens': ['avec_candide.',\n",
       "   'la_vieille_avait',\n",
       "   'vieille_avait_très',\n",
       "   'avait_très_bien'],\n",
       "  'span_tokens': ['très_bien_deviné',\n",
       "   'bien_deviné_que',\n",
       "   'deviné_que_ce',\n",
       "   'que_ce_fut',\n",
       "   'ce_fut_un',\n",
       "   'fut_un_cordelier',\n",
       "   'un_cordelier_à',\n",
       "   'cordelier_à_la'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 76,\n",
       "  'stop_token_fr': 84,\n",
       "  'history_tokens': ['très_bien_deviné',\n",
       "   'bien_deviné_que',\n",
       "   'deviné_que_ce',\n",
       "   'que_ce_fut'],\n",
       "  'span_tokens': ['ce_fut_un',\n",
       "   'fut_un_cordelier',\n",
       "   'un_cordelier_à',\n",
       "   'cordelier_à_la',\n",
       "   'à_la_grande',\n",
       "   'la_grande_manche',\n",
       "   'grande_manche_qui',\n",
       "   'manche_qui_vola'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 80,\n",
       "  'stop_token_fr': 87,\n",
       "  'history_tokens': ['ce_fut_un',\n",
       "   'fut_un_cordelier',\n",
       "   'un_cordelier_à',\n",
       "   'cordelier_à_la'],\n",
       "  'span_tokens': ['à_la_grande',\n",
       "   'la_grande_manche',\n",
       "   'grande_manche_qui',\n",
       "   'manche_qui_vola',\n",
       "   \"qui_vola_l'argent\",\n",
       "   \"vola_l'argent_et\",\n",
       "   \"l'argent_et_les\"],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 84,\n",
       "  'stop_token_fr': 91,\n",
       "  'history_tokens': ['à_la_grande',\n",
       "   'la_grande_manche',\n",
       "   'grande_manche_qui',\n",
       "   'manche_qui_vola'],\n",
       "  'span_tokens': [\"qui_vola_l'argent\",\n",
       "   \"vola_l'argent_et\",\n",
       "   \"l'argent_et_les\",\n",
       "   'et_les_bijoux',\n",
       "   'les_bijoux_de',\n",
       "   'bijoux_de_cunégonde',\n",
       "   'de_cunégonde_dans'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 87,\n",
       "  'stop_token_fr': 95,\n",
       "  'history_tokens': ['manche_qui_vola',\n",
       "   \"qui_vola_l'argent\",\n",
       "   \"vola_l'argent_et\",\n",
       "   \"l'argent_et_les\"],\n",
       "  'span_tokens': ['et_les_bijoux',\n",
       "   'les_bijoux_de',\n",
       "   'bijoux_de_cunégonde',\n",
       "   'de_cunégonde_dans',\n",
       "   'cunégonde_dans_la',\n",
       "   'dans_la_ville',\n",
       "   'la_ville_de',\n",
       "   'ville_de_badajos,'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 91,\n",
       "  'stop_token_fr': 99,\n",
       "  'history_tokens': ['et_les_bijoux',\n",
       "   'les_bijoux_de',\n",
       "   'bijoux_de_cunégonde',\n",
       "   'de_cunégonde_dans'],\n",
       "  'span_tokens': ['cunégonde_dans_la',\n",
       "   'dans_la_ville',\n",
       "   'la_ville_de',\n",
       "   'ville_de_badajos,',\n",
       "   \"de_badajos,_lorsqu'elle\",\n",
       "   '__sent_marker_2',\n",
       "   \"badajos,_lorsqu'elle_fuyait\",\n",
       "   \"lorsqu'elle_fuyait_en\"],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 95,\n",
       "  'stop_token_fr': 103,\n",
       "  'history_tokens': ['cunégonde_dans_la',\n",
       "   'dans_la_ville',\n",
       "   'la_ville_de',\n",
       "   'ville_de_badajos,'],\n",
       "  'span_tokens': [\"de_badajos,_lorsqu'elle\",\n",
       "   '__sent_marker_2',\n",
       "   \"badajos,_lorsqu'elle_fuyait\",\n",
       "   \"lorsqu'elle_fuyait_en\",\n",
       "   'fuyait_en_hâte',\n",
       "   'en_hâte_avec',\n",
       "   'hâte_avec_candide.',\n",
       "   'la_vieille_avait_très'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 99,\n",
       "  'stop_token_fr': 106,\n",
       "  'history_tokens': [\"de_badajos,_lorsqu'elle\",\n",
       "   '__sent_marker_2',\n",
       "   \"badajos,_lorsqu'elle_fuyait\",\n",
       "   \"lorsqu'elle_fuyait_en\"],\n",
       "  'span_tokens': ['fuyait_en_hâte',\n",
       "   'en_hâte_avec',\n",
       "   'hâte_avec_candide.',\n",
       "   'la_vieille_avait_très',\n",
       "   'vieille_avait_très_bien',\n",
       "   'avait_très_bien_deviné',\n",
       "   'très_bien_deviné_que'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 103,\n",
       "  'stop_token_fr': 110,\n",
       "  'history_tokens': ['fuyait_en_hâte',\n",
       "   'en_hâte_avec',\n",
       "   'hâte_avec_candide.',\n",
       "   'la_vieille_avait_très'],\n",
       "  'span_tokens': ['vieille_avait_très_bien',\n",
       "   'avait_très_bien_deviné',\n",
       "   'très_bien_deviné_que',\n",
       "   'bien_deviné_que_ce',\n",
       "   'deviné_que_ce_fut',\n",
       "   'que_ce_fut_un',\n",
       "   'ce_fut_un_cordelier'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 106,\n",
       "  'stop_token_fr': 114,\n",
       "  'history_tokens': ['la_vieille_avait_très',\n",
       "   'vieille_avait_très_bien',\n",
       "   'avait_très_bien_deviné',\n",
       "   'très_bien_deviné_que'],\n",
       "  'span_tokens': ['bien_deviné_que_ce',\n",
       "   'deviné_que_ce_fut',\n",
       "   'que_ce_fut_un',\n",
       "   'ce_fut_un_cordelier',\n",
       "   'fut_un_cordelier_à',\n",
       "   'un_cordelier_à_la',\n",
       "   'cordelier_à_la_grande',\n",
       "   'à_la_grande_manche'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 110,\n",
       "  'stop_token_fr': 118,\n",
       "  'history_tokens': ['bien_deviné_que_ce',\n",
       "   'deviné_que_ce_fut',\n",
       "   'que_ce_fut_un',\n",
       "   'ce_fut_un_cordelier'],\n",
       "  'span_tokens': ['fut_un_cordelier_à',\n",
       "   'un_cordelier_à_la',\n",
       "   'cordelier_à_la_grande',\n",
       "   'à_la_grande_manche',\n",
       "   'la_grande_manche_qui',\n",
       "   'grande_manche_qui_vola',\n",
       "   \"manche_qui_vola_l'argent\",\n",
       "   \"qui_vola_l'argent_et\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 120,\n",
       "  'stop': 128,\n",
       "  'start_token_fr': 114,\n",
       "  'stop_token_fr': 122,\n",
       "  'history_tokens': ['fut_un_cordelier_à',\n",
       "   'un_cordelier_à_la',\n",
       "   'cordelier_à_la_grande',\n",
       "   'à_la_grande_manche'],\n",
       "  'span_tokens': ['la_grande_manche_qui',\n",
       "   'grande_manche_qui_vola',\n",
       "   \"manche_qui_vola_l'argent\",\n",
       "   \"qui_vola_l'argent_et\",\n",
       "   \"vola_l'argent_et_les\",\n",
       "   \"l'argent_et_les_bijoux\",\n",
       "   'et_les_bijoux_de',\n",
       "   'les_bijoux_de_cunégonde'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 124,\n",
       "  'stop': 132,\n",
       "  'start_token_fr': 118,\n",
       "  'stop_token_fr': 125,\n",
       "  'history_tokens': ['la_grande_manche_qui',\n",
       "   'grande_manche_qui_vola',\n",
       "   \"manche_qui_vola_l'argent\",\n",
       "   \"qui_vola_l'argent_et\"],\n",
       "  'span_tokens': [\"vola_l'argent_et_les\",\n",
       "   \"l'argent_et_les_bijoux\",\n",
       "   'et_les_bijoux_de',\n",
       "   'les_bijoux_de_cunégonde',\n",
       "   'bijoux_de_cunégonde_dans',\n",
       "   'de_cunégonde_dans_la',\n",
       "   'cunégonde_dans_la_ville'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 128,\n",
       "  'stop': 136,\n",
       "  'start_token_fr': 122,\n",
       "  'stop_token_fr': 129,\n",
       "  'history_tokens': [\"vola_l'argent_et_les\",\n",
       "   \"l'argent_et_les_bijoux\",\n",
       "   'et_les_bijoux_de',\n",
       "   'les_bijoux_de_cunégonde'],\n",
       "  'span_tokens': ['bijoux_de_cunégonde_dans',\n",
       "   'de_cunégonde_dans_la',\n",
       "   'cunégonde_dans_la_ville',\n",
       "   'dans_la_ville_de',\n",
       "   'la_ville_de_badajos,',\n",
       "   \"ville_de_badajos,_lorsqu'elle\",\n",
       "   \"de_badajos,_lorsqu'elle_fuyait\"],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 11,\n",
       "  'start': 132,\n",
       "  'stop': 140,\n",
       "  'start_token_fr': 125,\n",
       "  'stop_token_fr': 133,\n",
       "  'history_tokens': ['les_bijoux_de_cunégonde',\n",
       "   'bijoux_de_cunégonde_dans',\n",
       "   'de_cunégonde_dans_la',\n",
       "   'cunégonde_dans_la_ville'],\n",
       "  'span_tokens': ['dans_la_ville_de',\n",
       "   'la_ville_de_badajos,',\n",
       "   \"ville_de_badajos,_lorsqu'elle\",\n",
       "   \"de_badajos,_lorsqu'elle_fuyait\",\n",
       "   \"badajos,_lorsqu'elle_fuyait_en\",\n",
       "   \"lorsqu'elle_fuyait_en_hâte\",\n",
       "   'fuyait_en_hâte_avec',\n",
       "   'en_hâte_avec_candide.'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 9,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['le',\n",
       "   'commis',\n",
       "   'de',\n",
       "   'l’épicier',\n",
       "   'traversa',\n",
       "   'la',\n",
       "   'rue',\n",
       "   'et',\n",
       "   'fit'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 5,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['commis', 'de', 'l’épicier', 'traversa'],\n",
       "  'span_tokens': ['la',\n",
       "   'rue',\n",
       "   'et',\n",
       "   'fit',\n",
       "   'pendant',\n",
       "   'a',\n",
       "   'celui',\n",
       "   'de',\n",
       "   'chez'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 9,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['la', 'rue', 'et', 'fit'],\n",
       "  'span_tokens': ['pendant',\n",
       "   'a',\n",
       "   'celui',\n",
       "   'de',\n",
       "   'chez',\n",
       "   'biggs,',\n",
       "   'de',\n",
       "   'l’autre',\n",
       "   'côté'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 23,\n",
       "  'history_tokens': ['a', 'celui', 'de', 'chez'],\n",
       "  'span_tokens': ['biggs,',\n",
       "   'de',\n",
       "   'l’autre',\n",
       "   'côté',\n",
       "   '__sent_marker_0',\n",
       "   'du',\n",
       "   'perron.',\n",
       "   'le_commis',\n",
       "   'commis_de'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 18,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['biggs,', 'de', 'l’autre', 'côté'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'du',\n",
       "   'perron.',\n",
       "   'le_commis',\n",
       "   'commis_de',\n",
       "   'de_l’épicier',\n",
       "   'l’épicier_traversa',\n",
       "   'traversa_la',\n",
       "   'la_rue'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 23,\n",
       "  'stop_token_fr': 32,\n",
       "  'history_tokens': ['du', 'perron.', 'le_commis', 'commis_de'],\n",
       "  'span_tokens': ['de_l’épicier',\n",
       "   'l’épicier_traversa',\n",
       "   'traversa_la',\n",
       "   'la_rue',\n",
       "   'rue_et',\n",
       "   'et_fit',\n",
       "   'fit_pendant',\n",
       "   'pendant_a',\n",
       "   'a_celui'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 36,\n",
       "  'history_tokens': ['de_l’épicier',\n",
       "   'l’épicier_traversa',\n",
       "   'traversa_la',\n",
       "   'la_rue'],\n",
       "  'span_tokens': ['rue_et',\n",
       "   'et_fit',\n",
       "   'fit_pendant',\n",
       "   'pendant_a',\n",
       "   'a_celui',\n",
       "   'celui_de',\n",
       "   'de_chez',\n",
       "   'chez_biggs,',\n",
       "   'biggs,_de'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 32,\n",
       "  'stop_token_fr': 41,\n",
       "  'history_tokens': ['et_fit', 'fit_pendant', 'pendant_a', 'a_celui'],\n",
       "  'span_tokens': ['celui_de',\n",
       "   'de_chez',\n",
       "   'chez_biggs,',\n",
       "   'biggs,_de',\n",
       "   '__sent_marker_1',\n",
       "   'de_l’autre',\n",
       "   'l’autre_côté',\n",
       "   'côté_du',\n",
       "   'du_perron.'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 36,\n",
       "  'stop_token_fr': 45,\n",
       "  'history_tokens': ['celui_de', 'de_chez', 'chez_biggs,', 'biggs,_de'],\n",
       "  'span_tokens': ['__sent_marker_1',\n",
       "   'de_l’autre',\n",
       "   'l’autre_côté',\n",
       "   'côté_du',\n",
       "   'du_perron.',\n",
       "   'le_commis_de',\n",
       "   'commis_de_l’épicier',\n",
       "   'de_l’épicier_traversa',\n",
       "   'l’épicier_traversa_la'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 41,\n",
       "  'stop_token_fr': 50,\n",
       "  'history_tokens': ['de_l’autre', 'l’autre_côté', 'côté_du', 'du_perron.'],\n",
       "  'span_tokens': ['le_commis_de',\n",
       "   'commis_de_l’épicier',\n",
       "   'de_l’épicier_traversa',\n",
       "   'l’épicier_traversa_la',\n",
       "   'traversa_la_rue',\n",
       "   'la_rue_et',\n",
       "   'rue_et_fit',\n",
       "   'et_fit_pendant',\n",
       "   'fit_pendant_a'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 45,\n",
       "  'stop_token_fr': 54,\n",
       "  'history_tokens': ['le_commis_de',\n",
       "   'commis_de_l’épicier',\n",
       "   'de_l’épicier_traversa',\n",
       "   'l’épicier_traversa_la'],\n",
       "  'span_tokens': ['traversa_la_rue',\n",
       "   'la_rue_et',\n",
       "   'rue_et_fit',\n",
       "   'et_fit_pendant',\n",
       "   'fit_pendant_a',\n",
       "   'pendant_a_celui',\n",
       "   'a_celui_de',\n",
       "   'celui_de_chez',\n",
       "   'de_chez_biggs,'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 50,\n",
       "  'stop_token_fr': 59,\n",
       "  'history_tokens': ['la_rue_et',\n",
       "   'rue_et_fit',\n",
       "   'et_fit_pendant',\n",
       "   'fit_pendant_a'],\n",
       "  'span_tokens': ['pendant_a_celui',\n",
       "   'a_celui_de',\n",
       "   'celui_de_chez',\n",
       "   'de_chez_biggs,',\n",
       "   '__sent_marker_2',\n",
       "   'chez_biggs,_de',\n",
       "   'biggs,_de_l’autre',\n",
       "   'de_l’autre_côté',\n",
       "   'l’autre_côté_du'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 54,\n",
       "  'stop_token_fr': 63,\n",
       "  'history_tokens': ['pendant_a_celui',\n",
       "   'a_celui_de',\n",
       "   'celui_de_chez',\n",
       "   'de_chez_biggs,'],\n",
       "  'span_tokens': ['__sent_marker_2',\n",
       "   'chez_biggs,_de',\n",
       "   'biggs,_de_l’autre',\n",
       "   'de_l’autre_côté',\n",
       "   'l’autre_côté_du',\n",
       "   'côté_du_perron.',\n",
       "   'le_commis_de_l’épicier',\n",
       "   'commis_de_l’épicier_traversa',\n",
       "   'de_l’épicier_traversa_la'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 59,\n",
       "  'stop_token_fr': 68,\n",
       "  'history_tokens': ['chez_biggs,_de',\n",
       "   'biggs,_de_l’autre',\n",
       "   'de_l’autre_côté',\n",
       "   'l’autre_côté_du'],\n",
       "  'span_tokens': ['côté_du_perron.',\n",
       "   'le_commis_de_l’épicier',\n",
       "   'commis_de_l’épicier_traversa',\n",
       "   'de_l’épicier_traversa_la',\n",
       "   'l’épicier_traversa_la_rue',\n",
       "   'traversa_la_rue_et',\n",
       "   'la_rue_et_fit',\n",
       "   'rue_et_fit_pendant',\n",
       "   'et_fit_pendant_a'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 63,\n",
       "  'stop_token_fr': 72,\n",
       "  'history_tokens': ['côté_du_perron.',\n",
       "   'le_commis_de_l’épicier',\n",
       "   'commis_de_l’épicier_traversa',\n",
       "   'de_l’épicier_traversa_la'],\n",
       "  'span_tokens': ['l’épicier_traversa_la_rue',\n",
       "   'traversa_la_rue_et',\n",
       "   'la_rue_et_fit',\n",
       "   'rue_et_fit_pendant',\n",
       "   'et_fit_pendant_a',\n",
       "   'fit_pendant_a_celui',\n",
       "   'pendant_a_celui_de',\n",
       "   'a_celui_de_chez',\n",
       "   'celui_de_chez_biggs,'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 12,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 68,\n",
       "  'stop_token_fr': 77,\n",
       "  'history_tokens': ['traversa_la_rue_et',\n",
       "   'la_rue_et_fit',\n",
       "   'rue_et_fit_pendant',\n",
       "   'et_fit_pendant_a'],\n",
       "  'span_tokens': ['fit_pendant_a_celui',\n",
       "   'pendant_a_celui_de',\n",
       "   'a_celui_de_chez',\n",
       "   'celui_de_chez_biggs,',\n",
       "   'de_chez_biggs,_de',\n",
       "   'chez_biggs,_de_l’autre',\n",
       "   'biggs,_de_l’autre_côté',\n",
       "   'de_l’autre_côté_du',\n",
       "   'l’autre_côté_du_perron.'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 8,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['trois',\n",
       "   'ans',\n",
       "   \"s'étaient\",\n",
       "   'écoulés',\n",
       "   'depuis',\n",
       "   'que',\n",
       "   'les',\n",
       "   'prisonniers'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['trois', 'ans', \"s'étaient\", 'écoulés'],\n",
       "  'span_tokens': ['depuis',\n",
       "   'que',\n",
       "   'les',\n",
       "   'prisonniers',\n",
       "   'de',\n",
       "   'richmond',\n",
       "   \"s'étaient\"],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 8,\n",
       "  'stop_token_fr': 15,\n",
       "  'history_tokens': ['depuis', 'que', 'les', 'prisonniers'],\n",
       "  'span_tokens': ['de', 'richmond', \"s'étaient\", 'enfuis,', 'et', 'que', 'de'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 19,\n",
       "  'history_tokens': ['prisonniers', 'de', 'richmond', \"s'étaient\"],\n",
       "  'span_tokens': ['enfuis,',\n",
       "   'et',\n",
       "   'que',\n",
       "   'de',\n",
       "   'fois,',\n",
       "   'pendant',\n",
       "   'ces',\n",
       "   'trois'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 15,\n",
       "  'stop_token_fr': 23,\n",
       "  'history_tokens': ['enfuis,', 'et', 'que', 'de'],\n",
       "  'span_tokens': ['fois,',\n",
       "   'pendant',\n",
       "   'ces',\n",
       "   'trois',\n",
       "   'années,',\n",
       "   'ils',\n",
       "   'parlèrent',\n",
       "   'de'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 19,\n",
       "  'stop_token_fr': 26,\n",
       "  'history_tokens': ['fois,', 'pendant', 'ces', 'trois'],\n",
       "  'span_tokens': ['années,',\n",
       "   'ils',\n",
       "   'parlèrent',\n",
       "   'de',\n",
       "   'la',\n",
       "   'patrie,',\n",
       "   'toujours'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 23,\n",
       "  'stop_token_fr': 30,\n",
       "  'history_tokens': ['années,', 'ils', 'parlèrent', 'de'],\n",
       "  'span_tokens': ['la',\n",
       "   'patrie,',\n",
       "   'toujours',\n",
       "   'présente',\n",
       "   'à',\n",
       "   '__sent_marker_0',\n",
       "   'leur'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 26,\n",
       "  'stop_token_fr': 34,\n",
       "  'history_tokens': ['de', 'la', 'patrie,', 'toujours'],\n",
       "  'span_tokens': ['présente',\n",
       "   'à',\n",
       "   '__sent_marker_0',\n",
       "   'leur',\n",
       "   'pensée!',\n",
       "   'trois_ans',\n",
       "   \"ans_s'étaient\",\n",
       "   \"s'étaient_écoulés\"],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 30,\n",
       "  'stop_token_fr': 38,\n",
       "  'history_tokens': ['présente', 'à', '__sent_marker_0', 'leur'],\n",
       "  'span_tokens': ['pensée!',\n",
       "   'trois_ans',\n",
       "   \"ans_s'étaient\",\n",
       "   \"s'étaient_écoulés\",\n",
       "   'écoulés_depuis',\n",
       "   'depuis_que',\n",
       "   'que_les',\n",
       "   'les_prisonniers'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 34,\n",
       "  'stop_token_fr': 42,\n",
       "  'history_tokens': ['pensée!',\n",
       "   'trois_ans',\n",
       "   \"ans_s'étaient\",\n",
       "   \"s'étaient_écoulés\"],\n",
       "  'span_tokens': ['écoulés_depuis',\n",
       "   'depuis_que',\n",
       "   'que_les',\n",
       "   'les_prisonniers',\n",
       "   'prisonniers_de',\n",
       "   'de_richmond',\n",
       "   \"richmond_s'étaient\",\n",
       "   \"s'étaient_enfuis,\"],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 38,\n",
       "  'stop_token_fr': 45,\n",
       "  'history_tokens': ['écoulés_depuis',\n",
       "   'depuis_que',\n",
       "   'que_les',\n",
       "   'les_prisonniers'],\n",
       "  'span_tokens': ['prisonniers_de',\n",
       "   'de_richmond',\n",
       "   \"richmond_s'étaient\",\n",
       "   \"s'étaient_enfuis,\",\n",
       "   'enfuis,_et',\n",
       "   'et_que',\n",
       "   'que_de'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 42,\n",
       "  'stop_token_fr': 49,\n",
       "  'history_tokens': ['prisonniers_de',\n",
       "   'de_richmond',\n",
       "   \"richmond_s'étaient\",\n",
       "   \"s'étaient_enfuis,\"],\n",
       "  'span_tokens': ['enfuis,_et',\n",
       "   'et_que',\n",
       "   'que_de',\n",
       "   'de_fois,',\n",
       "   'fois,_pendant',\n",
       "   'pendant_ces',\n",
       "   'ces_trois'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 45,\n",
       "  'stop_token_fr': 53,\n",
       "  'history_tokens': [\"s'étaient_enfuis,\", 'enfuis,_et', 'et_que', 'que_de'],\n",
       "  'span_tokens': ['de_fois,',\n",
       "   'fois,_pendant',\n",
       "   'pendant_ces',\n",
       "   'ces_trois',\n",
       "   'trois_années,',\n",
       "   'années,_ils',\n",
       "   'ils_parlèrent',\n",
       "   'parlèrent_de'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 49,\n",
       "  'stop_token_fr': 57,\n",
       "  'history_tokens': ['de_fois,', 'fois,_pendant', 'pendant_ces', 'ces_trois'],\n",
       "  'span_tokens': ['trois_années,',\n",
       "   'années,_ils',\n",
       "   'ils_parlèrent',\n",
       "   'parlèrent_de',\n",
       "   'de_la',\n",
       "   'la_patrie,',\n",
       "   'patrie,_toujours',\n",
       "   '__sent_marker_1'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 53,\n",
       "  'stop_token_fr': 60,\n",
       "  'history_tokens': ['trois_années,',\n",
       "   'années,_ils',\n",
       "   'ils_parlèrent',\n",
       "   'parlèrent_de'],\n",
       "  'span_tokens': ['de_la',\n",
       "   'la_patrie,',\n",
       "   'patrie,_toujours',\n",
       "   '__sent_marker_1',\n",
       "   'toujours_présente',\n",
       "   'présente_à',\n",
       "   'à_leur'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 57,\n",
       "  'stop_token_fr': 64,\n",
       "  'history_tokens': ['de_la',\n",
       "   'la_patrie,',\n",
       "   'patrie,_toujours',\n",
       "   '__sent_marker_1'],\n",
       "  'span_tokens': ['toujours_présente',\n",
       "   'présente_à',\n",
       "   'à_leur',\n",
       "   'leur_pensée!',\n",
       "   \"trois_ans_s'étaient\",\n",
       "   \"ans_s'étaient_écoulés\",\n",
       "   \"s'étaient_écoulés_depuis\"],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 60,\n",
       "  'stop_token_fr': 68,\n",
       "  'history_tokens': ['__sent_marker_1',\n",
       "   'toujours_présente',\n",
       "   'présente_à',\n",
       "   'à_leur'],\n",
       "  'span_tokens': ['leur_pensée!',\n",
       "   \"trois_ans_s'étaient\",\n",
       "   \"ans_s'étaient_écoulés\",\n",
       "   \"s'étaient_écoulés_depuis\",\n",
       "   'écoulés_depuis_que',\n",
       "   'depuis_que_les',\n",
       "   'que_les_prisonniers',\n",
       "   'les_prisonniers_de'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 64,\n",
       "  'stop_token_fr': 72,\n",
       "  'history_tokens': ['leur_pensée!',\n",
       "   \"trois_ans_s'étaient\",\n",
       "   \"ans_s'étaient_écoulés\",\n",
       "   \"s'étaient_écoulés_depuis\"],\n",
       "  'span_tokens': ['écoulés_depuis_que',\n",
       "   'depuis_que_les',\n",
       "   'que_les_prisonniers',\n",
       "   'les_prisonniers_de',\n",
       "   'prisonniers_de_richmond',\n",
       "   \"de_richmond_s'étaient\",\n",
       "   \"richmond_s'étaient_enfuis,\",\n",
       "   \"s'étaient_enfuis,_et\"],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 68,\n",
       "  'stop_token_fr': 75,\n",
       "  'history_tokens': ['écoulés_depuis_que',\n",
       "   'depuis_que_les',\n",
       "   'que_les_prisonniers',\n",
       "   'les_prisonniers_de'],\n",
       "  'span_tokens': ['prisonniers_de_richmond',\n",
       "   \"de_richmond_s'étaient\",\n",
       "   \"richmond_s'étaient_enfuis,\",\n",
       "   \"s'étaient_enfuis,_et\",\n",
       "   'enfuis,_et_que',\n",
       "   'et_que_de',\n",
       "   'que_de_fois,'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 72,\n",
       "  'stop_token_fr': 79,\n",
       "  'history_tokens': ['prisonniers_de_richmond',\n",
       "   \"de_richmond_s'étaient\",\n",
       "   \"richmond_s'étaient_enfuis,\",\n",
       "   \"s'étaient_enfuis,_et\"],\n",
       "  'span_tokens': ['enfuis,_et_que',\n",
       "   'et_que_de',\n",
       "   'que_de_fois,',\n",
       "   'de_fois,_pendant',\n",
       "   'fois,_pendant_ces',\n",
       "   'pendant_ces_trois',\n",
       "   'ces_trois_années,'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 75,\n",
       "  'stop_token_fr': 83,\n",
       "  'history_tokens': [\"s'étaient_enfuis,_et\",\n",
       "   'enfuis,_et_que',\n",
       "   'et_que_de',\n",
       "   'que_de_fois,'],\n",
       "  'span_tokens': ['de_fois,_pendant',\n",
       "   'fois,_pendant_ces',\n",
       "   'pendant_ces_trois',\n",
       "   'ces_trois_années,',\n",
       "   'trois_années,_ils',\n",
       "   'années,_ils_parlèrent',\n",
       "   'ils_parlèrent_de',\n",
       "   'parlèrent_de_la'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 79,\n",
       "  'stop_token_fr': 87,\n",
       "  'history_tokens': ['de_fois,_pendant',\n",
       "   'fois,_pendant_ces',\n",
       "   'pendant_ces_trois',\n",
       "   'ces_trois_années,'],\n",
       "  'span_tokens': ['trois_années,_ils',\n",
       "   'années,_ils_parlèrent',\n",
       "   'ils_parlèrent_de',\n",
       "   'parlèrent_de_la',\n",
       "   'de_la_patrie,',\n",
       "   '__sent_marker_2',\n",
       "   'la_patrie,_toujours',\n",
       "   'patrie,_toujours_présente'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 83,\n",
       "  'stop_token_fr': 91,\n",
       "  'history_tokens': ['trois_années,_ils',\n",
       "   'années,_ils_parlèrent',\n",
       "   'ils_parlèrent_de',\n",
       "   'parlèrent_de_la'],\n",
       "  'span_tokens': ['de_la_patrie,',\n",
       "   '__sent_marker_2',\n",
       "   'la_patrie,_toujours',\n",
       "   'patrie,_toujours_présente',\n",
       "   'toujours_présente_à',\n",
       "   'présente_à_leur',\n",
       "   'à_leur_pensée!',\n",
       "   \"trois_ans_s'étaient_écoulés\"],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 87,\n",
       "  'stop_token_fr': 94,\n",
       "  'history_tokens': ['de_la_patrie,',\n",
       "   '__sent_marker_2',\n",
       "   'la_patrie,_toujours',\n",
       "   'patrie,_toujours_présente'],\n",
       "  'span_tokens': ['toujours_présente_à',\n",
       "   'présente_à_leur',\n",
       "   'à_leur_pensée!',\n",
       "   \"trois_ans_s'étaient_écoulés\",\n",
       "   \"ans_s'étaient_écoulés_depuis\",\n",
       "   \"s'étaient_écoulés_depuis_que\",\n",
       "   'écoulés_depuis_que_les'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 91,\n",
       "  'stop_token_fr': 98,\n",
       "  'history_tokens': ['toujours_présente_à',\n",
       "   'présente_à_leur',\n",
       "   'à_leur_pensée!',\n",
       "   \"trois_ans_s'étaient_écoulés\"],\n",
       "  'span_tokens': [\"ans_s'étaient_écoulés_depuis\",\n",
       "   \"s'étaient_écoulés_depuis_que\",\n",
       "   'écoulés_depuis_que_les',\n",
       "   'depuis_que_les_prisonniers',\n",
       "   'que_les_prisonniers_de',\n",
       "   'les_prisonniers_de_richmond',\n",
       "   \"prisonniers_de_richmond_s'étaient\"],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 94,\n",
       "  'stop_token_fr': 102,\n",
       "  'history_tokens': [\"trois_ans_s'étaient_écoulés\",\n",
       "   \"ans_s'étaient_écoulés_depuis\",\n",
       "   \"s'étaient_écoulés_depuis_que\",\n",
       "   'écoulés_depuis_que_les'],\n",
       "  'span_tokens': ['depuis_que_les_prisonniers',\n",
       "   'que_les_prisonniers_de',\n",
       "   'les_prisonniers_de_richmond',\n",
       "   \"prisonniers_de_richmond_s'étaient\",\n",
       "   \"de_richmond_s'étaient_enfuis,\",\n",
       "   \"richmond_s'étaient_enfuis,_et\",\n",
       "   \"s'étaient_enfuis,_et_que\",\n",
       "   'enfuis,_et_que_de'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 98,\n",
       "  'stop_token_fr': 106,\n",
       "  'history_tokens': ['depuis_que_les_prisonniers',\n",
       "   'que_les_prisonniers_de',\n",
       "   'les_prisonniers_de_richmond',\n",
       "   \"prisonniers_de_richmond_s'étaient\"],\n",
       "  'span_tokens': [\"de_richmond_s'étaient_enfuis,\",\n",
       "   \"richmond_s'étaient_enfuis,_et\",\n",
       "   \"s'étaient_enfuis,_et_que\",\n",
       "   'enfuis,_et_que_de',\n",
       "   'et_que_de_fois,',\n",
       "   'que_de_fois,_pendant',\n",
       "   'de_fois,_pendant_ces',\n",
       "   'fois,_pendant_ces_trois'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 102,\n",
       "  'stop_token_fr': 109,\n",
       "  'history_tokens': [\"de_richmond_s'étaient_enfuis,\",\n",
       "   \"richmond_s'étaient_enfuis,_et\",\n",
       "   \"s'étaient_enfuis,_et_que\",\n",
       "   'enfuis,_et_que_de'],\n",
       "  'span_tokens': ['et_que_de_fois,',\n",
       "   'que_de_fois,_pendant',\n",
       "   'de_fois,_pendant_ces',\n",
       "   'fois,_pendant_ces_trois',\n",
       "   'pendant_ces_trois_années,',\n",
       "   'ces_trois_années,_ils',\n",
       "   'trois_années,_ils_parlèrent'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 106,\n",
       "  'stop_token_fr': 113,\n",
       "  'history_tokens': ['et_que_de_fois,',\n",
       "   'que_de_fois,_pendant',\n",
       "   'de_fois,_pendant_ces',\n",
       "   'fois,_pendant_ces_trois'],\n",
       "  'span_tokens': ['pendant_ces_trois_années,',\n",
       "   'ces_trois_années,_ils',\n",
       "   'trois_années,_ils_parlèrent',\n",
       "   'années,_ils_parlèrent_de',\n",
       "   'ils_parlèrent_de_la',\n",
       "   'parlèrent_de_la_patrie,',\n",
       "   'de_la_patrie,_toujours'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 13,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 109,\n",
       "  'stop_token_fr': 117,\n",
       "  'history_tokens': ['fois,_pendant_ces_trois',\n",
       "   'pendant_ces_trois_années,',\n",
       "   'ces_trois_années,_ils',\n",
       "   'trois_années,_ils_parlèrent'],\n",
       "  'span_tokens': ['années,_ils_parlèrent_de',\n",
       "   'ils_parlèrent_de_la',\n",
       "   'parlèrent_de_la_patrie,',\n",
       "   'de_la_patrie,_toujours',\n",
       "   'la_patrie,_toujours_présente',\n",
       "   'patrie,_toujours_présente_à',\n",
       "   'toujours_présente_à_leur',\n",
       "   'présente_à_leur_pensée!'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 4,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['ainsi,', 'cette', 'demoiselle', 'gautier,'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 2,\n",
       "  'stop_token_fr': 6,\n",
       "  'history_tokens': ['ainsi,', 'cette'],\n",
       "  'span_tokens': ['demoiselle', 'gautier,', 'il', 'parait'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 8,\n",
       "  'history_tokens': ['ainsi,', 'cette', 'demoiselle', 'gautier,'],\n",
       "  'span_tokens': ['il', 'parait', \"qu'elle\", 'a'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 6,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': ['demoiselle', 'gautier,', 'il', 'parait'],\n",
       "  'span_tokens': [\"qu'elle\", 'a', 'fait', 'un'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 8,\n",
       "  'stop_token_fr': 12,\n",
       "  'history_tokens': ['il', 'parait', \"qu'elle\", 'a'],\n",
       "  'span_tokens': ['fait', 'un', 'peu', 'la'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': [\"qu'elle\", 'a', 'fait', 'un'],\n",
       "  'span_tokens': ['peu', 'la', 'vie,', '__sent_marker_0'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 12,\n",
       "  'stop_token_fr': 16,\n",
       "  'history_tokens': ['fait', 'un', 'peu', 'la'],\n",
       "  'span_tokens': ['vie,', '__sent_marker_0', 'passez-mois', \"l'expression.\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['peu', 'la', 'vie,', '__sent_marker_0'],\n",
       "  'span_tokens': ['passez-mois',\n",
       "   \"l'expression.\",\n",
       "   'ainsi,_cette',\n",
       "   'cette_demoiselle'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 16,\n",
       "  'stop_token_fr': 20,\n",
       "  'history_tokens': ['vie,',\n",
       "   '__sent_marker_0',\n",
       "   'passez-mois',\n",
       "   \"l'expression.\"],\n",
       "  'span_tokens': ['ainsi,_cette',\n",
       "   'cette_demoiselle',\n",
       "   'demoiselle_gautier,',\n",
       "   'gautier,_il'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 18,\n",
       "  'stop_token_fr': 22,\n",
       "  'history_tokens': ['passez-mois',\n",
       "   \"l'expression.\",\n",
       "   'ainsi,_cette',\n",
       "   'cette_demoiselle'],\n",
       "  'span_tokens': ['demoiselle_gautier,',\n",
       "   'gautier,_il',\n",
       "   'il_parait',\n",
       "   \"parait_qu'elle\"],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 20,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['ainsi,_cette',\n",
       "   'cette_demoiselle',\n",
       "   'demoiselle_gautier,',\n",
       "   'gautier,_il'],\n",
       "  'span_tokens': ['il_parait', \"parait_qu'elle\", \"qu'elle_a\", 'a_fait'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 22,\n",
       "  'stop_token_fr': 26,\n",
       "  'history_tokens': ['demoiselle_gautier,',\n",
       "   'gautier,_il',\n",
       "   'il_parait',\n",
       "   \"parait_qu'elle\"],\n",
       "  'span_tokens': [\"qu'elle_a\", 'a_fait', 'fait_un', 'un_peu'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 24,\n",
       "  'stop_token_fr': 28,\n",
       "  'history_tokens': ['il_parait', \"parait_qu'elle\", \"qu'elle_a\", 'a_fait'],\n",
       "  'span_tokens': ['fait_un', 'un_peu', '__sent_marker_1', 'peu_la'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 26,\n",
       "  'stop_token_fr': 31,\n",
       "  'history_tokens': [\"qu'elle_a\", 'a_fait', 'fait_un', 'un_peu'],\n",
       "  'span_tokens': ['__sent_marker_1',\n",
       "   'peu_la',\n",
       "   'la_vie,',\n",
       "   'vie,_passez-mois',\n",
       "   \"passez-mois_l'expression.\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 28,\n",
       "  'stop_token_fr': 33,\n",
       "  'history_tokens': ['fait_un', 'un_peu', '__sent_marker_1', 'peu_la'],\n",
       "  'span_tokens': ['la_vie,',\n",
       "   'vie,_passez-mois',\n",
       "   \"passez-mois_l'expression.\",\n",
       "   'ainsi,_cette_demoiselle',\n",
       "   'cette_demoiselle_gautier,'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 31,\n",
       "  'stop_token_fr': 35,\n",
       "  'history_tokens': ['peu_la',\n",
       "   'la_vie,',\n",
       "   'vie,_passez-mois',\n",
       "   \"passez-mois_l'expression.\"],\n",
       "  'span_tokens': ['ainsi,_cette_demoiselle',\n",
       "   'cette_demoiselle_gautier,',\n",
       "   'demoiselle_gautier,_il',\n",
       "   'gautier,_il_parait'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 33,\n",
       "  'stop_token_fr': 37,\n",
       "  'history_tokens': ['vie,_passez-mois',\n",
       "   \"passez-mois_l'expression.\",\n",
       "   'ainsi,_cette_demoiselle',\n",
       "   'cette_demoiselle_gautier,'],\n",
       "  'span_tokens': ['demoiselle_gautier,_il',\n",
       "   'gautier,_il_parait',\n",
       "   \"il_parait_qu'elle\",\n",
       "   \"parait_qu'elle_a\"],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 35,\n",
       "  'stop_token_fr': 39,\n",
       "  'history_tokens': ['ainsi,_cette_demoiselle',\n",
       "   'cette_demoiselle_gautier,',\n",
       "   'demoiselle_gautier,_il',\n",
       "   'gautier,_il_parait'],\n",
       "  'span_tokens': [\"il_parait_qu'elle\",\n",
       "   \"parait_qu'elle_a\",\n",
       "   \"qu'elle_a_fait\",\n",
       "   'a_fait_un'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 37,\n",
       "  'stop_token_fr': 41,\n",
       "  'history_tokens': ['demoiselle_gautier,_il',\n",
       "   'gautier,_il_parait',\n",
       "   \"il_parait_qu'elle\",\n",
       "   \"parait_qu'elle_a\"],\n",
       "  'span_tokens': [\"qu'elle_a_fait\",\n",
       "   'a_fait_un',\n",
       "   '__sent_marker_2',\n",
       "   'fait_un_peu'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 39,\n",
       "  'stop_token_fr': 43,\n",
       "  'history_tokens': [\"il_parait_qu'elle\",\n",
       "   \"parait_qu'elle_a\",\n",
       "   \"qu'elle_a_fait\",\n",
       "   'a_fait_un'],\n",
       "  'span_tokens': ['__sent_marker_2',\n",
       "   'fait_un_peu',\n",
       "   'un_peu_la',\n",
       "   'peu_la_vie,'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 41,\n",
       "  'stop_token_fr': 45,\n",
       "  'history_tokens': [\"qu'elle_a_fait\",\n",
       "   'a_fait_un',\n",
       "   '__sent_marker_2',\n",
       "   'fait_un_peu'],\n",
       "  'span_tokens': ['un_peu_la',\n",
       "   'peu_la_vie,',\n",
       "   'la_vie,_passez-mois',\n",
       "   \"vie,_passez-mois_l'expression.\"],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 43,\n",
       "  'stop_token_fr': 47,\n",
       "  'history_tokens': ['__sent_marker_2',\n",
       "   'fait_un_peu',\n",
       "   'un_peu_la',\n",
       "   'peu_la_vie,'],\n",
       "  'span_tokens': ['la_vie,_passez-mois',\n",
       "   \"vie,_passez-mois_l'expression.\",\n",
       "   'ainsi,_cette_demoiselle_gautier,',\n",
       "   'cette_demoiselle_gautier,_il'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 45,\n",
       "  'stop_token_fr': 49,\n",
       "  'history_tokens': ['un_peu_la',\n",
       "   'peu_la_vie,',\n",
       "   'la_vie,_passez-mois',\n",
       "   \"vie,_passez-mois_l'expression.\"],\n",
       "  'span_tokens': ['ainsi,_cette_demoiselle_gautier,',\n",
       "   'cette_demoiselle_gautier,_il',\n",
       "   'demoiselle_gautier,_il_parait',\n",
       "   \"gautier,_il_parait_qu'elle\"],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 47,\n",
       "  'stop_token_fr': 51,\n",
       "  'history_tokens': ['la_vie,_passez-mois',\n",
       "   \"vie,_passez-mois_l'expression.\",\n",
       "   'ainsi,_cette_demoiselle_gautier,',\n",
       "   'cette_demoiselle_gautier,_il'],\n",
       "  'span_tokens': ['demoiselle_gautier,_il_parait',\n",
       "   \"gautier,_il_parait_qu'elle\",\n",
       "   \"il_parait_qu'elle_a\",\n",
       "   \"parait_qu'elle_a_fait\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 49,\n",
       "  'stop_token_fr': 53,\n",
       "  'history_tokens': ['ainsi,_cette_demoiselle_gautier,',\n",
       "   'cette_demoiselle_gautier,_il',\n",
       "   'demoiselle_gautier,_il_parait',\n",
       "   \"gautier,_il_parait_qu'elle\"],\n",
       "  'span_tokens': [\"il_parait_qu'elle_a\",\n",
       "   \"parait_qu'elle_a_fait\",\n",
       "   \"qu'elle_a_fait_un\",\n",
       "   'a_fait_un_peu'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 51,\n",
       "  'stop_token_fr': 55,\n",
       "  'history_tokens': ['demoiselle_gautier,_il_parait',\n",
       "   \"gautier,_il_parait_qu'elle\",\n",
       "   \"il_parait_qu'elle_a\",\n",
       "   \"parait_qu'elle_a_fait\"],\n",
       "  'span_tokens': [\"qu'elle_a_fait_un\",\n",
       "   'a_fait_un_peu',\n",
       "   'fait_un_peu_la',\n",
       "   'un_peu_la_vie,'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 14,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 53,\n",
       "  'stop_token_fr': 57,\n",
       "  'history_tokens': [\"il_parait_qu'elle_a\",\n",
       "   \"parait_qu'elle_a_fait\",\n",
       "   \"qu'elle_a_fait_un\",\n",
       "   'a_fait_un_peu'],\n",
       "  'span_tokens': ['fait_un_peu_la',\n",
       "   'un_peu_la_vie,',\n",
       "   'peu_la_vie,_passez-mois',\n",
       "   \"la_vie,_passez-mois_l'expression.\"],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 15,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['--',\n",
       "   'il',\n",
       "   \"n'y\",\n",
       "   'aurait',\n",
       "   '__sent_marker_0',\n",
       "   \"qu'un\",\n",
       "   'moyen.'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 15,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['--', 'il', \"n'y\", 'aurait'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   \"qu'un\",\n",
       "   'moyen.',\n",
       "   '--_il',\n",
       "   '__sent_marker_1',\n",
       "   \"il_n'y\",\n",
       "   \"n'y_aurait\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 15,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['aurait', '__sent_marker_0', \"qu'un\", 'moyen.'],\n",
       "  'span_tokens': ['--_il',\n",
       "   '__sent_marker_1',\n",
       "   \"il_n'y\",\n",
       "   \"n'y_aurait\",\n",
       "   \"aurait_qu'un\",\n",
       "   '__sent_marker_2',\n",
       "   \"qu'un_moyen.\"],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 15,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['--_il', '__sent_marker_1', \"il_n'y\", \"n'y_aurait\"],\n",
       "  'span_tokens': [\"aurait_qu'un\",\n",
       "   '__sent_marker_2',\n",
       "   \"qu'un_moyen.\",\n",
       "   \"--_il_n'y\",\n",
       "   \"il_n'y_aurait\",\n",
       "   \"n'y_aurait_qu'un\",\n",
       "   \"aurait_qu'un_moyen.\"],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 15,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': [\"n'y_aurait\",\n",
       "   \"aurait_qu'un\",\n",
       "   '__sent_marker_2',\n",
       "   \"qu'un_moyen.\"],\n",
       "  'span_tokens': [\"--_il_n'y\",\n",
       "   \"il_n'y_aurait\",\n",
       "   \"n'y_aurait_qu'un\",\n",
       "   \"aurait_qu'un_moyen.\",\n",
       "   \"--_il_n'y_aurait\",\n",
       "   \"il_n'y_aurait_qu'un\",\n",
       "   \"n'y_aurait_qu'un_moyen.\"],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 8,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['nous',\n",
       "   \"n'avions\",\n",
       "   'pas',\n",
       "   'été',\n",
       "   'longtemps',\n",
       "   'hors',\n",
       "   'de',\n",
       "   'la'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 12,\n",
       "  'history_tokens': ['nous', \"n'avions\", 'pas', 'été'],\n",
       "  'span_tokens': ['longtemps',\n",
       "   'hors',\n",
       "   'de',\n",
       "   'la',\n",
       "   'boutique',\n",
       "   'que',\n",
       "   'le',\n",
       "   'mercier'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 8,\n",
       "  'stop_token_fr': 15,\n",
       "  'history_tokens': ['longtemps', 'hors', 'de', 'la'],\n",
       "  'span_tokens': ['boutique',\n",
       "   'que',\n",
       "   'le',\n",
       "   'mercier',\n",
       "   \"s'aperçut\",\n",
       "   'que',\n",
       "   'la'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 12,\n",
       "  'stop_token_fr': 19,\n",
       "  'history_tokens': ['boutique', 'que', 'le', 'mercier'],\n",
       "  'span_tokens': [\"s'aperçut\",\n",
       "   'que',\n",
       "   'la',\n",
       "   'pièce',\n",
       "   \"d'étoffe\",\n",
       "   'avait',\n",
       "   'disparu,'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 15,\n",
       "  'stop_token_fr': 23,\n",
       "  'history_tokens': ['mercier', \"s'aperçut\", 'que', 'la'],\n",
       "  'span_tokens': ['pièce',\n",
       "   \"d'étoffe\",\n",
       "   'avait',\n",
       "   'disparu,',\n",
       "   'et',\n",
       "   'envoya',\n",
       "   'ses',\n",
       "   'commis'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 19,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['pièce', \"d'étoffe\", 'avait', 'disparu,'],\n",
       "  'span_tokens': ['et',\n",
       "   'envoya',\n",
       "   'ses',\n",
       "   'commis',\n",
       "   'qui',\n",
       "   \"d'un\",\n",
       "   'côté,',\n",
       "   'qui'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 23,\n",
       "  'stop_token_fr': 31,\n",
       "  'history_tokens': ['et', 'envoya', 'ses', 'commis'],\n",
       "  'span_tokens': ['qui',\n",
       "   \"d'un\",\n",
       "   'côté,',\n",
       "   'qui',\n",
       "   \"d'un\",\n",
       "   'autre;',\n",
       "   'et',\n",
       "   'bientôt'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 35,\n",
       "  'history_tokens': ['qui', \"d'un\", 'côté,', 'qui'],\n",
       "  'span_tokens': [\"d'un\",\n",
       "   'autre;',\n",
       "   'et',\n",
       "   'bientôt',\n",
       "   'ils',\n",
       "   'eurent',\n",
       "   'saisi',\n",
       "   'la'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 31,\n",
       "  'stop_token_fr': 38,\n",
       "  'history_tokens': [\"d'un\", 'autre;', 'et', 'bientôt'],\n",
       "  'span_tokens': ['ils', 'eurent', 'saisi', 'la', 'femme', 'qui', 'portait'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 35,\n",
       "  'stop_token_fr': 42,\n",
       "  'history_tokens': ['ils', 'eurent', 'saisi', 'la'],\n",
       "  'span_tokens': ['femme',\n",
       "   'qui',\n",
       "   'portait',\n",
       "   'la',\n",
       "   'pièce,',\n",
       "   'et',\n",
       "   'trouvèrent'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 38,\n",
       "  'stop_token_fr': 46,\n",
       "  'history_tokens': ['la', 'femme', 'qui', 'portait'],\n",
       "  'span_tokens': ['la',\n",
       "   'pièce,',\n",
       "   'et',\n",
       "   'trouvèrent',\n",
       "   'le',\n",
       "   'damas',\n",
       "   'sur',\n",
       "   'elle;'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 42,\n",
       "  'stop_token_fr': 50,\n",
       "  'history_tokens': ['la', 'pièce,', 'et', 'trouvèrent'],\n",
       "  'span_tokens': ['le',\n",
       "   'damas',\n",
       "   'sur',\n",
       "   'elle;',\n",
       "   'pour',\n",
       "   'moi',\n",
       "   'je',\n",
       "   \"m'étais\"],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 46,\n",
       "  'stop_token_fr': 54,\n",
       "  'history_tokens': ['le', 'damas', 'sur', 'elle;'],\n",
       "  'span_tokens': ['pour',\n",
       "   'moi',\n",
       "   'je',\n",
       "   \"m'étais\",\n",
       "   'faufilée',\n",
       "   'par',\n",
       "   'chance',\n",
       "   'dans'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 50,\n",
       "  'stop_token_fr': 58,\n",
       "  'history_tokens': ['pour', 'moi', 'je', \"m'étais\"],\n",
       "  'span_tokens': ['faufilée',\n",
       "   'par',\n",
       "   'chance',\n",
       "   'dans',\n",
       "   'une',\n",
       "   'maison',\n",
       "   'où',\n",
       "   'il'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 54,\n",
       "  'stop_token_fr': 62,\n",
       "  'history_tokens': ['faufilée', 'par', 'chance', 'dans'],\n",
       "  'span_tokens': ['une', 'maison', 'où', 'il', 'y', 'avait', 'une', 'chambre'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 58,\n",
       "  'stop_token_fr': 65,\n",
       "  'history_tokens': ['une', 'maison', 'où', 'il'],\n",
       "  'span_tokens': ['y', 'avait', 'une', 'chambre', 'à', 'dentelle,', 'au'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 62,\n",
       "  'stop_token_fr': 69,\n",
       "  'history_tokens': ['y', 'avait', 'une', 'chambre'],\n",
       "  'span_tokens': ['à',\n",
       "   'dentelle,',\n",
       "   'au',\n",
       "   'palier',\n",
       "   'du',\n",
       "   'premier',\n",
       "   'escalier;'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 65,\n",
       "  'stop_token_fr': 73,\n",
       "  'history_tokens': ['chambre', 'à', 'dentelle,', 'au'],\n",
       "  'span_tokens': ['palier',\n",
       "   'du',\n",
       "   'premier',\n",
       "   'escalier;',\n",
       "   'et',\n",
       "   \"j'eus\",\n",
       "   'la',\n",
       "   'satisfaction,'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 69,\n",
       "  'stop_token_fr': 77,\n",
       "  'history_tokens': ['palier', 'du', 'premier', 'escalier;'],\n",
       "  'span_tokens': ['et',\n",
       "   \"j'eus\",\n",
       "   'la',\n",
       "   'satisfaction,',\n",
       "   'ou',\n",
       "   'la',\n",
       "   'terreur,',\n",
       "   'vraiment,'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 73,\n",
       "  'stop_token_fr': 81,\n",
       "  'history_tokens': ['et', \"j'eus\", 'la', 'satisfaction,'],\n",
       "  'span_tokens': ['ou',\n",
       "   'la',\n",
       "   'terreur,',\n",
       "   'vraiment,',\n",
       "   'de',\n",
       "   'regarder',\n",
       "   'par',\n",
       "   'la'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 77,\n",
       "  'stop_token_fr': 85,\n",
       "  'history_tokens': ['ou', 'la', 'terreur,', 'vraiment,'],\n",
       "  'span_tokens': ['de',\n",
       "   'regarder',\n",
       "   'par',\n",
       "   'la',\n",
       "   'fenêtre',\n",
       "   'et',\n",
       "   'de',\n",
       "   'voir'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 81,\n",
       "  'stop_token_fr': 89,\n",
       "  'history_tokens': ['de', 'regarder', 'par', 'la'],\n",
       "  'span_tokens': ['fenêtre',\n",
       "   'et',\n",
       "   'de',\n",
       "   'voir',\n",
       "   'traîner',\n",
       "   'la',\n",
       "   'pauvre',\n",
       "   'créature'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 85,\n",
       "  'stop_token_fr': 92,\n",
       "  'history_tokens': ['fenêtre', 'et', 'de', 'voir'],\n",
       "  'span_tokens': ['traîner',\n",
       "   'la',\n",
       "   'pauvre',\n",
       "   'créature',\n",
       "   'devant',\n",
       "   'la',\n",
       "   'justice,'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 89,\n",
       "  'stop_token_fr': 96,\n",
       "  'history_tokens': ['traîner', 'la', 'pauvre', 'créature'],\n",
       "  'span_tokens': ['devant',\n",
       "   'la',\n",
       "   'justice,',\n",
       "   'qui',\n",
       "   \"l'envoya\",\n",
       "   'sur-le-champ',\n",
       "   '__sent_marker_0'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 92,\n",
       "  'stop_token_fr': 100,\n",
       "  'history_tokens': ['créature', 'devant', 'la', 'justice,'],\n",
       "  'span_tokens': ['qui',\n",
       "   \"l'envoya\",\n",
       "   'sur-le-champ',\n",
       "   '__sent_marker_0',\n",
       "   'à',\n",
       "   'newgate.',\n",
       "   \"nous_n'avions\",\n",
       "   \"n'avions_pas\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 96,\n",
       "  'stop_token_fr': 104,\n",
       "  'history_tokens': ['qui', \"l'envoya\", 'sur-le-champ', '__sent_marker_0'],\n",
       "  'span_tokens': ['à',\n",
       "   'newgate.',\n",
       "   \"nous_n'avions\",\n",
       "   \"n'avions_pas\",\n",
       "   'pas_été',\n",
       "   'été_longtemps',\n",
       "   'longtemps_hors',\n",
       "   'hors_de'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 100,\n",
       "  'stop_token_fr': 108,\n",
       "  'history_tokens': ['à', 'newgate.', \"nous_n'avions\", \"n'avions_pas\"],\n",
       "  'span_tokens': ['pas_été',\n",
       "   'été_longtemps',\n",
       "   'longtemps_hors',\n",
       "   'hors_de',\n",
       "   'de_la',\n",
       "   'la_boutique',\n",
       "   'boutique_que',\n",
       "   'que_le'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 104,\n",
       "  'stop_token_fr': 112,\n",
       "  'history_tokens': ['pas_été', 'été_longtemps', 'longtemps_hors', 'hors_de'],\n",
       "  'span_tokens': ['de_la',\n",
       "   'la_boutique',\n",
       "   'boutique_que',\n",
       "   'que_le',\n",
       "   'le_mercier',\n",
       "   \"mercier_s'aperçut\",\n",
       "   \"s'aperçut_que\",\n",
       "   'que_la'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 108,\n",
       "  'stop_token_fr': 116,\n",
       "  'history_tokens': ['de_la', 'la_boutique', 'boutique_que', 'que_le'],\n",
       "  'span_tokens': ['le_mercier',\n",
       "   \"mercier_s'aperçut\",\n",
       "   \"s'aperçut_que\",\n",
       "   'que_la',\n",
       "   'la_pièce',\n",
       "   \"pièce_d'étoffe\",\n",
       "   \"d'étoffe_avait\",\n",
       "   'avait_disparu,'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 112,\n",
       "  'stop_token_fr': 119,\n",
       "  'history_tokens': ['le_mercier',\n",
       "   \"mercier_s'aperçut\",\n",
       "   \"s'aperçut_que\",\n",
       "   'que_la'],\n",
       "  'span_tokens': ['la_pièce',\n",
       "   \"pièce_d'étoffe\",\n",
       "   \"d'étoffe_avait\",\n",
       "   'avait_disparu,',\n",
       "   'disparu,_et',\n",
       "   'et_envoya',\n",
       "   'envoya_ses'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 120,\n",
       "  'stop': 128,\n",
       "  'start_token_fr': 116,\n",
       "  'stop_token_fr': 123,\n",
       "  'history_tokens': ['la_pièce',\n",
       "   \"pièce_d'étoffe\",\n",
       "   \"d'étoffe_avait\",\n",
       "   'avait_disparu,'],\n",
       "  'span_tokens': ['disparu,_et',\n",
       "   'et_envoya',\n",
       "   'envoya_ses',\n",
       "   'ses_commis',\n",
       "   'commis_qui',\n",
       "   \"qui_d'un\",\n",
       "   \"d'un_côté,\"],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 124,\n",
       "  'stop': 132,\n",
       "  'start_token_fr': 119,\n",
       "  'stop_token_fr': 127,\n",
       "  'history_tokens': ['avait_disparu,',\n",
       "   'disparu,_et',\n",
       "   'et_envoya',\n",
       "   'envoya_ses'],\n",
       "  'span_tokens': ['ses_commis',\n",
       "   'commis_qui',\n",
       "   \"qui_d'un\",\n",
       "   \"d'un_côté,\",\n",
       "   'côté,_qui',\n",
       "   \"qui_d'un\",\n",
       "   \"d'un_autre;\",\n",
       "   'autre;_et'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 128,\n",
       "  'stop': 136,\n",
       "  'start_token_fr': 123,\n",
       "  'stop_token_fr': 131,\n",
       "  'history_tokens': ['ses_commis', 'commis_qui', \"qui_d'un\", \"d'un_côté,\"],\n",
       "  'span_tokens': ['côté,_qui',\n",
       "   \"qui_d'un\",\n",
       "   \"d'un_autre;\",\n",
       "   'autre;_et',\n",
       "   'et_bientôt',\n",
       "   'bientôt_ils',\n",
       "   'ils_eurent',\n",
       "   'eurent_saisi'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 132,\n",
       "  'stop': 140,\n",
       "  'start_token_fr': 127,\n",
       "  'stop_token_fr': 135,\n",
       "  'history_tokens': ['côté,_qui', \"qui_d'un\", \"d'un_autre;\", 'autre;_et'],\n",
       "  'span_tokens': ['et_bientôt',\n",
       "   'bientôt_ils',\n",
       "   'ils_eurent',\n",
       "   'eurent_saisi',\n",
       "   'saisi_la',\n",
       "   'la_femme',\n",
       "   'femme_qui',\n",
       "   'qui_portait'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 136,\n",
       "  'stop': 144,\n",
       "  'start_token_fr': 131,\n",
       "  'stop_token_fr': 139,\n",
       "  'history_tokens': ['et_bientôt',\n",
       "   'bientôt_ils',\n",
       "   'ils_eurent',\n",
       "   'eurent_saisi'],\n",
       "  'span_tokens': ['saisi_la',\n",
       "   'la_femme',\n",
       "   'femme_qui',\n",
       "   'qui_portait',\n",
       "   'portait_la',\n",
       "   'la_pièce,',\n",
       "   'pièce,_et',\n",
       "   'et_trouvèrent'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 140,\n",
       "  'stop': 148,\n",
       "  'start_token_fr': 135,\n",
       "  'stop_token_fr': 142,\n",
       "  'history_tokens': ['saisi_la', 'la_femme', 'femme_qui', 'qui_portait'],\n",
       "  'span_tokens': ['portait_la',\n",
       "   'la_pièce,',\n",
       "   'pièce,_et',\n",
       "   'et_trouvèrent',\n",
       "   'trouvèrent_le',\n",
       "   'le_damas',\n",
       "   'damas_sur'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 144,\n",
       "  'stop': 152,\n",
       "  'start_token_fr': 139,\n",
       "  'stop_token_fr': 146,\n",
       "  'history_tokens': ['portait_la', 'la_pièce,', 'pièce,_et', 'et_trouvèrent'],\n",
       "  'span_tokens': ['trouvèrent_le',\n",
       "   'le_damas',\n",
       "   'damas_sur',\n",
       "   'sur_elle;',\n",
       "   'elle;_pour',\n",
       "   'pour_moi',\n",
       "   'moi_je'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 148,\n",
       "  'stop': 156,\n",
       "  'start_token_fr': 142,\n",
       "  'stop_token_fr': 150,\n",
       "  'history_tokens': ['et_trouvèrent',\n",
       "   'trouvèrent_le',\n",
       "   'le_damas',\n",
       "   'damas_sur'],\n",
       "  'span_tokens': ['sur_elle;',\n",
       "   'elle;_pour',\n",
       "   'pour_moi',\n",
       "   'moi_je',\n",
       "   \"je_m'étais\",\n",
       "   \"m'étais_faufilée\",\n",
       "   'faufilée_par',\n",
       "   'par_chance'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 152,\n",
       "  'stop': 160,\n",
       "  'start_token_fr': 146,\n",
       "  'stop_token_fr': 154,\n",
       "  'history_tokens': ['sur_elle;', 'elle;_pour', 'pour_moi', 'moi_je'],\n",
       "  'span_tokens': [\"je_m'étais\",\n",
       "   \"m'étais_faufilée\",\n",
       "   'faufilée_par',\n",
       "   'par_chance',\n",
       "   'chance_dans',\n",
       "   'dans_une',\n",
       "   'une_maison',\n",
       "   'maison_où'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 156,\n",
       "  'stop': 164,\n",
       "  'start_token_fr': 150,\n",
       "  'stop_token_fr': 158,\n",
       "  'history_tokens': [\"je_m'étais\",\n",
       "   \"m'étais_faufilée\",\n",
       "   'faufilée_par',\n",
       "   'par_chance'],\n",
       "  'span_tokens': ['chance_dans',\n",
       "   'dans_une',\n",
       "   'une_maison',\n",
       "   'maison_où',\n",
       "   'où_il',\n",
       "   'il_y',\n",
       "   'y_avait',\n",
       "   'avait_une'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 160,\n",
       "  'stop': 168,\n",
       "  'start_token_fr': 154,\n",
       "  'stop_token_fr': 162,\n",
       "  'history_tokens': ['chance_dans', 'dans_une', 'une_maison', 'maison_où'],\n",
       "  'span_tokens': ['où_il',\n",
       "   'il_y',\n",
       "   'y_avait',\n",
       "   'avait_une',\n",
       "   'une_chambre',\n",
       "   'chambre_à',\n",
       "   'à_dentelle,',\n",
       "   'dentelle,_au'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 164,\n",
       "  'stop': 172,\n",
       "  'start_token_fr': 158,\n",
       "  'stop_token_fr': 166,\n",
       "  'history_tokens': ['où_il', 'il_y', 'y_avait', 'avait_une'],\n",
       "  'span_tokens': ['une_chambre',\n",
       "   'chambre_à',\n",
       "   'à_dentelle,',\n",
       "   'dentelle,_au',\n",
       "   'au_palier',\n",
       "   'palier_du',\n",
       "   'du_premier',\n",
       "   'premier_escalier;'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 168,\n",
       "  'stop': 176,\n",
       "  'start_token_fr': 162,\n",
       "  'stop_token_fr': 169,\n",
       "  'history_tokens': ['une_chambre',\n",
       "   'chambre_à',\n",
       "   'à_dentelle,',\n",
       "   'dentelle,_au'],\n",
       "  'span_tokens': ['au_palier',\n",
       "   'palier_du',\n",
       "   'du_premier',\n",
       "   'premier_escalier;',\n",
       "   'escalier;_et',\n",
       "   \"et_j'eus\",\n",
       "   \"j'eus_la\"],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 172,\n",
       "  'stop': 180,\n",
       "  'start_token_fr': 166,\n",
       "  'stop_token_fr': 173,\n",
       "  'history_tokens': ['au_palier',\n",
       "   'palier_du',\n",
       "   'du_premier',\n",
       "   'premier_escalier;'],\n",
       "  'span_tokens': ['escalier;_et',\n",
       "   \"et_j'eus\",\n",
       "   \"j'eus_la\",\n",
       "   'la_satisfaction,',\n",
       "   'satisfaction,_ou',\n",
       "   'ou_la',\n",
       "   'la_terreur,'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 176,\n",
       "  'stop': 184,\n",
       "  'start_token_fr': 169,\n",
       "  'stop_token_fr': 177,\n",
       "  'history_tokens': ['premier_escalier;',\n",
       "   'escalier;_et',\n",
       "   \"et_j'eus\",\n",
       "   \"j'eus_la\"],\n",
       "  'span_tokens': ['la_satisfaction,',\n",
       "   'satisfaction,_ou',\n",
       "   'ou_la',\n",
       "   'la_terreur,',\n",
       "   'terreur,_vraiment,',\n",
       "   'vraiment,_de',\n",
       "   'de_regarder',\n",
       "   'regarder_par'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 180,\n",
       "  'stop': 188,\n",
       "  'start_token_fr': 173,\n",
       "  'stop_token_fr': 181,\n",
       "  'history_tokens': ['la_satisfaction,',\n",
       "   'satisfaction,_ou',\n",
       "   'ou_la',\n",
       "   'la_terreur,'],\n",
       "  'span_tokens': ['terreur,_vraiment,',\n",
       "   'vraiment,_de',\n",
       "   'de_regarder',\n",
       "   'regarder_par',\n",
       "   'par_la',\n",
       "   'la_fenêtre',\n",
       "   'fenêtre_et',\n",
       "   'et_de'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 184,\n",
       "  'stop': 192,\n",
       "  'start_token_fr': 177,\n",
       "  'stop_token_fr': 185,\n",
       "  'history_tokens': ['terreur,_vraiment,',\n",
       "   'vraiment,_de',\n",
       "   'de_regarder',\n",
       "   'regarder_par'],\n",
       "  'span_tokens': ['par_la',\n",
       "   'la_fenêtre',\n",
       "   'fenêtre_et',\n",
       "   'et_de',\n",
       "   'de_voir',\n",
       "   'voir_traîner',\n",
       "   'traîner_la',\n",
       "   'la_pauvre'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 188,\n",
       "  'stop': 196,\n",
       "  'start_token_fr': 181,\n",
       "  'stop_token_fr': 189,\n",
       "  'history_tokens': ['par_la', 'la_fenêtre', 'fenêtre_et', 'et_de'],\n",
       "  'span_tokens': ['de_voir',\n",
       "   'voir_traîner',\n",
       "   'traîner_la',\n",
       "   'la_pauvre',\n",
       "   'pauvre_créature',\n",
       "   'créature_devant',\n",
       "   'devant_la',\n",
       "   'la_justice,'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 192,\n",
       "  'stop': 200,\n",
       "  'start_token_fr': 185,\n",
       "  'stop_token_fr': 192,\n",
       "  'history_tokens': ['de_voir', 'voir_traîner', 'traîner_la', 'la_pauvre'],\n",
       "  'span_tokens': ['pauvre_créature',\n",
       "   'créature_devant',\n",
       "   'devant_la',\n",
       "   'la_justice,',\n",
       "   'justice,_qui',\n",
       "   '__sent_marker_1',\n",
       "   \"qui_l'envoya\"],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 196,\n",
       "  'stop': 204,\n",
       "  'start_token_fr': 189,\n",
       "  'stop_token_fr': 196,\n",
       "  'history_tokens': ['pauvre_créature',\n",
       "   'créature_devant',\n",
       "   'devant_la',\n",
       "   'la_justice,'],\n",
       "  'span_tokens': ['justice,_qui',\n",
       "   '__sent_marker_1',\n",
       "   \"qui_l'envoya\",\n",
       "   \"l'envoya_sur-le-champ\",\n",
       "   'sur-le-champ_à',\n",
       "   'à_newgate.',\n",
       "   \"nous_n'avions_pas\"],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 200,\n",
       "  'stop': 208,\n",
       "  'start_token_fr': 192,\n",
       "  'stop_token_fr': 200,\n",
       "  'history_tokens': ['la_justice,',\n",
       "   'justice,_qui',\n",
       "   '__sent_marker_1',\n",
       "   \"qui_l'envoya\"],\n",
       "  'span_tokens': [\"l'envoya_sur-le-champ\",\n",
       "   'sur-le-champ_à',\n",
       "   'à_newgate.',\n",
       "   \"nous_n'avions_pas\",\n",
       "   \"n'avions_pas_été\",\n",
       "   'pas_été_longtemps',\n",
       "   'été_longtemps_hors',\n",
       "   'longtemps_hors_de'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 204,\n",
       "  'stop': 212,\n",
       "  'start_token_fr': 196,\n",
       "  'stop_token_fr': 204,\n",
       "  'history_tokens': [\"l'envoya_sur-le-champ\",\n",
       "   'sur-le-champ_à',\n",
       "   'à_newgate.',\n",
       "   \"nous_n'avions_pas\"],\n",
       "  'span_tokens': [\"n'avions_pas_été\",\n",
       "   'pas_été_longtemps',\n",
       "   'été_longtemps_hors',\n",
       "   'longtemps_hors_de',\n",
       "   'hors_de_la',\n",
       "   'de_la_boutique',\n",
       "   'la_boutique_que',\n",
       "   'boutique_que_le'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 208,\n",
       "  'stop': 216,\n",
       "  'start_token_fr': 200,\n",
       "  'stop_token_fr': 208,\n",
       "  'history_tokens': [\"n'avions_pas_été\",\n",
       "   'pas_été_longtemps',\n",
       "   'été_longtemps_hors',\n",
       "   'longtemps_hors_de'],\n",
       "  'span_tokens': ['hors_de_la',\n",
       "   'de_la_boutique',\n",
       "   'la_boutique_que',\n",
       "   'boutique_que_le',\n",
       "   'que_le_mercier',\n",
       "   \"le_mercier_s'aperçut\",\n",
       "   \"mercier_s'aperçut_que\",\n",
       "   \"s'aperçut_que_la\"],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 212,\n",
       "  'stop': 220,\n",
       "  'start_token_fr': 204,\n",
       "  'stop_token_fr': 212,\n",
       "  'history_tokens': ['hors_de_la',\n",
       "   'de_la_boutique',\n",
       "   'la_boutique_que',\n",
       "   'boutique_que_le'],\n",
       "  'span_tokens': ['que_le_mercier',\n",
       "   \"le_mercier_s'aperçut\",\n",
       "   \"mercier_s'aperçut_que\",\n",
       "   \"s'aperçut_que_la\",\n",
       "   'que_la_pièce',\n",
       "   \"la_pièce_d'étoffe\",\n",
       "   \"pièce_d'étoffe_avait\",\n",
       "   \"d'étoffe_avait_disparu,\"],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 216,\n",
       "  'stop': 224,\n",
       "  'start_token_fr': 208,\n",
       "  'stop_token_fr': 216,\n",
       "  'history_tokens': ['que_le_mercier',\n",
       "   \"le_mercier_s'aperçut\",\n",
       "   \"mercier_s'aperçut_que\",\n",
       "   \"s'aperçut_que_la\"],\n",
       "  'span_tokens': ['que_la_pièce',\n",
       "   \"la_pièce_d'étoffe\",\n",
       "   \"pièce_d'étoffe_avait\",\n",
       "   \"d'étoffe_avait_disparu,\",\n",
       "   'avait_disparu,_et',\n",
       "   'disparu,_et_envoya',\n",
       "   'et_envoya_ses',\n",
       "   'envoya_ses_commis'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 220,\n",
       "  'stop': 228,\n",
       "  'start_token_fr': 212,\n",
       "  'stop_token_fr': 219,\n",
       "  'history_tokens': ['que_la_pièce',\n",
       "   \"la_pièce_d'étoffe\",\n",
       "   \"pièce_d'étoffe_avait\",\n",
       "   \"d'étoffe_avait_disparu,\"],\n",
       "  'span_tokens': ['avait_disparu,_et',\n",
       "   'disparu,_et_envoya',\n",
       "   'et_envoya_ses',\n",
       "   'envoya_ses_commis',\n",
       "   'ses_commis_qui',\n",
       "   \"commis_qui_d'un\",\n",
       "   \"qui_d'un_côté,\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 224,\n",
       "  'stop': 232,\n",
       "  'start_token_fr': 216,\n",
       "  'stop_token_fr': 223,\n",
       "  'history_tokens': ['avait_disparu,_et',\n",
       "   'disparu,_et_envoya',\n",
       "   'et_envoya_ses',\n",
       "   'envoya_ses_commis'],\n",
       "  'span_tokens': ['ses_commis_qui',\n",
       "   \"commis_qui_d'un\",\n",
       "   \"qui_d'un_côté,\",\n",
       "   \"d'un_côté,_qui\",\n",
       "   \"côté,_qui_d'un\",\n",
       "   \"qui_d'un_autre;\",\n",
       "   \"d'un_autre;_et\"],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 228,\n",
       "  'stop': 236,\n",
       "  'start_token_fr': 219,\n",
       "  'stop_token_fr': 227,\n",
       "  'history_tokens': ['envoya_ses_commis',\n",
       "   'ses_commis_qui',\n",
       "   \"commis_qui_d'un\",\n",
       "   \"qui_d'un_côté,\"],\n",
       "  'span_tokens': [\"d'un_côté,_qui\",\n",
       "   \"côté,_qui_d'un\",\n",
       "   \"qui_d'un_autre;\",\n",
       "   \"d'un_autre;_et\",\n",
       "   'autre;_et_bientôt',\n",
       "   'et_bientôt_ils',\n",
       "   'bientôt_ils_eurent',\n",
       "   'ils_eurent_saisi'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 232,\n",
       "  'stop': 240,\n",
       "  'start_token_fr': 223,\n",
       "  'stop_token_fr': 231,\n",
       "  'history_tokens': [\"d'un_côté,_qui\",\n",
       "   \"côté,_qui_d'un\",\n",
       "   \"qui_d'un_autre;\",\n",
       "   \"d'un_autre;_et\"],\n",
       "  'span_tokens': ['autre;_et_bientôt',\n",
       "   'et_bientôt_ils',\n",
       "   'bientôt_ils_eurent',\n",
       "   'ils_eurent_saisi',\n",
       "   'eurent_saisi_la',\n",
       "   'saisi_la_femme',\n",
       "   'la_femme_qui',\n",
       "   'femme_qui_portait'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 236,\n",
       "  'stop': 244,\n",
       "  'start_token_fr': 227,\n",
       "  'stop_token_fr': 235,\n",
       "  'history_tokens': ['autre;_et_bientôt',\n",
       "   'et_bientôt_ils',\n",
       "   'bientôt_ils_eurent',\n",
       "   'ils_eurent_saisi'],\n",
       "  'span_tokens': ['eurent_saisi_la',\n",
       "   'saisi_la_femme',\n",
       "   'la_femme_qui',\n",
       "   'femme_qui_portait',\n",
       "   'qui_portait_la',\n",
       "   'portait_la_pièce,',\n",
       "   'la_pièce,_et',\n",
       "   'pièce,_et_trouvèrent'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 240,\n",
       "  'stop': 248,\n",
       "  'start_token_fr': 231,\n",
       "  'stop_token_fr': 239,\n",
       "  'history_tokens': ['eurent_saisi_la',\n",
       "   'saisi_la_femme',\n",
       "   'la_femme_qui',\n",
       "   'femme_qui_portait'],\n",
       "  'span_tokens': ['qui_portait_la',\n",
       "   'portait_la_pièce,',\n",
       "   'la_pièce,_et',\n",
       "   'pièce,_et_trouvèrent',\n",
       "   'et_trouvèrent_le',\n",
       "   'trouvèrent_le_damas',\n",
       "   'le_damas_sur',\n",
       "   'damas_sur_elle;'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 244,\n",
       "  'stop': 252,\n",
       "  'start_token_fr': 235,\n",
       "  'stop_token_fr': 243,\n",
       "  'history_tokens': ['qui_portait_la',\n",
       "   'portait_la_pièce,',\n",
       "   'la_pièce,_et',\n",
       "   'pièce,_et_trouvèrent'],\n",
       "  'span_tokens': ['et_trouvèrent_le',\n",
       "   'trouvèrent_le_damas',\n",
       "   'le_damas_sur',\n",
       "   'damas_sur_elle;',\n",
       "   'sur_elle;_pour',\n",
       "   'elle;_pour_moi',\n",
       "   'pour_moi_je',\n",
       "   \"moi_je_m'étais\"],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 248,\n",
       "  'stop': 256,\n",
       "  'start_token_fr': 239,\n",
       "  'stop_token_fr': 246,\n",
       "  'history_tokens': ['et_trouvèrent_le',\n",
       "   'trouvèrent_le_damas',\n",
       "   'le_damas_sur',\n",
       "   'damas_sur_elle;'],\n",
       "  'span_tokens': ['sur_elle;_pour',\n",
       "   'elle;_pour_moi',\n",
       "   'pour_moi_je',\n",
       "   \"moi_je_m'étais\",\n",
       "   \"je_m'étais_faufilée\",\n",
       "   \"m'étais_faufilée_par\",\n",
       "   'faufilée_par_chance'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 252,\n",
       "  'stop': 260,\n",
       "  'start_token_fr': 243,\n",
       "  'stop_token_fr': 250,\n",
       "  'history_tokens': ['sur_elle;_pour',\n",
       "   'elle;_pour_moi',\n",
       "   'pour_moi_je',\n",
       "   \"moi_je_m'étais\"],\n",
       "  'span_tokens': [\"je_m'étais_faufilée\",\n",
       "   \"m'étais_faufilée_par\",\n",
       "   'faufilée_par_chance',\n",
       "   'par_chance_dans',\n",
       "   'chance_dans_une',\n",
       "   'dans_une_maison',\n",
       "   'une_maison_où'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 256,\n",
       "  'stop': 264,\n",
       "  'start_token_fr': 246,\n",
       "  'stop_token_fr': 254,\n",
       "  'history_tokens': [\"moi_je_m'étais\",\n",
       "   \"je_m'étais_faufilée\",\n",
       "   \"m'étais_faufilée_par\",\n",
       "   'faufilée_par_chance'],\n",
       "  'span_tokens': ['par_chance_dans',\n",
       "   'chance_dans_une',\n",
       "   'dans_une_maison',\n",
       "   'une_maison_où',\n",
       "   'maison_où_il',\n",
       "   'où_il_y',\n",
       "   'il_y_avait',\n",
       "   'y_avait_une'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 260,\n",
       "  'stop': 268,\n",
       "  'start_token_fr': 250,\n",
       "  'stop_token_fr': 258,\n",
       "  'history_tokens': ['par_chance_dans',\n",
       "   'chance_dans_une',\n",
       "   'dans_une_maison',\n",
       "   'une_maison_où'],\n",
       "  'span_tokens': ['maison_où_il',\n",
       "   'où_il_y',\n",
       "   'il_y_avait',\n",
       "   'y_avait_une',\n",
       "   'avait_une_chambre',\n",
       "   'une_chambre_à',\n",
       "   'chambre_à_dentelle,',\n",
       "   'à_dentelle,_au'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 264,\n",
       "  'stop': 272,\n",
       "  'start_token_fr': 254,\n",
       "  'stop_token_fr': 262,\n",
       "  'history_tokens': ['maison_où_il', 'où_il_y', 'il_y_avait', 'y_avait_une'],\n",
       "  'span_tokens': ['avait_une_chambre',\n",
       "   'une_chambre_à',\n",
       "   'chambre_à_dentelle,',\n",
       "   'à_dentelle,_au',\n",
       "   'dentelle,_au_palier',\n",
       "   'au_palier_du',\n",
       "   'palier_du_premier',\n",
       "   'du_premier_escalier;'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 268,\n",
       "  'stop': 276,\n",
       "  'start_token_fr': 258,\n",
       "  'stop_token_fr': 266,\n",
       "  'history_tokens': ['avait_une_chambre',\n",
       "   'une_chambre_à',\n",
       "   'chambre_à_dentelle,',\n",
       "   'à_dentelle,_au'],\n",
       "  'span_tokens': ['dentelle,_au_palier',\n",
       "   'au_palier_du',\n",
       "   'palier_du_premier',\n",
       "   'du_premier_escalier;',\n",
       "   'premier_escalier;_et',\n",
       "   \"escalier;_et_j'eus\",\n",
       "   \"et_j'eus_la\",\n",
       "   \"j'eus_la_satisfaction,\"],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 272,\n",
       "  'stop': 280,\n",
       "  'start_token_fr': 262,\n",
       "  'stop_token_fr': 270,\n",
       "  'history_tokens': ['dentelle,_au_palier',\n",
       "   'au_palier_du',\n",
       "   'palier_du_premier',\n",
       "   'du_premier_escalier;'],\n",
       "  'span_tokens': ['premier_escalier;_et',\n",
       "   \"escalier;_et_j'eus\",\n",
       "   \"et_j'eus_la\",\n",
       "   \"j'eus_la_satisfaction,\",\n",
       "   'la_satisfaction,_ou',\n",
       "   'satisfaction,_ou_la',\n",
       "   'ou_la_terreur,',\n",
       "   'la_terreur,_vraiment,'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 276,\n",
       "  'stop': 284,\n",
       "  'start_token_fr': 266,\n",
       "  'stop_token_fr': 273,\n",
       "  'history_tokens': ['premier_escalier;_et',\n",
       "   \"escalier;_et_j'eus\",\n",
       "   \"et_j'eus_la\",\n",
       "   \"j'eus_la_satisfaction,\"],\n",
       "  'span_tokens': ['la_satisfaction,_ou',\n",
       "   'satisfaction,_ou_la',\n",
       "   'ou_la_terreur,',\n",
       "   'la_terreur,_vraiment,',\n",
       "   'terreur,_vraiment,_de',\n",
       "   'vraiment,_de_regarder',\n",
       "   'de_regarder_par'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 280,\n",
       "  'stop': 288,\n",
       "  'start_token_fr': 270,\n",
       "  'stop_token_fr': 277,\n",
       "  'history_tokens': ['la_satisfaction,_ou',\n",
       "   'satisfaction,_ou_la',\n",
       "   'ou_la_terreur,',\n",
       "   'la_terreur,_vraiment,'],\n",
       "  'span_tokens': ['terreur,_vraiment,_de',\n",
       "   'vraiment,_de_regarder',\n",
       "   'de_regarder_par',\n",
       "   'regarder_par_la',\n",
       "   'par_la_fenêtre',\n",
       "   'la_fenêtre_et',\n",
       "   'fenêtre_et_de'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 284,\n",
       "  'stop': 292,\n",
       "  'start_token_fr': 273,\n",
       "  'stop_token_fr': 281,\n",
       "  'history_tokens': ['la_terreur,_vraiment,',\n",
       "   'terreur,_vraiment,_de',\n",
       "   'vraiment,_de_regarder',\n",
       "   'de_regarder_par'],\n",
       "  'span_tokens': ['regarder_par_la',\n",
       "   'par_la_fenêtre',\n",
       "   'la_fenêtre_et',\n",
       "   'fenêtre_et_de',\n",
       "   'et_de_voir',\n",
       "   'de_voir_traîner',\n",
       "   'voir_traîner_la',\n",
       "   'traîner_la_pauvre'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 288,\n",
       "  'stop': 296,\n",
       "  'start_token_fr': 277,\n",
       "  'stop_token_fr': 285,\n",
       "  'history_tokens': ['regarder_par_la',\n",
       "   'par_la_fenêtre',\n",
       "   'la_fenêtre_et',\n",
       "   'fenêtre_et_de'],\n",
       "  'span_tokens': ['et_de_voir',\n",
       "   'de_voir_traîner',\n",
       "   'voir_traîner_la',\n",
       "   'traîner_la_pauvre',\n",
       "   'la_pauvre_créature',\n",
       "   'pauvre_créature_devant',\n",
       "   'créature_devant_la',\n",
       "   'devant_la_justice,'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 292,\n",
       "  'stop': 300,\n",
       "  'start_token_fr': 281,\n",
       "  'stop_token_fr': 289,\n",
       "  'history_tokens': ['et_de_voir',\n",
       "   'de_voir_traîner',\n",
       "   'voir_traîner_la',\n",
       "   'traîner_la_pauvre'],\n",
       "  'span_tokens': ['la_pauvre_créature',\n",
       "   'pauvre_créature_devant',\n",
       "   'créature_devant_la',\n",
       "   'devant_la_justice,',\n",
       "   '__sent_marker_2',\n",
       "   'la_justice,_qui',\n",
       "   \"justice,_qui_l'envoya\",\n",
       "   \"qui_l'envoya_sur-le-champ\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 296,\n",
       "  'stop': 304,\n",
       "  'start_token_fr': 285,\n",
       "  'stop_token_fr': 293,\n",
       "  'history_tokens': ['la_pauvre_créature',\n",
       "   'pauvre_créature_devant',\n",
       "   'créature_devant_la',\n",
       "   'devant_la_justice,'],\n",
       "  'span_tokens': ['__sent_marker_2',\n",
       "   'la_justice,_qui',\n",
       "   \"justice,_qui_l'envoya\",\n",
       "   \"qui_l'envoya_sur-le-champ\",\n",
       "   \"l'envoya_sur-le-champ_à\",\n",
       "   'sur-le-champ_à_newgate.',\n",
       "   \"nous_n'avions_pas_été\",\n",
       "   \"n'avions_pas_été_longtemps\"],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 300,\n",
       "  'stop': 308,\n",
       "  'start_token_fr': 289,\n",
       "  'stop_token_fr': 296,\n",
       "  'history_tokens': ['__sent_marker_2',\n",
       "   'la_justice,_qui',\n",
       "   \"justice,_qui_l'envoya\",\n",
       "   \"qui_l'envoya_sur-le-champ\"],\n",
       "  'span_tokens': [\"l'envoya_sur-le-champ_à\",\n",
       "   'sur-le-champ_à_newgate.',\n",
       "   \"nous_n'avions_pas_été\",\n",
       "   \"n'avions_pas_été_longtemps\",\n",
       "   'pas_été_longtemps_hors',\n",
       "   'été_longtemps_hors_de',\n",
       "   'longtemps_hors_de_la'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 304,\n",
       "  'stop': 312,\n",
       "  'start_token_fr': 293,\n",
       "  'stop_token_fr': 300,\n",
       "  'history_tokens': [\"l'envoya_sur-le-champ_à\",\n",
       "   'sur-le-champ_à_newgate.',\n",
       "   \"nous_n'avions_pas_été\",\n",
       "   \"n'avions_pas_été_longtemps\"],\n",
       "  'span_tokens': ['pas_été_longtemps_hors',\n",
       "   'été_longtemps_hors_de',\n",
       "   'longtemps_hors_de_la',\n",
       "   'hors_de_la_boutique',\n",
       "   'de_la_boutique_que',\n",
       "   'la_boutique_que_le',\n",
       "   'boutique_que_le_mercier'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 308,\n",
       "  'stop': 316,\n",
       "  'start_token_fr': 296,\n",
       "  'stop_token_fr': 304,\n",
       "  'history_tokens': [\"n'avions_pas_été_longtemps\",\n",
       "   'pas_été_longtemps_hors',\n",
       "   'été_longtemps_hors_de',\n",
       "   'longtemps_hors_de_la'],\n",
       "  'span_tokens': ['hors_de_la_boutique',\n",
       "   'de_la_boutique_que',\n",
       "   'la_boutique_que_le',\n",
       "   'boutique_que_le_mercier',\n",
       "   \"que_le_mercier_s'aperçut\",\n",
       "   \"le_mercier_s'aperçut_que\",\n",
       "   \"mercier_s'aperçut_que_la\",\n",
       "   \"s'aperçut_que_la_pièce\"],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 312,\n",
       "  'stop': 320,\n",
       "  'start_token_fr': 300,\n",
       "  'stop_token_fr': 308,\n",
       "  'history_tokens': ['hors_de_la_boutique',\n",
       "   'de_la_boutique_que',\n",
       "   'la_boutique_que_le',\n",
       "   'boutique_que_le_mercier'],\n",
       "  'span_tokens': [\"que_le_mercier_s'aperçut\",\n",
       "   \"le_mercier_s'aperçut_que\",\n",
       "   \"mercier_s'aperçut_que_la\",\n",
       "   \"s'aperçut_que_la_pièce\",\n",
       "   \"que_la_pièce_d'étoffe\",\n",
       "   \"la_pièce_d'étoffe_avait\",\n",
       "   \"pièce_d'étoffe_avait_disparu,\",\n",
       "   \"d'étoffe_avait_disparu,_et\"],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 316,\n",
       "  'stop': 324,\n",
       "  'start_token_fr': 304,\n",
       "  'stop_token_fr': 312,\n",
       "  'history_tokens': [\"que_le_mercier_s'aperçut\",\n",
       "   \"le_mercier_s'aperçut_que\",\n",
       "   \"mercier_s'aperçut_que_la\",\n",
       "   \"s'aperçut_que_la_pièce\"],\n",
       "  'span_tokens': [\"que_la_pièce_d'étoffe\",\n",
       "   \"la_pièce_d'étoffe_avait\",\n",
       "   \"pièce_d'étoffe_avait_disparu,\",\n",
       "   \"d'étoffe_avait_disparu,_et\",\n",
       "   'avait_disparu,_et_envoya',\n",
       "   'disparu,_et_envoya_ses',\n",
       "   'et_envoya_ses_commis',\n",
       "   'envoya_ses_commis_qui'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 320,\n",
       "  'stop': 328,\n",
       "  'start_token_fr': 308,\n",
       "  'stop_token_fr': 316,\n",
       "  'history_tokens': [\"que_la_pièce_d'étoffe\",\n",
       "   \"la_pièce_d'étoffe_avait\",\n",
       "   \"pièce_d'étoffe_avait_disparu,\",\n",
       "   \"d'étoffe_avait_disparu,_et\"],\n",
       "  'span_tokens': ['avait_disparu,_et_envoya',\n",
       "   'disparu,_et_envoya_ses',\n",
       "   'et_envoya_ses_commis',\n",
       "   'envoya_ses_commis_qui',\n",
       "   \"ses_commis_qui_d'un\",\n",
       "   \"commis_qui_d'un_côté,\",\n",
       "   \"qui_d'un_côté,_qui\",\n",
       "   \"d'un_côté,_qui_d'un\"],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 324,\n",
       "  'stop': 332,\n",
       "  'start_token_fr': 312,\n",
       "  'stop_token_fr': 320,\n",
       "  'history_tokens': ['avait_disparu,_et_envoya',\n",
       "   'disparu,_et_envoya_ses',\n",
       "   'et_envoya_ses_commis',\n",
       "   'envoya_ses_commis_qui'],\n",
       "  'span_tokens': [\"ses_commis_qui_d'un\",\n",
       "   \"commis_qui_d'un_côté,\",\n",
       "   \"qui_d'un_côté,_qui\",\n",
       "   \"d'un_côté,_qui_d'un\",\n",
       "   \"côté,_qui_d'un_autre;\",\n",
       "   \"qui_d'un_autre;_et\",\n",
       "   \"d'un_autre;_et_bientôt\",\n",
       "   'autre;_et_bientôt_ils'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 328,\n",
       "  'stop': 336,\n",
       "  'start_token_fr': 316,\n",
       "  'stop_token_fr': 323,\n",
       "  'history_tokens': [\"ses_commis_qui_d'un\",\n",
       "   \"commis_qui_d'un_côté,\",\n",
       "   \"qui_d'un_côté,_qui\",\n",
       "   \"d'un_côté,_qui_d'un\"],\n",
       "  'span_tokens': [\"côté,_qui_d'un_autre;\",\n",
       "   \"qui_d'un_autre;_et\",\n",
       "   \"d'un_autre;_et_bientôt\",\n",
       "   'autre;_et_bientôt_ils',\n",
       "   'et_bientôt_ils_eurent',\n",
       "   'bientôt_ils_eurent_saisi',\n",
       "   'ils_eurent_saisi_la'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 332,\n",
       "  'stop': 340,\n",
       "  'start_token_fr': 320,\n",
       "  'stop_token_fr': 327,\n",
       "  'history_tokens': [\"côté,_qui_d'un_autre;\",\n",
       "   \"qui_d'un_autre;_et\",\n",
       "   \"d'un_autre;_et_bientôt\",\n",
       "   'autre;_et_bientôt_ils'],\n",
       "  'span_tokens': ['et_bientôt_ils_eurent',\n",
       "   'bientôt_ils_eurent_saisi',\n",
       "   'ils_eurent_saisi_la',\n",
       "   'eurent_saisi_la_femme',\n",
       "   'saisi_la_femme_qui',\n",
       "   'la_femme_qui_portait',\n",
       "   'femme_qui_portait_la'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 336,\n",
       "  'stop': 344,\n",
       "  'start_token_fr': 323,\n",
       "  'stop_token_fr': 331,\n",
       "  'history_tokens': ['autre;_et_bientôt_ils',\n",
       "   'et_bientôt_ils_eurent',\n",
       "   'bientôt_ils_eurent_saisi',\n",
       "   'ils_eurent_saisi_la'],\n",
       "  'span_tokens': ['eurent_saisi_la_femme',\n",
       "   'saisi_la_femme_qui',\n",
       "   'la_femme_qui_portait',\n",
       "   'femme_qui_portait_la',\n",
       "   'qui_portait_la_pièce,',\n",
       "   'portait_la_pièce,_et',\n",
       "   'la_pièce,_et_trouvèrent',\n",
       "   'pièce,_et_trouvèrent_le'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 340,\n",
       "  'stop': 348,\n",
       "  'start_token_fr': 327,\n",
       "  'stop_token_fr': 335,\n",
       "  'history_tokens': ['eurent_saisi_la_femme',\n",
       "   'saisi_la_femme_qui',\n",
       "   'la_femme_qui_portait',\n",
       "   'femme_qui_portait_la'],\n",
       "  'span_tokens': ['qui_portait_la_pièce,',\n",
       "   'portait_la_pièce,_et',\n",
       "   'la_pièce,_et_trouvèrent',\n",
       "   'pièce,_et_trouvèrent_le',\n",
       "   'et_trouvèrent_le_damas',\n",
       "   'trouvèrent_le_damas_sur',\n",
       "   'le_damas_sur_elle;',\n",
       "   'damas_sur_elle;_pour'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 344,\n",
       "  'stop': 352,\n",
       "  'start_token_fr': 331,\n",
       "  'stop_token_fr': 339,\n",
       "  'history_tokens': ['qui_portait_la_pièce,',\n",
       "   'portait_la_pièce,_et',\n",
       "   'la_pièce,_et_trouvèrent',\n",
       "   'pièce,_et_trouvèrent_le'],\n",
       "  'span_tokens': ['et_trouvèrent_le_damas',\n",
       "   'trouvèrent_le_damas_sur',\n",
       "   'le_damas_sur_elle;',\n",
       "   'damas_sur_elle;_pour',\n",
       "   'sur_elle;_pour_moi',\n",
       "   'elle;_pour_moi_je',\n",
       "   \"pour_moi_je_m'étais\",\n",
       "   \"moi_je_m'étais_faufilée\"],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 348,\n",
       "  'stop': 356,\n",
       "  'start_token_fr': 335,\n",
       "  'stop_token_fr': 343,\n",
       "  'history_tokens': ['et_trouvèrent_le_damas',\n",
       "   'trouvèrent_le_damas_sur',\n",
       "   'le_damas_sur_elle;',\n",
       "   'damas_sur_elle;_pour'],\n",
       "  'span_tokens': ['sur_elle;_pour_moi',\n",
       "   'elle;_pour_moi_je',\n",
       "   \"pour_moi_je_m'étais\",\n",
       "   \"moi_je_m'étais_faufilée\",\n",
       "   \"je_m'étais_faufilée_par\",\n",
       "   \"m'étais_faufilée_par_chance\",\n",
       "   'faufilée_par_chance_dans',\n",
       "   'par_chance_dans_une'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 352,\n",
       "  'stop': 360,\n",
       "  'start_token_fr': 339,\n",
       "  'stop_token_fr': 346,\n",
       "  'history_tokens': ['sur_elle;_pour_moi',\n",
       "   'elle;_pour_moi_je',\n",
       "   \"pour_moi_je_m'étais\",\n",
       "   \"moi_je_m'étais_faufilée\"],\n",
       "  'span_tokens': [\"je_m'étais_faufilée_par\",\n",
       "   \"m'étais_faufilée_par_chance\",\n",
       "   'faufilée_par_chance_dans',\n",
       "   'par_chance_dans_une',\n",
       "   'chance_dans_une_maison',\n",
       "   'dans_une_maison_où',\n",
       "   'une_maison_où_il'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 356,\n",
       "  'stop': 364,\n",
       "  'start_token_fr': 343,\n",
       "  'stop_token_fr': 350,\n",
       "  'history_tokens': [\"je_m'étais_faufilée_par\",\n",
       "   \"m'étais_faufilée_par_chance\",\n",
       "   'faufilée_par_chance_dans',\n",
       "   'par_chance_dans_une'],\n",
       "  'span_tokens': ['chance_dans_une_maison',\n",
       "   'dans_une_maison_où',\n",
       "   'une_maison_où_il',\n",
       "   'maison_où_il_y',\n",
       "   'où_il_y_avait',\n",
       "   'il_y_avait_une',\n",
       "   'y_avait_une_chambre'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 360,\n",
       "  'stop': 368,\n",
       "  'start_token_fr': 346,\n",
       "  'stop_token_fr': 354,\n",
       "  'history_tokens': ['par_chance_dans_une',\n",
       "   'chance_dans_une_maison',\n",
       "   'dans_une_maison_où',\n",
       "   'une_maison_où_il'],\n",
       "  'span_tokens': ['maison_où_il_y',\n",
       "   'où_il_y_avait',\n",
       "   'il_y_avait_une',\n",
       "   'y_avait_une_chambre',\n",
       "   'avait_une_chambre_à',\n",
       "   'une_chambre_à_dentelle,',\n",
       "   'chambre_à_dentelle,_au',\n",
       "   'à_dentelle,_au_palier'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 364,\n",
       "  'stop': 372,\n",
       "  'start_token_fr': 350,\n",
       "  'stop_token_fr': 358,\n",
       "  'history_tokens': ['maison_où_il_y',\n",
       "   'où_il_y_avait',\n",
       "   'il_y_avait_une',\n",
       "   'y_avait_une_chambre'],\n",
       "  'span_tokens': ['avait_une_chambre_à',\n",
       "   'une_chambre_à_dentelle,',\n",
       "   'chambre_à_dentelle,_au',\n",
       "   'à_dentelle,_au_palier',\n",
       "   'dentelle,_au_palier_du',\n",
       "   'au_palier_du_premier',\n",
       "   'palier_du_premier_escalier;',\n",
       "   'du_premier_escalier;_et'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 368,\n",
       "  'stop': 376,\n",
       "  'start_token_fr': 354,\n",
       "  'stop_token_fr': 362,\n",
       "  'history_tokens': ['avait_une_chambre_à',\n",
       "   'une_chambre_à_dentelle,',\n",
       "   'chambre_à_dentelle,_au',\n",
       "   'à_dentelle,_au_palier'],\n",
       "  'span_tokens': ['dentelle,_au_palier_du',\n",
       "   'au_palier_du_premier',\n",
       "   'palier_du_premier_escalier;',\n",
       "   'du_premier_escalier;_et',\n",
       "   \"premier_escalier;_et_j'eus\",\n",
       "   \"escalier;_et_j'eus_la\",\n",
       "   \"et_j'eus_la_satisfaction,\",\n",
       "   \"j'eus_la_satisfaction,_ou\"],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 372,\n",
       "  'stop': 380,\n",
       "  'start_token_fr': 358,\n",
       "  'stop_token_fr': 366,\n",
       "  'history_tokens': ['dentelle,_au_palier_du',\n",
       "   'au_palier_du_premier',\n",
       "   'palier_du_premier_escalier;',\n",
       "   'du_premier_escalier;_et'],\n",
       "  'span_tokens': [\"premier_escalier;_et_j'eus\",\n",
       "   \"escalier;_et_j'eus_la\",\n",
       "   \"et_j'eus_la_satisfaction,\",\n",
       "   \"j'eus_la_satisfaction,_ou\",\n",
       "   'la_satisfaction,_ou_la',\n",
       "   'satisfaction,_ou_la_terreur,',\n",
       "   'ou_la_terreur,_vraiment,',\n",
       "   'la_terreur,_vraiment,_de'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 376,\n",
       "  'stop': 384,\n",
       "  'start_token_fr': 362,\n",
       "  'stop_token_fr': 370,\n",
       "  'history_tokens': [\"premier_escalier;_et_j'eus\",\n",
       "   \"escalier;_et_j'eus_la\",\n",
       "   \"et_j'eus_la_satisfaction,\",\n",
       "   \"j'eus_la_satisfaction,_ou\"],\n",
       "  'span_tokens': ['la_satisfaction,_ou_la',\n",
       "   'satisfaction,_ou_la_terreur,',\n",
       "   'ou_la_terreur,_vraiment,',\n",
       "   'la_terreur,_vraiment,_de',\n",
       "   'terreur,_vraiment,_de_regarder',\n",
       "   'vraiment,_de_regarder_par',\n",
       "   'de_regarder_par_la',\n",
       "   'regarder_par_la_fenêtre'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 380,\n",
       "  'stop': 388,\n",
       "  'start_token_fr': 366,\n",
       "  'stop_token_fr': 373,\n",
       "  'history_tokens': ['la_satisfaction,_ou_la',\n",
       "   'satisfaction,_ou_la_terreur,',\n",
       "   'ou_la_terreur,_vraiment,',\n",
       "   'la_terreur,_vraiment,_de'],\n",
       "  'span_tokens': ['terreur,_vraiment,_de_regarder',\n",
       "   'vraiment,_de_regarder_par',\n",
       "   'de_regarder_par_la',\n",
       "   'regarder_par_la_fenêtre',\n",
       "   'par_la_fenêtre_et',\n",
       "   'la_fenêtre_et_de',\n",
       "   'fenêtre_et_de_voir'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 384,\n",
       "  'stop': 392,\n",
       "  'start_token_fr': 370,\n",
       "  'stop_token_fr': 377,\n",
       "  'history_tokens': ['terreur,_vraiment,_de_regarder',\n",
       "   'vraiment,_de_regarder_par',\n",
       "   'de_regarder_par_la',\n",
       "   'regarder_par_la_fenêtre'],\n",
       "  'span_tokens': ['par_la_fenêtre_et',\n",
       "   'la_fenêtre_et_de',\n",
       "   'fenêtre_et_de_voir',\n",
       "   'et_de_voir_traîner',\n",
       "   'de_voir_traîner_la',\n",
       "   'voir_traîner_la_pauvre',\n",
       "   'traîner_la_pauvre_créature'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 388,\n",
       "  'stop': 396,\n",
       "  'start_token_fr': 373,\n",
       "  'stop_token_fr': 381,\n",
       "  'history_tokens': ['regarder_par_la_fenêtre',\n",
       "   'par_la_fenêtre_et',\n",
       "   'la_fenêtre_et_de',\n",
       "   'fenêtre_et_de_voir'],\n",
       "  'span_tokens': ['et_de_voir_traîner',\n",
       "   'de_voir_traîner_la',\n",
       "   'voir_traîner_la_pauvre',\n",
       "   'traîner_la_pauvre_créature',\n",
       "   'la_pauvre_créature_devant',\n",
       "   'pauvre_créature_devant_la',\n",
       "   'créature_devant_la_justice,',\n",
       "   'devant_la_justice,_qui'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 16,\n",
       "  'start': 392,\n",
       "  'stop': 400,\n",
       "  'start_token_fr': 377,\n",
       "  'stop_token_fr': 385,\n",
       "  'history_tokens': ['et_de_voir_traîner',\n",
       "   'de_voir_traîner_la',\n",
       "   'voir_traîner_la_pauvre',\n",
       "   'traîner_la_pauvre_créature'],\n",
       "  'span_tokens': ['la_pauvre_créature_devant',\n",
       "   'pauvre_créature_devant_la',\n",
       "   'créature_devant_la_justice,',\n",
       "   'devant_la_justice,_qui',\n",
       "   \"la_justice,_qui_l'envoya\",\n",
       "   \"justice,_qui_l'envoya_sur-le-champ\",\n",
       "   \"qui_l'envoya_sur-le-champ_à\",\n",
       "   \"l'envoya_sur-le-champ_à_newgate.\"],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 17,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['pour',\n",
       "   'quelle',\n",
       "   '__sent_marker_0',\n",
       "   'cause',\n",
       "   '__sent_marker_1',\n",
       "   '?',\n",
       "   '__sent_marker_2'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 17,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['pour', 'quelle', '__sent_marker_0', 'cause'],\n",
       "  'span_tokens': ['__sent_marker_1',\n",
       "   '?',\n",
       "   '__sent_marker_2',\n",
       "   'pour_quelle',\n",
       "   'quelle_cause',\n",
       "   'cause_?',\n",
       "   'pour_quelle_cause'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 17,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['cause', '__sent_marker_1', '?', '__sent_marker_2'],\n",
       "  'span_tokens': ['pour_quelle',\n",
       "   'quelle_cause',\n",
       "   'cause_?',\n",
       "   'pour_quelle_cause',\n",
       "   'quelle_cause_?',\n",
       "   'pour_quelle_cause_?',\n",
       "   'pour_dup0'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 17,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['pour_quelle',\n",
       "   'quelle_cause',\n",
       "   'cause_?',\n",
       "   'pour_quelle_cause'],\n",
       "  'span_tokens': ['quelle_cause_?',\n",
       "   'pour_quelle_cause_?',\n",
       "   'pour_dup0',\n",
       "   'quelle_dup1',\n",
       "   'cause_dup2',\n",
       "   '?_dup3',\n",
       "   'pour_dup4'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 17,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['pour_quelle_cause',\n",
       "   'quelle_cause_?',\n",
       "   'pour_quelle_cause_?',\n",
       "   'pour_dup0'],\n",
       "  'span_tokens': ['quelle_dup1',\n",
       "   'cause_dup2',\n",
       "   '?_dup3',\n",
       "   'pour_dup4',\n",
       "   'quelle_dup5',\n",
       "   'cause_dup6',\n",
       "   '?_dup7'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 6,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['mais,', 'malgré', 'leurs', 'efforts,', 'le', 'ballon'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 3,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': ['mais,', 'malgré', 'leurs'],\n",
       "  'span_tokens': ['efforts,',\n",
       "   'le',\n",
       "   'ballon',\n",
       "   \"s'abaissait\",\n",
       "   'toujours,',\n",
       "   'en',\n",
       "   'même'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 6,\n",
       "  'stop_token_fr': 13,\n",
       "  'history_tokens': ['leurs', 'efforts,', 'le', 'ballon'],\n",
       "  'span_tokens': [\"s'abaissait\",\n",
       "   'toujours,',\n",
       "   'en',\n",
       "   'même',\n",
       "   'temps',\n",
       "   \"qu'il\",\n",
       "   'se'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 16,\n",
       "  'history_tokens': [\"s'abaissait\", 'toujours,', 'en', 'même'],\n",
       "  'span_tokens': ['temps', \"qu'il\", 'se', 'déplaçait', 'avec', 'une'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 13,\n",
       "  'stop_token_fr': 19,\n",
       "  'history_tokens': ['même', 'temps', \"qu'il\", 'se'],\n",
       "  'span_tokens': ['déplaçait',\n",
       "   'avec',\n",
       "   'une',\n",
       "   'extrême',\n",
       "   'vitesse,',\n",
       "   'suivant'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 16,\n",
       "  'stop_token_fr': 22,\n",
       "  'history_tokens': ['se', 'déplaçait', 'avec', 'une'],\n",
       "  'span_tokens': ['extrême', 'vitesse,', 'suivant', 'la', 'direction', 'du'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 19,\n",
       "  'stop_token_fr': 26,\n",
       "  'history_tokens': ['une', 'extrême', 'vitesse,', 'suivant'],\n",
       "  'span_tokens': ['la',\n",
       "   'direction',\n",
       "   'du',\n",
       "   'vent,',\n",
       "   \"c'est-à-dire\",\n",
       "   'du',\n",
       "   'nord-est'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 22,\n",
       "  'stop_token_fr': 29,\n",
       "  'history_tokens': ['suivant', 'la', 'direction', 'du'],\n",
       "  'span_tokens': ['vent,',\n",
       "   \"c'est-à-dire\",\n",
       "   'du',\n",
       "   'nord-est',\n",
       "   '__sent_marker_0',\n",
       "   'au',\n",
       "   'sud-ouest.'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 26,\n",
       "  'stop_token_fr': 32,\n",
       "  'history_tokens': ['vent,', \"c'est-à-dire\", 'du', 'nord-est'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'au',\n",
       "   'sud-ouest.',\n",
       "   'mais,_malgré',\n",
       "   'malgré_leurs',\n",
       "   'leurs_efforts,'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 29,\n",
       "  'stop_token_fr': 35,\n",
       "  'history_tokens': ['nord-est', '__sent_marker_0', 'au', 'sud-ouest.'],\n",
       "  'span_tokens': ['mais,_malgré',\n",
       "   'malgré_leurs',\n",
       "   'leurs_efforts,',\n",
       "   'efforts,_le',\n",
       "   'le_ballon',\n",
       "   \"ballon_s'abaissait\"],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 32,\n",
       "  'stop_token_fr': 38,\n",
       "  'history_tokens': ['sud-ouest.',\n",
       "   'mais,_malgré',\n",
       "   'malgré_leurs',\n",
       "   'leurs_efforts,'],\n",
       "  'span_tokens': ['efforts,_le',\n",
       "   'le_ballon',\n",
       "   \"ballon_s'abaissait\",\n",
       "   \"s'abaissait_toujours,\",\n",
       "   'toujours,_en',\n",
       "   'en_même'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 35,\n",
       "  'stop_token_fr': 42,\n",
       "  'history_tokens': ['leurs_efforts,',\n",
       "   'efforts,_le',\n",
       "   'le_ballon',\n",
       "   \"ballon_s'abaissait\"],\n",
       "  'span_tokens': [\"s'abaissait_toujours,\",\n",
       "   'toujours,_en',\n",
       "   'en_même',\n",
       "   'même_temps',\n",
       "   \"temps_qu'il\",\n",
       "   \"qu'il_se\",\n",
       "   'se_déplaçait'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 38,\n",
       "  'stop_token_fr': 45,\n",
       "  'history_tokens': [\"ballon_s'abaissait\",\n",
       "   \"s'abaissait_toujours,\",\n",
       "   'toujours,_en',\n",
       "   'en_même'],\n",
       "  'span_tokens': ['même_temps',\n",
       "   \"temps_qu'il\",\n",
       "   \"qu'il_se\",\n",
       "   'se_déplaçait',\n",
       "   'déplaçait_avec',\n",
       "   'avec_une',\n",
       "   'une_extrême'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 42,\n",
       "  'stop_token_fr': 48,\n",
       "  'history_tokens': ['même_temps', \"temps_qu'il\", \"qu'il_se\", 'se_déplaçait'],\n",
       "  'span_tokens': ['déplaçait_avec',\n",
       "   'avec_une',\n",
       "   'une_extrême',\n",
       "   'extrême_vitesse,',\n",
       "   'vitesse,_suivant',\n",
       "   'suivant_la'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 45,\n",
       "  'stop_token_fr': 51,\n",
       "  'history_tokens': ['se_déplaçait',\n",
       "   'déplaçait_avec',\n",
       "   'avec_une',\n",
       "   'une_extrême'],\n",
       "  'span_tokens': ['extrême_vitesse,',\n",
       "   'vitesse,_suivant',\n",
       "   'suivant_la',\n",
       "   'la_direction',\n",
       "   'direction_du',\n",
       "   'du_vent,'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 48,\n",
       "  'stop_token_fr': 54,\n",
       "  'history_tokens': ['une_extrême',\n",
       "   'extrême_vitesse,',\n",
       "   'vitesse,_suivant',\n",
       "   'suivant_la'],\n",
       "  'span_tokens': ['la_direction',\n",
       "   'direction_du',\n",
       "   'du_vent,',\n",
       "   \"vent,_c'est-à-dire\",\n",
       "   '__sent_marker_1',\n",
       "   \"c'est-à-dire_du\"],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 51,\n",
       "  'stop_token_fr': 58,\n",
       "  'history_tokens': ['suivant_la', 'la_direction', 'direction_du', 'du_vent,'],\n",
       "  'span_tokens': [\"vent,_c'est-à-dire\",\n",
       "   '__sent_marker_1',\n",
       "   \"c'est-à-dire_du\",\n",
       "   'du_nord-est',\n",
       "   'nord-est_au',\n",
       "   'au_sud-ouest.',\n",
       "   'mais,_malgré_leurs'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 54,\n",
       "  'stop_token_fr': 61,\n",
       "  'history_tokens': ['du_vent,',\n",
       "   \"vent,_c'est-à-dire\",\n",
       "   '__sent_marker_1',\n",
       "   \"c'est-à-dire_du\"],\n",
       "  'span_tokens': ['du_nord-est',\n",
       "   'nord-est_au',\n",
       "   'au_sud-ouest.',\n",
       "   'mais,_malgré_leurs',\n",
       "   'malgré_leurs_efforts,',\n",
       "   'leurs_efforts,_le',\n",
       "   'efforts,_le_ballon'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 58,\n",
       "  'stop_token_fr': 64,\n",
       "  'history_tokens': ['du_nord-est',\n",
       "   'nord-est_au',\n",
       "   'au_sud-ouest.',\n",
       "   'mais,_malgré_leurs'],\n",
       "  'span_tokens': ['malgré_leurs_efforts,',\n",
       "   'leurs_efforts,_le',\n",
       "   'efforts,_le_ballon',\n",
       "   \"le_ballon_s'abaissait\",\n",
       "   \"ballon_s'abaissait_toujours,\",\n",
       "   \"s'abaissait_toujours,_en\"],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 61,\n",
       "  'stop_token_fr': 67,\n",
       "  'history_tokens': ['mais,_malgré_leurs',\n",
       "   'malgré_leurs_efforts,',\n",
       "   'leurs_efforts,_le',\n",
       "   'efforts,_le_ballon'],\n",
       "  'span_tokens': [\"le_ballon_s'abaissait\",\n",
       "   \"ballon_s'abaissait_toujours,\",\n",
       "   \"s'abaissait_toujours,_en\",\n",
       "   'toujours,_en_même',\n",
       "   'en_même_temps',\n",
       "   \"même_temps_qu'il\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 64,\n",
       "  'stop_token_fr': 71,\n",
       "  'history_tokens': ['efforts,_le_ballon',\n",
       "   \"le_ballon_s'abaissait\",\n",
       "   \"ballon_s'abaissait_toujours,\",\n",
       "   \"s'abaissait_toujours,_en\"],\n",
       "  'span_tokens': ['toujours,_en_même',\n",
       "   'en_même_temps',\n",
       "   \"même_temps_qu'il\",\n",
       "   \"temps_qu'il_se\",\n",
       "   \"qu'il_se_déplaçait\",\n",
       "   'se_déplaçait_avec',\n",
       "   'déplaçait_avec_une'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 67,\n",
       "  'stop_token_fr': 74,\n",
       "  'history_tokens': [\"s'abaissait_toujours,_en\",\n",
       "   'toujours,_en_même',\n",
       "   'en_même_temps',\n",
       "   \"même_temps_qu'il\"],\n",
       "  'span_tokens': [\"temps_qu'il_se\",\n",
       "   \"qu'il_se_déplaçait\",\n",
       "   'se_déplaçait_avec',\n",
       "   'déplaçait_avec_une',\n",
       "   'avec_une_extrême',\n",
       "   'une_extrême_vitesse,',\n",
       "   'extrême_vitesse,_suivant'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 71,\n",
       "  'stop_token_fr': 77,\n",
       "  'history_tokens': [\"temps_qu'il_se\",\n",
       "   \"qu'il_se_déplaçait\",\n",
       "   'se_déplaçait_avec',\n",
       "   'déplaçait_avec_une'],\n",
       "  'span_tokens': ['avec_une_extrême',\n",
       "   'une_extrême_vitesse,',\n",
       "   'extrême_vitesse,_suivant',\n",
       "   'vitesse,_suivant_la',\n",
       "   'suivant_la_direction',\n",
       "   'la_direction_du'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 74,\n",
       "  'stop_token_fr': 80,\n",
       "  'history_tokens': ['déplaçait_avec_une',\n",
       "   'avec_une_extrême',\n",
       "   'une_extrême_vitesse,',\n",
       "   'extrême_vitesse,_suivant'],\n",
       "  'span_tokens': ['vitesse,_suivant_la',\n",
       "   'suivant_la_direction',\n",
       "   'la_direction_du',\n",
       "   'direction_du_vent,',\n",
       "   '__sent_marker_2',\n",
       "   \"du_vent,_c'est-à-dire\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 77,\n",
       "  'stop_token_fr': 83,\n",
       "  'history_tokens': ['extrême_vitesse,_suivant',\n",
       "   'vitesse,_suivant_la',\n",
       "   'suivant_la_direction',\n",
       "   'la_direction_du'],\n",
       "  'span_tokens': ['direction_du_vent,',\n",
       "   '__sent_marker_2',\n",
       "   \"du_vent,_c'est-à-dire\",\n",
       "   \"vent,_c'est-à-dire_du\",\n",
       "   \"c'est-à-dire_du_nord-est\",\n",
       "   'du_nord-est_au'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 80,\n",
       "  'stop_token_fr': 87,\n",
       "  'history_tokens': ['la_direction_du',\n",
       "   'direction_du_vent,',\n",
       "   '__sent_marker_2',\n",
       "   \"du_vent,_c'est-à-dire\"],\n",
       "  'span_tokens': [\"vent,_c'est-à-dire_du\",\n",
       "   \"c'est-à-dire_du_nord-est\",\n",
       "   'du_nord-est_au',\n",
       "   'nord-est_au_sud-ouest.',\n",
       "   'mais,_malgré_leurs_efforts,',\n",
       "   'malgré_leurs_efforts,_le',\n",
       "   'leurs_efforts,_le_ballon'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 83,\n",
       "  'stop_token_fr': 90,\n",
       "  'history_tokens': [\"du_vent,_c'est-à-dire\",\n",
       "   \"vent,_c'est-à-dire_du\",\n",
       "   \"c'est-à-dire_du_nord-est\",\n",
       "   'du_nord-est_au'],\n",
       "  'span_tokens': ['nord-est_au_sud-ouest.',\n",
       "   'mais,_malgré_leurs_efforts,',\n",
       "   'malgré_leurs_efforts,_le',\n",
       "   'leurs_efforts,_le_ballon',\n",
       "   \"efforts,_le_ballon_s'abaissait\",\n",
       "   \"le_ballon_s'abaissait_toujours,\",\n",
       "   \"ballon_s'abaissait_toujours,_en\"],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 87,\n",
       "  'stop_token_fr': 93,\n",
       "  'history_tokens': ['nord-est_au_sud-ouest.',\n",
       "   'mais,_malgré_leurs_efforts,',\n",
       "   'malgré_leurs_efforts,_le',\n",
       "   'leurs_efforts,_le_ballon'],\n",
       "  'span_tokens': [\"efforts,_le_ballon_s'abaissait\",\n",
       "   \"le_ballon_s'abaissait_toujours,\",\n",
       "   \"ballon_s'abaissait_toujours,_en\",\n",
       "   \"s'abaissait_toujours,_en_même\",\n",
       "   'toujours,_en_même_temps',\n",
       "   \"en_même_temps_qu'il\"],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 90,\n",
       "  'stop_token_fr': 96,\n",
       "  'history_tokens': ['leurs_efforts,_le_ballon',\n",
       "   \"efforts,_le_ballon_s'abaissait\",\n",
       "   \"le_ballon_s'abaissait_toujours,\",\n",
       "   \"ballon_s'abaissait_toujours,_en\"],\n",
       "  'span_tokens': [\"s'abaissait_toujours,_en_même\",\n",
       "   'toujours,_en_même_temps',\n",
       "   \"en_même_temps_qu'il\",\n",
       "   \"même_temps_qu'il_se\",\n",
       "   \"temps_qu'il_se_déplaçait\",\n",
       "   \"qu'il_se_déplaçait_avec\"],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 93,\n",
       "  'stop_token_fr': 99,\n",
       "  'history_tokens': [\"ballon_s'abaissait_toujours,_en\",\n",
       "   \"s'abaissait_toujours,_en_même\",\n",
       "   'toujours,_en_même_temps',\n",
       "   \"en_même_temps_qu'il\"],\n",
       "  'span_tokens': [\"même_temps_qu'il_se\",\n",
       "   \"temps_qu'il_se_déplaçait\",\n",
       "   \"qu'il_se_déplaçait_avec\",\n",
       "   'se_déplaçait_avec_une',\n",
       "   'déplaçait_avec_une_extrême',\n",
       "   'avec_une_extrême_vitesse,'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 120,\n",
       "  'stop': 128,\n",
       "  'start_token_fr': 96,\n",
       "  'stop_token_fr': 103,\n",
       "  'history_tokens': [\"en_même_temps_qu'il\",\n",
       "   \"même_temps_qu'il_se\",\n",
       "   \"temps_qu'il_se_déplaçait\",\n",
       "   \"qu'il_se_déplaçait_avec\"],\n",
       "  'span_tokens': ['se_déplaçait_avec_une',\n",
       "   'déplaçait_avec_une_extrême',\n",
       "   'avec_une_extrême_vitesse,',\n",
       "   'une_extrême_vitesse,_suivant',\n",
       "   'extrême_vitesse,_suivant_la',\n",
       "   'vitesse,_suivant_la_direction',\n",
       "   'suivant_la_direction_du'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 124,\n",
       "  'stop': 132,\n",
       "  'start_token_fr': 99,\n",
       "  'stop_token_fr': 106,\n",
       "  'history_tokens': [\"qu'il_se_déplaçait_avec\",\n",
       "   'se_déplaçait_avec_une',\n",
       "   'déplaçait_avec_une_extrême',\n",
       "   'avec_une_extrême_vitesse,'],\n",
       "  'span_tokens': ['une_extrême_vitesse,_suivant',\n",
       "   'extrême_vitesse,_suivant_la',\n",
       "   'vitesse,_suivant_la_direction',\n",
       "   'suivant_la_direction_du',\n",
       "   'la_direction_du_vent,',\n",
       "   \"direction_du_vent,_c'est-à-dire\",\n",
       "   \"du_vent,_c'est-à-dire_du\"],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 18,\n",
       "  'start': 128,\n",
       "  'stop': 136,\n",
       "  'start_token_fr': 103,\n",
       "  'stop_token_fr': 109,\n",
       "  'history_tokens': ['une_extrême_vitesse,_suivant',\n",
       "   'extrême_vitesse,_suivant_la',\n",
       "   'vitesse,_suivant_la_direction',\n",
       "   'suivant_la_direction_du'],\n",
       "  'span_tokens': ['la_direction_du_vent,',\n",
       "   \"direction_du_vent,_c'est-à-dire\",\n",
       "   \"du_vent,_c'est-à-dire_du\",\n",
       "   \"vent,_c'est-à-dire_du_nord-est\",\n",
       "   \"c'est-à-dire_du_nord-est_au\",\n",
       "   'du_nord-est_au_sud-ouest.'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 19,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['--',\n",
       "   '__sent_marker_0',\n",
       "   '__sent_marker_1',\n",
       "   '__sent_marker_2',\n",
       "   'aramis!',\n",
       "   '--_aramis!',\n",
       "   '--_dup0'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 19,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['--',\n",
       "   '__sent_marker_0',\n",
       "   '__sent_marker_1',\n",
       "   '__sent_marker_2'],\n",
       "  'span_tokens': ['aramis!',\n",
       "   '--_aramis!',\n",
       "   '--_dup0',\n",
       "   'aramis!_dup1',\n",
       "   '--_dup2',\n",
       "   'aramis!_dup3',\n",
       "   '--_dup4'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 19,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['__sent_marker_2', 'aramis!', '--_aramis!', '--_dup0'],\n",
       "  'span_tokens': ['aramis!_dup1',\n",
       "   '--_dup2',\n",
       "   'aramis!_dup3',\n",
       "   '--_dup4',\n",
       "   'aramis!_dup5',\n",
       "   '--_dup6',\n",
       "   'aramis!_dup7'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 19,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['aramis!_dup1', '--_dup2', 'aramis!_dup3', '--_dup4'],\n",
       "  'span_tokens': ['aramis!_dup5',\n",
       "   '--_dup6',\n",
       "   'aramis!_dup7',\n",
       "   '--_dup8',\n",
       "   'aramis!_dup9',\n",
       "   '--_dup10',\n",
       "   'aramis!_dup11'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 19,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['--_dup4', 'aramis!_dup5', '--_dup6', 'aramis!_dup7'],\n",
       "  'span_tokens': ['--_dup8',\n",
       "   'aramis!_dup9',\n",
       "   '--_dup10',\n",
       "   'aramis!_dup11',\n",
       "   '--_dup12',\n",
       "   'aramis!_dup13',\n",
       "   '--_dup14'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 9,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['en',\n",
       "   'grandissant,',\n",
       "   \"l'éducation\",\n",
       "   'anglaise',\n",
       "   'corrigea',\n",
       "   'en',\n",
       "   'grande',\n",
       "   'partie',\n",
       "   'les'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['en', 'grandissant,', \"l'éducation\", 'anglaise'],\n",
       "  'span_tokens': ['corrigea',\n",
       "   'en',\n",
       "   'grande',\n",
       "   'partie',\n",
       "   'les',\n",
       "   'défauts',\n",
       "   'de',\n",
       "   'sa',\n",
       "   'nature',\n",
       "   'trop'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 9,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['en', 'grande', 'partie', 'les'],\n",
       "  'span_tokens': ['défauts',\n",
       "   'de',\n",
       "   'sa',\n",
       "   'nature',\n",
       "   'trop',\n",
       "   'française.',\n",
       "   'quand',\n",
       "   'elle',\n",
       "   'quitta'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 22,\n",
       "  'history_tokens': ['de', 'sa', 'nature', 'trop'],\n",
       "  'span_tokens': ['française.',\n",
       "   'quand',\n",
       "   'elle',\n",
       "   'quitta',\n",
       "   'sa',\n",
       "   'pension,',\n",
       "   'je',\n",
       "   'trouvai'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 18,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['française.', 'quand', 'elle', 'quitta'],\n",
       "  'span_tokens': ['sa',\n",
       "   'pension,',\n",
       "   'je',\n",
       "   'trouvai',\n",
       "   'en',\n",
       "   'elle',\n",
       "   'une',\n",
       "   'compagne',\n",
       "   'agréable'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 22,\n",
       "  'stop_token_fr': 32,\n",
       "  'history_tokens': ['sa', 'pension,', 'je', 'trouvai'],\n",
       "  'span_tokens': ['en',\n",
       "   'elle',\n",
       "   'une',\n",
       "   'compagne',\n",
       "   'agréable',\n",
       "   'et',\n",
       "   'complaisante;',\n",
       "   'elle',\n",
       "   'était',\n",
       "   'docile,'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 36,\n",
       "  'history_tokens': ['elle', 'une', 'compagne', 'agréable'],\n",
       "  'span_tokens': ['et',\n",
       "   'complaisante;',\n",
       "   'elle',\n",
       "   'était',\n",
       "   'docile,',\n",
       "   \"d'un\",\n",
       "   'bon',\n",
       "   'naturel,',\n",
       "   'et'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 32,\n",
       "  'stop_token_fr': 40,\n",
       "  'history_tokens': ['complaisante;', 'elle', 'était', 'docile,'],\n",
       "  'span_tokens': [\"d'un\",\n",
       "   'bon',\n",
       "   'naturel,',\n",
       "   'et',\n",
       "   'avait',\n",
       "   '__sent_marker_0',\n",
       "   \"d'excellents\",\n",
       "   'principes.'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 36,\n",
       "  'stop_token_fr': 45,\n",
       "  'history_tokens': [\"d'un\", 'bon', 'naturel,', 'et'],\n",
       "  'span_tokens': ['avait',\n",
       "   '__sent_marker_0',\n",
       "   \"d'excellents\",\n",
       "   'principes.',\n",
       "   'en_grandissant,',\n",
       "   \"grandissant,_l'éducation\",\n",
       "   \"l'éducation_anglaise\",\n",
       "   'anglaise_corrigea',\n",
       "   'corrigea_en'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 40,\n",
       "  'stop_token_fr': 50,\n",
       "  'history_tokens': ['avait', '__sent_marker_0', \"d'excellents\", 'principes.'],\n",
       "  'span_tokens': ['en_grandissant,',\n",
       "   \"grandissant,_l'éducation\",\n",
       "   \"l'éducation_anglaise\",\n",
       "   'anglaise_corrigea',\n",
       "   'corrigea_en',\n",
       "   'en_grande',\n",
       "   'grande_partie',\n",
       "   'partie_les',\n",
       "   'les_défauts',\n",
       "   'défauts_de'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 45,\n",
       "  'stop_token_fr': 54,\n",
       "  'history_tokens': [\"grandissant,_l'éducation\",\n",
       "   \"l'éducation_anglaise\",\n",
       "   'anglaise_corrigea',\n",
       "   'corrigea_en'],\n",
       "  'span_tokens': ['en_grande',\n",
       "   'grande_partie',\n",
       "   'partie_les',\n",
       "   'les_défauts',\n",
       "   'défauts_de',\n",
       "   'de_sa',\n",
       "   'sa_nature',\n",
       "   'nature_trop',\n",
       "   'trop_française.'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 50,\n",
       "  'stop_token_fr': 58,\n",
       "  'history_tokens': ['grande_partie',\n",
       "   'partie_les',\n",
       "   'les_défauts',\n",
       "   'défauts_de'],\n",
       "  'span_tokens': ['de_sa',\n",
       "   'sa_nature',\n",
       "   'nature_trop',\n",
       "   'trop_française.',\n",
       "   'française._quand',\n",
       "   'quand_elle',\n",
       "   'elle_quitta',\n",
       "   'quitta_sa'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 54,\n",
       "  'stop_token_fr': 63,\n",
       "  'history_tokens': ['de_sa', 'sa_nature', 'nature_trop', 'trop_française.'],\n",
       "  'span_tokens': ['française._quand',\n",
       "   'quand_elle',\n",
       "   'elle_quitta',\n",
       "   'quitta_sa',\n",
       "   'sa_pension,',\n",
       "   'pension,_je',\n",
       "   'je_trouvai',\n",
       "   'trouvai_en',\n",
       "   'en_elle'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 58,\n",
       "  'stop_token_fr': 68,\n",
       "  'history_tokens': ['française._quand',\n",
       "   'quand_elle',\n",
       "   'elle_quitta',\n",
       "   'quitta_sa'],\n",
       "  'span_tokens': ['sa_pension,',\n",
       "   'pension,_je',\n",
       "   'je_trouvai',\n",
       "   'trouvai_en',\n",
       "   'en_elle',\n",
       "   'elle_une',\n",
       "   'une_compagne',\n",
       "   'compagne_agréable',\n",
       "   'agréable_et',\n",
       "   'et_complaisante;'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 63,\n",
       "  'stop_token_fr': 72,\n",
       "  'history_tokens': ['pension,_je', 'je_trouvai', 'trouvai_en', 'en_elle'],\n",
       "  'span_tokens': ['elle_une',\n",
       "   'une_compagne',\n",
       "   'compagne_agréable',\n",
       "   'agréable_et',\n",
       "   'et_complaisante;',\n",
       "   'complaisante;_elle',\n",
       "   'elle_était',\n",
       "   'était_docile,',\n",
       "   \"docile,_d'un\"],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 68,\n",
       "  'stop_token_fr': 76,\n",
       "  'history_tokens': ['une_compagne',\n",
       "   'compagne_agréable',\n",
       "   'agréable_et',\n",
       "   'et_complaisante;'],\n",
       "  'span_tokens': ['complaisante;_elle',\n",
       "   'elle_était',\n",
       "   'était_docile,',\n",
       "   \"docile,_d'un\",\n",
       "   \"d'un_bon\",\n",
       "   'bon_naturel,',\n",
       "   '__sent_marker_1',\n",
       "   'naturel,_et'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 72,\n",
       "  'stop_token_fr': 81,\n",
       "  'history_tokens': ['complaisante;_elle',\n",
       "   'elle_était',\n",
       "   'était_docile,',\n",
       "   \"docile,_d'un\"],\n",
       "  'span_tokens': [\"d'un_bon\",\n",
       "   'bon_naturel,',\n",
       "   '__sent_marker_1',\n",
       "   'naturel,_et',\n",
       "   'et_avait',\n",
       "   \"avait_d'excellents\",\n",
       "   \"d'excellents_principes.\",\n",
       "   \"en_grandissant,_l'éducation\",\n",
       "   \"grandissant,_l'éducation_anglaise\"],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 76,\n",
       "  'stop_token_fr': 86,\n",
       "  'history_tokens': [\"d'un_bon\",\n",
       "   'bon_naturel,',\n",
       "   '__sent_marker_1',\n",
       "   'naturel,_et'],\n",
       "  'span_tokens': ['et_avait',\n",
       "   \"avait_d'excellents\",\n",
       "   \"d'excellents_principes.\",\n",
       "   \"en_grandissant,_l'éducation\",\n",
       "   \"grandissant,_l'éducation_anglaise\",\n",
       "   \"l'éducation_anglaise_corrigea\",\n",
       "   'anglaise_corrigea_en',\n",
       "   'corrigea_en_grande',\n",
       "   'en_grande_partie',\n",
       "   'grande_partie_les'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 81,\n",
       "  'stop_token_fr': 90,\n",
       "  'history_tokens': [\"avait_d'excellents\",\n",
       "   \"d'excellents_principes.\",\n",
       "   \"en_grandissant,_l'éducation\",\n",
       "   \"grandissant,_l'éducation_anglaise\"],\n",
       "  'span_tokens': [\"l'éducation_anglaise_corrigea\",\n",
       "   'anglaise_corrigea_en',\n",
       "   'corrigea_en_grande',\n",
       "   'en_grande_partie',\n",
       "   'grande_partie_les',\n",
       "   'partie_les_défauts',\n",
       "   'les_défauts_de',\n",
       "   'défauts_de_sa',\n",
       "   'de_sa_nature'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 86,\n",
       "  'stop_token_fr': 94,\n",
       "  'history_tokens': ['anglaise_corrigea_en',\n",
       "   'corrigea_en_grande',\n",
       "   'en_grande_partie',\n",
       "   'grande_partie_les'],\n",
       "  'span_tokens': ['partie_les_défauts',\n",
       "   'les_défauts_de',\n",
       "   'défauts_de_sa',\n",
       "   'de_sa_nature',\n",
       "   'sa_nature_trop',\n",
       "   'nature_trop_française.',\n",
       "   'trop_française._quand',\n",
       "   'française._quand_elle'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 90,\n",
       "  'stop_token_fr': 99,\n",
       "  'history_tokens': ['partie_les_défauts',\n",
       "   'les_défauts_de',\n",
       "   'défauts_de_sa',\n",
       "   'de_sa_nature'],\n",
       "  'span_tokens': ['sa_nature_trop',\n",
       "   'nature_trop_française.',\n",
       "   'trop_française._quand',\n",
       "   'française._quand_elle',\n",
       "   'quand_elle_quitta',\n",
       "   'elle_quitta_sa',\n",
       "   'quitta_sa_pension,',\n",
       "   'sa_pension,_je',\n",
       "   'pension,_je_trouvai'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 94,\n",
       "  'stop_token_fr': 104,\n",
       "  'history_tokens': ['sa_nature_trop',\n",
       "   'nature_trop_française.',\n",
       "   'trop_française._quand',\n",
       "   'française._quand_elle'],\n",
       "  'span_tokens': ['quand_elle_quitta',\n",
       "   'elle_quitta_sa',\n",
       "   'quitta_sa_pension,',\n",
       "   'sa_pension,_je',\n",
       "   'pension,_je_trouvai',\n",
       "   'je_trouvai_en',\n",
       "   'trouvai_en_elle',\n",
       "   'en_elle_une',\n",
       "   'elle_une_compagne',\n",
       "   'une_compagne_agréable'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 99,\n",
       "  'stop_token_fr': 108,\n",
       "  'history_tokens': ['elle_quitta_sa',\n",
       "   'quitta_sa_pension,',\n",
       "   'sa_pension,_je',\n",
       "   'pension,_je_trouvai'],\n",
       "  'span_tokens': ['je_trouvai_en',\n",
       "   'trouvai_en_elle',\n",
       "   'en_elle_une',\n",
       "   'elle_une_compagne',\n",
       "   'une_compagne_agréable',\n",
       "   'compagne_agréable_et',\n",
       "   'agréable_et_complaisante;',\n",
       "   'et_complaisante;_elle',\n",
       "   'complaisante;_elle_était'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 104,\n",
       "  'stop_token_fr': 112,\n",
       "  'history_tokens': ['trouvai_en_elle',\n",
       "   'en_elle_une',\n",
       "   'elle_une_compagne',\n",
       "   'une_compagne_agréable'],\n",
       "  'span_tokens': ['compagne_agréable_et',\n",
       "   'agréable_et_complaisante;',\n",
       "   'et_complaisante;_elle',\n",
       "   'complaisante;_elle_était',\n",
       "   'elle_était_docile,',\n",
       "   \"était_docile,_d'un\",\n",
       "   \"docile,_d'un_bon\",\n",
       "   '__sent_marker_2'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 108,\n",
       "  'stop_token_fr': 117,\n",
       "  'history_tokens': ['compagne_agréable_et',\n",
       "   'agréable_et_complaisante;',\n",
       "   'et_complaisante;_elle',\n",
       "   'complaisante;_elle_était'],\n",
       "  'span_tokens': ['elle_était_docile,',\n",
       "   \"était_docile,_d'un\",\n",
       "   \"docile,_d'un_bon\",\n",
       "   '__sent_marker_2',\n",
       "   \"d'un_bon_naturel,\",\n",
       "   'bon_naturel,_et',\n",
       "   'naturel,_et_avait',\n",
       "   \"et_avait_d'excellents\",\n",
       "   \"avait_d'excellents_principes.\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 112,\n",
       "  'stop_token_fr': 122,\n",
       "  'history_tokens': ['elle_était_docile,',\n",
       "   \"était_docile,_d'un\",\n",
       "   \"docile,_d'un_bon\",\n",
       "   '__sent_marker_2'],\n",
       "  'span_tokens': [\"d'un_bon_naturel,\",\n",
       "   'bon_naturel,_et',\n",
       "   'naturel,_et_avait',\n",
       "   \"et_avait_d'excellents\",\n",
       "   \"avait_d'excellents_principes.\",\n",
       "   \"en_grandissant,_l'éducation_anglaise\",\n",
       "   \"grandissant,_l'éducation_anglaise_corrigea\",\n",
       "   \"l'éducation_anglaise_corrigea_en\",\n",
       "   'anglaise_corrigea_en_grande',\n",
       "   'corrigea_en_grande_partie'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 117,\n",
       "  'stop_token_fr': 126,\n",
       "  'history_tokens': ['bon_naturel,_et',\n",
       "   'naturel,_et_avait',\n",
       "   \"et_avait_d'excellents\",\n",
       "   \"avait_d'excellents_principes.\"],\n",
       "  'span_tokens': [\"en_grandissant,_l'éducation_anglaise\",\n",
       "   \"grandissant,_l'éducation_anglaise_corrigea\",\n",
       "   \"l'éducation_anglaise_corrigea_en\",\n",
       "   'anglaise_corrigea_en_grande',\n",
       "   'corrigea_en_grande_partie',\n",
       "   'en_grande_partie_les',\n",
       "   'grande_partie_les_défauts',\n",
       "   'partie_les_défauts_de',\n",
       "   'les_défauts_de_sa'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 122,\n",
       "  'stop_token_fr': 130,\n",
       "  'history_tokens': [\"grandissant,_l'éducation_anglaise_corrigea\",\n",
       "   \"l'éducation_anglaise_corrigea_en\",\n",
       "   'anglaise_corrigea_en_grande',\n",
       "   'corrigea_en_grande_partie'],\n",
       "  'span_tokens': ['en_grande_partie_les',\n",
       "   'grande_partie_les_défauts',\n",
       "   'partie_les_défauts_de',\n",
       "   'les_défauts_de_sa',\n",
       "   'défauts_de_sa_nature',\n",
       "   'de_sa_nature_trop',\n",
       "   'sa_nature_trop_française.',\n",
       "   'nature_trop_française._quand'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 126,\n",
       "  'stop_token_fr': 135,\n",
       "  'history_tokens': ['en_grande_partie_les',\n",
       "   'grande_partie_les_défauts',\n",
       "   'partie_les_défauts_de',\n",
       "   'les_défauts_de_sa'],\n",
       "  'span_tokens': ['défauts_de_sa_nature',\n",
       "   'de_sa_nature_trop',\n",
       "   'sa_nature_trop_française.',\n",
       "   'nature_trop_française._quand',\n",
       "   'trop_française._quand_elle',\n",
       "   'française._quand_elle_quitta',\n",
       "   'quand_elle_quitta_sa',\n",
       "   'elle_quitta_sa_pension,',\n",
       "   'quitta_sa_pension,_je'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 130,\n",
       "  'stop_token_fr': 140,\n",
       "  'history_tokens': ['défauts_de_sa_nature',\n",
       "   'de_sa_nature_trop',\n",
       "   'sa_nature_trop_française.',\n",
       "   'nature_trop_française._quand'],\n",
       "  'span_tokens': ['trop_française._quand_elle',\n",
       "   'française._quand_elle_quitta',\n",
       "   'quand_elle_quitta_sa',\n",
       "   'elle_quitta_sa_pension,',\n",
       "   'quitta_sa_pension,_je',\n",
       "   'sa_pension,_je_trouvai',\n",
       "   'pension,_je_trouvai_en',\n",
       "   'je_trouvai_en_elle',\n",
       "   'trouvai_en_elle_une',\n",
       "   'en_elle_une_compagne'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 120,\n",
       "  'stop': 128,\n",
       "  'start_token_fr': 135,\n",
       "  'stop_token_fr': 144,\n",
       "  'history_tokens': ['française._quand_elle_quitta',\n",
       "   'quand_elle_quitta_sa',\n",
       "   'elle_quitta_sa_pension,',\n",
       "   'quitta_sa_pension,_je'],\n",
       "  'span_tokens': ['sa_pension,_je_trouvai',\n",
       "   'pension,_je_trouvai_en',\n",
       "   'je_trouvai_en_elle',\n",
       "   'trouvai_en_elle_une',\n",
       "   'en_elle_une_compagne',\n",
       "   'elle_une_compagne_agréable',\n",
       "   'une_compagne_agréable_et',\n",
       "   'compagne_agréable_et_complaisante;',\n",
       "   'agréable_et_complaisante;_elle'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 124,\n",
       "  'stop': 132,\n",
       "  'start_token_fr': 140,\n",
       "  'stop_token_fr': 148,\n",
       "  'history_tokens': ['pension,_je_trouvai_en',\n",
       "   'je_trouvai_en_elle',\n",
       "   'trouvai_en_elle_une',\n",
       "   'en_elle_une_compagne'],\n",
       "  'span_tokens': ['elle_une_compagne_agréable',\n",
       "   'une_compagne_agréable_et',\n",
       "   'compagne_agréable_et_complaisante;',\n",
       "   'agréable_et_complaisante;_elle',\n",
       "   'et_complaisante;_elle_était',\n",
       "   'complaisante;_elle_était_docile,',\n",
       "   \"elle_était_docile,_d'un\",\n",
       "   \"était_docile,_d'un_bon\"],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 20,\n",
       "  'start': 128,\n",
       "  'stop': 136,\n",
       "  'start_token_fr': 144,\n",
       "  'stop_token_fr': 153,\n",
       "  'history_tokens': ['elle_une_compagne_agréable',\n",
       "   'une_compagne_agréable_et',\n",
       "   'compagne_agréable_et_complaisante;',\n",
       "   'agréable_et_complaisante;_elle'],\n",
       "  'span_tokens': ['et_complaisante;_elle_était',\n",
       "   'complaisante;_elle_était_docile,',\n",
       "   \"elle_était_docile,_d'un\",\n",
       "   \"était_docile,_d'un_bon\",\n",
       "   \"docile,_d'un_bon_naturel,\",\n",
       "   \"d'un_bon_naturel,_et\",\n",
       "   'bon_naturel,_et_avait',\n",
       "   \"naturel,_et_avait_d'excellents\",\n",
       "   \"et_avait_d'excellents_principes.\"],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 8,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['«un',\n",
       "   'naufragé!',\n",
       "   \"s'écria\",\n",
       "   'pencroff,',\n",
       "   'abandonné',\n",
       "   'à',\n",
       "   'quelques',\n",
       "   'cents'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 12,\n",
       "  'history_tokens': ['«un', 'naufragé!', \"s'écria\", 'pencroff,'],\n",
       "  'span_tokens': ['abandonné',\n",
       "   'à',\n",
       "   'quelques',\n",
       "   'cents',\n",
       "   'milles',\n",
       "   'de',\n",
       "   'nous',\n",
       "   'sur'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 8,\n",
       "  'stop_token_fr': 16,\n",
       "  'history_tokens': ['abandonné', 'à', 'quelques', 'cents'],\n",
       "  'span_tokens': ['milles',\n",
       "   'de',\n",
       "   'nous',\n",
       "   'sur',\n",
       "   'cette',\n",
       "   'île',\n",
       "   '__sent_marker_0',\n",
       "   'tabor!'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 12,\n",
       "  'stop_token_fr': 20,\n",
       "  'history_tokens': ['milles', 'de', 'nous', 'sur'],\n",
       "  'span_tokens': ['cette',\n",
       "   'île',\n",
       "   '__sent_marker_0',\n",
       "   'tabor!',\n",
       "   'ah!',\n",
       "   '«un_naufragé!',\n",
       "   \"naufragé!_s'écria\",\n",
       "   \"s'écria_pencroff,\"],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 16,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['cette', 'île', '__sent_marker_0', 'tabor!'],\n",
       "  'span_tokens': ['ah!',\n",
       "   '«un_naufragé!',\n",
       "   \"naufragé!_s'écria\",\n",
       "   \"s'écria_pencroff,\",\n",
       "   'pencroff,_abandonné',\n",
       "   'abandonné_à',\n",
       "   'à_quelques',\n",
       "   'quelques_cents'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 20,\n",
       "  'stop_token_fr': 28,\n",
       "  'history_tokens': ['ah!',\n",
       "   '«un_naufragé!',\n",
       "   \"naufragé!_s'écria\",\n",
       "   \"s'écria_pencroff,\"],\n",
       "  'span_tokens': ['pencroff,_abandonné',\n",
       "   'abandonné_à',\n",
       "   'à_quelques',\n",
       "   'quelques_cents',\n",
       "   'cents_milles',\n",
       "   'milles_de',\n",
       "   'de_nous',\n",
       "   'nous_sur'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 24,\n",
       "  'stop_token_fr': 33,\n",
       "  'history_tokens': ['pencroff,_abandonné',\n",
       "   'abandonné_à',\n",
       "   'à_quelques',\n",
       "   'quelques_cents'],\n",
       "  'span_tokens': ['cents_milles',\n",
       "   'milles_de',\n",
       "   'de_nous',\n",
       "   'nous_sur',\n",
       "   '__sent_marker_1',\n",
       "   'sur_cette',\n",
       "   'cette_île',\n",
       "   'île_tabor!',\n",
       "   'tabor!_ah!'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 28,\n",
       "  'stop_token_fr': 37,\n",
       "  'history_tokens': ['cents_milles', 'milles_de', 'de_nous', 'nous_sur'],\n",
       "  'span_tokens': ['__sent_marker_1',\n",
       "   'sur_cette',\n",
       "   'cette_île',\n",
       "   'île_tabor!',\n",
       "   'tabor!_ah!',\n",
       "   \"«un_naufragé!_s'écria\",\n",
       "   \"naufragé!_s'écria_pencroff,\",\n",
       "   \"s'écria_pencroff,_abandonné\",\n",
       "   'pencroff,_abandonné_à'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 33,\n",
       "  'stop_token_fr': 41,\n",
       "  'history_tokens': ['sur_cette', 'cette_île', 'île_tabor!', 'tabor!_ah!'],\n",
       "  'span_tokens': [\"«un_naufragé!_s'écria\",\n",
       "   \"naufragé!_s'écria_pencroff,\",\n",
       "   \"s'écria_pencroff,_abandonné\",\n",
       "   'pencroff,_abandonné_à',\n",
       "   'abandonné_à_quelques',\n",
       "   'à_quelques_cents',\n",
       "   'quelques_cents_milles',\n",
       "   'cents_milles_de'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 37,\n",
       "  'stop_token_fr': 45,\n",
       "  'history_tokens': [\"«un_naufragé!_s'écria\",\n",
       "   \"naufragé!_s'écria_pencroff,\",\n",
       "   \"s'écria_pencroff,_abandonné\",\n",
       "   'pencroff,_abandonné_à'],\n",
       "  'span_tokens': ['abandonné_à_quelques',\n",
       "   'à_quelques_cents',\n",
       "   'quelques_cents_milles',\n",
       "   'cents_milles_de',\n",
       "   'milles_de_nous',\n",
       "   '__sent_marker_2',\n",
       "   'de_nous_sur',\n",
       "   'nous_sur_cette'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 41,\n",
       "  'stop_token_fr': 49,\n",
       "  'history_tokens': ['abandonné_à_quelques',\n",
       "   'à_quelques_cents',\n",
       "   'quelques_cents_milles',\n",
       "   'cents_milles_de'],\n",
       "  'span_tokens': ['milles_de_nous',\n",
       "   '__sent_marker_2',\n",
       "   'de_nous_sur',\n",
       "   'nous_sur_cette',\n",
       "   'sur_cette_île',\n",
       "   'cette_île_tabor!',\n",
       "   'île_tabor!_ah!',\n",
       "   \"«un_naufragé!_s'écria_pencroff,\"],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 45,\n",
       "  'stop_token_fr': 53,\n",
       "  'history_tokens': ['milles_de_nous',\n",
       "   '__sent_marker_2',\n",
       "   'de_nous_sur',\n",
       "   'nous_sur_cette'],\n",
       "  'span_tokens': ['sur_cette_île',\n",
       "   'cette_île_tabor!',\n",
       "   'île_tabor!_ah!',\n",
       "   \"«un_naufragé!_s'écria_pencroff,\",\n",
       "   \"naufragé!_s'écria_pencroff,_abandonné\",\n",
       "   \"s'écria_pencroff,_abandonné_à\",\n",
       "   'pencroff,_abandonné_à_quelques',\n",
       "   'abandonné_à_quelques_cents'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 49,\n",
       "  'stop_token_fr': 57,\n",
       "  'history_tokens': ['sur_cette_île',\n",
       "   'cette_île_tabor!',\n",
       "   'île_tabor!_ah!',\n",
       "   \"«un_naufragé!_s'écria_pencroff,\"],\n",
       "  'span_tokens': [\"naufragé!_s'écria_pencroff,_abandonné\",\n",
       "   \"s'écria_pencroff,_abandonné_à\",\n",
       "   'pencroff,_abandonné_à_quelques',\n",
       "   'abandonné_à_quelques_cents',\n",
       "   'à_quelques_cents_milles',\n",
       "   'quelques_cents_milles_de',\n",
       "   'cents_milles_de_nous',\n",
       "   'milles_de_nous_sur'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 21,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 53,\n",
       "  'stop_token_fr': 61,\n",
       "  'history_tokens': [\"naufragé!_s'écria_pencroff,_abandonné\",\n",
       "   \"s'écria_pencroff,_abandonné_à\",\n",
       "   'pencroff,_abandonné_à_quelques',\n",
       "   'abandonné_à_quelques_cents'],\n",
       "  'span_tokens': ['à_quelques_cents_milles',\n",
       "   'quelques_cents_milles_de',\n",
       "   'cents_milles_de_nous',\n",
       "   'milles_de_nous_sur',\n",
       "   'de_nous_sur_cette',\n",
       "   'nous_sur_cette_île',\n",
       "   'sur_cette_île_tabor!',\n",
       "   'cette_île_tabor!_ah!'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['donner', 'à', 'des', 'gens', 'même', 'sans', 'mérite'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 3,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': ['donner', 'à', 'des'],\n",
       "  'span_tokens': ['gens', 'même', 'sans', 'mérite', 'toutes', 'les', 'places'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['gens', 'même', 'sans', 'mérite'],\n",
       "  'span_tokens': ['toutes',\n",
       "   'les',\n",
       "   'places',\n",
       "   'de',\n",
       "   'l’armée,',\n",
       "   'toutes',\n",
       "   'les'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 17,\n",
       "  'history_tokens': ['mérite', 'toutes', 'les', 'places'],\n",
       "  'span_tokens': ['de',\n",
       "   'l’armée,',\n",
       "   'toutes',\n",
       "   'les',\n",
       "   '__sent_marker_0',\n",
       "   'croix',\n",
       "   '?'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 20,\n",
       "  'history_tokens': ['de', 'l’armée,', 'toutes', 'les'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'croix',\n",
       "   '?',\n",
       "   'donner_à',\n",
       "   'à_des',\n",
       "   'des_gens'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 17,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['les', '__sent_marker_0', 'croix', '?'],\n",
       "  'span_tokens': ['donner_à',\n",
       "   'à_des',\n",
       "   'des_gens',\n",
       "   'gens_même',\n",
       "   'même_sans',\n",
       "   'sans_mérite',\n",
       "   'mérite_toutes'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 20,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['?', 'donner_à', 'à_des', 'des_gens'],\n",
       "  'span_tokens': ['gens_même',\n",
       "   'même_sans',\n",
       "   'sans_mérite',\n",
       "   'mérite_toutes',\n",
       "   'toutes_les',\n",
       "   'les_places',\n",
       "   'places_de'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 24,\n",
       "  'stop_token_fr': 30,\n",
       "  'history_tokens': ['gens_même', 'même_sans', 'sans_mérite', 'mérite_toutes'],\n",
       "  'span_tokens': ['toutes_les',\n",
       "   'les_places',\n",
       "   'places_de',\n",
       "   'de_l’armée,',\n",
       "   '__sent_marker_1',\n",
       "   'l’armée,_toutes'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 34,\n",
       "  'history_tokens': ['mérite_toutes', 'toutes_les', 'les_places', 'places_de'],\n",
       "  'span_tokens': ['de_l’armée,',\n",
       "   '__sent_marker_1',\n",
       "   'l’armée,_toutes',\n",
       "   'toutes_les',\n",
       "   'les_croix',\n",
       "   'croix_?',\n",
       "   'donner_à_des'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 30,\n",
       "  'stop_token_fr': 37,\n",
       "  'history_tokens': ['places_de',\n",
       "   'de_l’armée,',\n",
       "   '__sent_marker_1',\n",
       "   'l’armée,_toutes'],\n",
       "  'span_tokens': ['toutes_les',\n",
       "   'les_croix',\n",
       "   'croix_?',\n",
       "   'donner_à_des',\n",
       "   'à_des_gens',\n",
       "   'des_gens_même',\n",
       "   'gens_même_sans'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 34,\n",
       "  'stop_token_fr': 41,\n",
       "  'history_tokens': ['toutes_les', 'les_croix', 'croix_?', 'donner_à_des'],\n",
       "  'span_tokens': ['à_des_gens',\n",
       "   'des_gens_même',\n",
       "   'gens_même_sans',\n",
       "   'même_sans_mérite',\n",
       "   'sans_mérite_toutes',\n",
       "   'mérite_toutes_les',\n",
       "   'toutes_les_places'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 37,\n",
       "  'stop_token_fr': 44,\n",
       "  'history_tokens': ['donner_à_des',\n",
       "   'à_des_gens',\n",
       "   'des_gens_même',\n",
       "   'gens_même_sans'],\n",
       "  'span_tokens': ['même_sans_mérite',\n",
       "   'sans_mérite_toutes',\n",
       "   'mérite_toutes_les',\n",
       "   'toutes_les_places',\n",
       "   'les_places_de',\n",
       "   '__sent_marker_2',\n",
       "   'places_de_l’armée,'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 41,\n",
       "  'stop_token_fr': 47,\n",
       "  'history_tokens': ['même_sans_mérite',\n",
       "   'sans_mérite_toutes',\n",
       "   'mérite_toutes_les',\n",
       "   'toutes_les_places'],\n",
       "  'span_tokens': ['les_places_de',\n",
       "   '__sent_marker_2',\n",
       "   'places_de_l’armée,',\n",
       "   'de_l’armée,_toutes',\n",
       "   'l’armée,_toutes_les',\n",
       "   'toutes_les_croix'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 44,\n",
       "  'stop_token_fr': 51,\n",
       "  'history_tokens': ['toutes_les_places',\n",
       "   'les_places_de',\n",
       "   '__sent_marker_2',\n",
       "   'places_de_l’armée,'],\n",
       "  'span_tokens': ['de_l’armée,_toutes',\n",
       "   'l’armée,_toutes_les',\n",
       "   'toutes_les_croix',\n",
       "   'les_croix_?',\n",
       "   'donner_à_des_gens',\n",
       "   'à_des_gens_même',\n",
       "   'des_gens_même_sans'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 47,\n",
       "  'stop_token_fr': 54,\n",
       "  'history_tokens': ['places_de_l’armée,',\n",
       "   'de_l’armée,_toutes',\n",
       "   'l’armée,_toutes_les',\n",
       "   'toutes_les_croix'],\n",
       "  'span_tokens': ['les_croix_?',\n",
       "   'donner_à_des_gens',\n",
       "   'à_des_gens_même',\n",
       "   'des_gens_même_sans',\n",
       "   'gens_même_sans_mérite',\n",
       "   'même_sans_mérite_toutes',\n",
       "   'sans_mérite_toutes_les'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 51,\n",
       "  'stop_token_fr': 58,\n",
       "  'history_tokens': ['les_croix_?',\n",
       "   'donner_à_des_gens',\n",
       "   'à_des_gens_même',\n",
       "   'des_gens_même_sans'],\n",
       "  'span_tokens': ['gens_même_sans_mérite',\n",
       "   'même_sans_mérite_toutes',\n",
       "   'sans_mérite_toutes_les',\n",
       "   'mérite_toutes_les_places',\n",
       "   'toutes_les_places_de',\n",
       "   'les_places_de_l’armée,',\n",
       "   'places_de_l’armée,_toutes'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 22,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 54,\n",
       "  'stop_token_fr': 61,\n",
       "  'history_tokens': ['des_gens_même_sans',\n",
       "   'gens_même_sans_mérite',\n",
       "   'même_sans_mérite_toutes',\n",
       "   'sans_mérite_toutes_les'],\n",
       "  'span_tokens': ['mérite_toutes_les_places',\n",
       "   'toutes_les_places_de',\n",
       "   'les_places_de_l’armée,',\n",
       "   'places_de_l’armée,_toutes',\n",
       "   'de_l’armée,_toutes_les',\n",
       "   'l’armée,_toutes_les_croix',\n",
       "   'toutes_les_croix_?'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 23,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 9,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['--',\n",
       "   'quand',\n",
       "   'vous',\n",
       "   'voudrez,',\n",
       "   'monsieur,',\n",
       "   'dit',\n",
       "   'athos',\n",
       "   'en',\n",
       "   'se'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 23,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['--', 'quand', 'vous', 'voudrez,'],\n",
       "  'span_tokens': ['monsieur,',\n",
       "   'dit',\n",
       "   'athos',\n",
       "   'en',\n",
       "   'se',\n",
       "   'mettant',\n",
       "   '__sent_marker_0',\n",
       "   'en',\n",
       "   'garde.',\n",
       "   '--_quand'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 23,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 9,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['dit', 'athos', 'en', 'se'],\n",
       "  'span_tokens': ['mettant',\n",
       "   '__sent_marker_0',\n",
       "   'en',\n",
       "   'garde.',\n",
       "   '--_quand',\n",
       "   'quand_vous',\n",
       "   'vous_voudrez,',\n",
       "   'voudrez,_monsieur,',\n",
       "   'monsieur,_dit'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 23,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 22,\n",
       "  'history_tokens': ['__sent_marker_0', 'en', 'garde.', '--_quand'],\n",
       "  'span_tokens': ['quand_vous',\n",
       "   'vous_voudrez,',\n",
       "   'voudrez,_monsieur,',\n",
       "   'monsieur,_dit',\n",
       "   'dit_athos',\n",
       "   'athos_en',\n",
       "   '__sent_marker_1',\n",
       "   'en_se'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 23,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 18,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['quand_vous',\n",
       "   'vous_voudrez,',\n",
       "   'voudrez,_monsieur,',\n",
       "   'monsieur,_dit'],\n",
       "  'span_tokens': ['dit_athos',\n",
       "   'athos_en',\n",
       "   '__sent_marker_1',\n",
       "   'en_se',\n",
       "   'se_mettant',\n",
       "   'mettant_en',\n",
       "   'en_garde.',\n",
       "   '--_quand_vous',\n",
       "   'quand_vous_voudrez,'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 23,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 22,\n",
       "  'stop_token_fr': 32,\n",
       "  'history_tokens': ['dit_athos', 'athos_en', '__sent_marker_1', 'en_se'],\n",
       "  'span_tokens': ['se_mettant',\n",
       "   'mettant_en',\n",
       "   'en_garde.',\n",
       "   '--_quand_vous',\n",
       "   'quand_vous_voudrez,',\n",
       "   'vous_voudrez,_monsieur,',\n",
       "   'voudrez,_monsieur,_dit',\n",
       "   'monsieur,_dit_athos',\n",
       "   '__sent_marker_2',\n",
       "   'dit_athos_en'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 23,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 36,\n",
       "  'history_tokens': ['mettant_en',\n",
       "   'en_garde.',\n",
       "   '--_quand_vous',\n",
       "   'quand_vous_voudrez,'],\n",
       "  'span_tokens': ['vous_voudrez,_monsieur,',\n",
       "   'voudrez,_monsieur,_dit',\n",
       "   'monsieur,_dit_athos',\n",
       "   '__sent_marker_2',\n",
       "   'dit_athos_en',\n",
       "   'athos_en_se',\n",
       "   'en_se_mettant',\n",
       "   'se_mettant_en',\n",
       "   'mettant_en_garde.'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 23,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 32,\n",
       "  'stop_token_fr': 40,\n",
       "  'history_tokens': ['voudrez,_monsieur,_dit',\n",
       "   'monsieur,_dit_athos',\n",
       "   '__sent_marker_2',\n",
       "   'dit_athos_en'],\n",
       "  'span_tokens': ['athos_en_se',\n",
       "   'en_se_mettant',\n",
       "   'se_mettant_en',\n",
       "   'mettant_en_garde.',\n",
       "   '--_quand_vous_voudrez,',\n",
       "   'quand_vous_voudrez,_monsieur,',\n",
       "   'vous_voudrez,_monsieur,_dit',\n",
       "   'voudrez,_monsieur,_dit_athos'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 23,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 36,\n",
       "  'stop_token_fr': 45,\n",
       "  'history_tokens': ['athos_en_se',\n",
       "   'en_se_mettant',\n",
       "   'se_mettant_en',\n",
       "   'mettant_en_garde.'],\n",
       "  'span_tokens': ['--_quand_vous_voudrez,',\n",
       "   'quand_vous_voudrez,_monsieur,',\n",
       "   'vous_voudrez,_monsieur,_dit',\n",
       "   'voudrez,_monsieur,_dit_athos',\n",
       "   'monsieur,_dit_athos_en',\n",
       "   'dit_athos_en_se',\n",
       "   'athos_en_se_mettant',\n",
       "   'en_se_mettant_en',\n",
       "   'se_mettant_en_garde.'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['if,',\n",
       "   'to',\n",
       "   'anticipate',\n",
       "   'everything',\n",
       "   'while',\n",
       "   'he',\n",
       "   'is',\n",
       "   'under',\n",
       "   'lock',\n",
       "   'and'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 5,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['to', 'anticipate', 'everything', 'while'],\n",
       "  'span_tokens': ['he',\n",
       "   'is',\n",
       "   'under',\n",
       "   'lock',\n",
       "   'and',\n",
       "   'key,',\n",
       "   'rassi',\n",
       "   'should',\n",
       "   'grow'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 19,\n",
       "  'history_tokens': ['is', 'under', 'lock', 'and'],\n",
       "  'span_tokens': ['key,',\n",
       "   'rassi',\n",
       "   'should',\n",
       "   'grow',\n",
       "   'too',\n",
       "   'impatient',\n",
       "   'and',\n",
       "   'have',\n",
       "   'me'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['key,', 'rassi', 'should', 'grow'],\n",
       "  'span_tokens': ['too',\n",
       "   'impatient',\n",
       "   'and',\n",
       "   'have',\n",
       "   'me',\n",
       "   'poisoned,',\n",
       "   'fabrizio',\n",
       "   'may',\n",
       "   'run',\n",
       "   'a'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 19,\n",
       "  'stop_token_fr': 29,\n",
       "  'history_tokens': ['impatient', 'and', 'have', 'me'],\n",
       "  'span_tokens': ['poisoned,',\n",
       "   'fabrizio',\n",
       "   'may',\n",
       "   'run',\n",
       "   'a',\n",
       "   '__sent_marker_0',\n",
       "   'certain',\n",
       "   'risk.',\n",
       "   'if,_to',\n",
       "   'to_anticipate'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 24,\n",
       "  'stop_token_fr': 34,\n",
       "  'history_tokens': ['fabrizio', 'may', 'run', 'a'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'certain',\n",
       "   'risk.',\n",
       "   'if,_to',\n",
       "   'to_anticipate',\n",
       "   'anticipate_everything',\n",
       "   'everything_while',\n",
       "   'while_he',\n",
       "   'he_is',\n",
       "   'is_under'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 29,\n",
       "  'stop_token_fr': 38,\n",
       "  'history_tokens': ['certain', 'risk.', 'if,_to', 'to_anticipate'],\n",
       "  'span_tokens': ['anticipate_everything',\n",
       "   'everything_while',\n",
       "   'while_he',\n",
       "   'he_is',\n",
       "   'is_under',\n",
       "   'under_lock',\n",
       "   'lock_and',\n",
       "   'and_key,',\n",
       "   'key,_rassi'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 34,\n",
       "  'stop_token_fr': 43,\n",
       "  'history_tokens': ['everything_while', 'while_he', 'he_is', 'is_under'],\n",
       "  'span_tokens': ['under_lock',\n",
       "   'lock_and',\n",
       "   'and_key,',\n",
       "   'key,_rassi',\n",
       "   'rassi_should',\n",
       "   'should_grow',\n",
       "   'grow_too',\n",
       "   'too_impatient',\n",
       "   'impatient_and'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 38,\n",
       "  'stop_token_fr': 48,\n",
       "  'history_tokens': ['under_lock', 'lock_and', 'and_key,', 'key,_rassi'],\n",
       "  'span_tokens': ['rassi_should',\n",
       "   'should_grow',\n",
       "   'grow_too',\n",
       "   'too_impatient',\n",
       "   'impatient_and',\n",
       "   'and_have',\n",
       "   'have_me',\n",
       "   'me_poisoned,',\n",
       "   'poisoned,_fabrizio',\n",
       "   'fabrizio_may'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 43,\n",
       "  'stop_token_fr': 53,\n",
       "  'history_tokens': ['should_grow',\n",
       "   'grow_too',\n",
       "   'too_impatient',\n",
       "   'impatient_and'],\n",
       "  'span_tokens': ['and_have',\n",
       "   'have_me',\n",
       "   'me_poisoned,',\n",
       "   'poisoned,_fabrizio',\n",
       "   'fabrizio_may',\n",
       "   '__sent_marker_1',\n",
       "   'may_run',\n",
       "   'run_a',\n",
       "   'a_certain',\n",
       "   'certain_risk.'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 48,\n",
       "  'stop_token_fr': 58,\n",
       "  'history_tokens': ['have_me',\n",
       "   'me_poisoned,',\n",
       "   'poisoned,_fabrizio',\n",
       "   'fabrizio_may'],\n",
       "  'span_tokens': ['__sent_marker_1',\n",
       "   'may_run',\n",
       "   'run_a',\n",
       "   'a_certain',\n",
       "   'certain_risk.',\n",
       "   'if,_to_anticipate',\n",
       "   'to_anticipate_everything',\n",
       "   'anticipate_everything_while',\n",
       "   'everything_while_he',\n",
       "   'while_he_is'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 53,\n",
       "  'stop_token_fr': 63,\n",
       "  'history_tokens': ['may_run', 'run_a', 'a_certain', 'certain_risk.'],\n",
       "  'span_tokens': ['if,_to_anticipate',\n",
       "   'to_anticipate_everything',\n",
       "   'anticipate_everything_while',\n",
       "   'everything_while_he',\n",
       "   'while_he_is',\n",
       "   'he_is_under',\n",
       "   'is_under_lock',\n",
       "   'under_lock_and',\n",
       "   'lock_and_key,',\n",
       "   'and_key,_rassi'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 58,\n",
       "  'stop_token_fr': 67,\n",
       "  'history_tokens': ['to_anticipate_everything',\n",
       "   'anticipate_everything_while',\n",
       "   'everything_while_he',\n",
       "   'while_he_is'],\n",
       "  'span_tokens': ['he_is_under',\n",
       "   'is_under_lock',\n",
       "   'under_lock_and',\n",
       "   'lock_and_key,',\n",
       "   'and_key,_rassi',\n",
       "   'key,_rassi_should',\n",
       "   'rassi_should_grow',\n",
       "   'should_grow_too',\n",
       "   'grow_too_impatient'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 63,\n",
       "  'stop_token_fr': 72,\n",
       "  'history_tokens': ['is_under_lock',\n",
       "   'under_lock_and',\n",
       "   'lock_and_key,',\n",
       "   'and_key,_rassi'],\n",
       "  'span_tokens': ['key,_rassi_should',\n",
       "   'rassi_should_grow',\n",
       "   'should_grow_too',\n",
       "   'grow_too_impatient',\n",
       "   'too_impatient_and',\n",
       "   'impatient_and_have',\n",
       "   'and_have_me',\n",
       "   'have_me_poisoned,',\n",
       "   'me_poisoned,_fabrizio'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 67,\n",
       "  'stop_token_fr': 77,\n",
       "  'history_tokens': ['key,_rassi_should',\n",
       "   'rassi_should_grow',\n",
       "   'should_grow_too',\n",
       "   'grow_too_impatient'],\n",
       "  'span_tokens': ['too_impatient_and',\n",
       "   'impatient_and_have',\n",
       "   'and_have_me',\n",
       "   'have_me_poisoned,',\n",
       "   'me_poisoned,_fabrizio',\n",
       "   '__sent_marker_2',\n",
       "   'poisoned,_fabrizio_may',\n",
       "   'fabrizio_may_run',\n",
       "   'may_run_a',\n",
       "   'run_a_certain'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 72,\n",
       "  'stop_token_fr': 82,\n",
       "  'history_tokens': ['impatient_and_have',\n",
       "   'and_have_me',\n",
       "   'have_me_poisoned,',\n",
       "   'me_poisoned,_fabrizio'],\n",
       "  'span_tokens': ['__sent_marker_2',\n",
       "   'poisoned,_fabrizio_may',\n",
       "   'fabrizio_may_run',\n",
       "   'may_run_a',\n",
       "   'run_a_certain',\n",
       "   'a_certain_risk.',\n",
       "   'if,_to_anticipate_everything',\n",
       "   'to_anticipate_everything_while',\n",
       "   'anticipate_everything_while_he',\n",
       "   'everything_while_he_is'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 77,\n",
       "  'stop_token_fr': 87,\n",
       "  'history_tokens': ['poisoned,_fabrizio_may',\n",
       "   'fabrizio_may_run',\n",
       "   'may_run_a',\n",
       "   'run_a_certain'],\n",
       "  'span_tokens': ['a_certain_risk.',\n",
       "   'if,_to_anticipate_everything',\n",
       "   'to_anticipate_everything_while',\n",
       "   'anticipate_everything_while_he',\n",
       "   'everything_while_he_is',\n",
       "   'while_he_is_under',\n",
       "   'he_is_under_lock',\n",
       "   'is_under_lock_and',\n",
       "   'under_lock_and_key,',\n",
       "   'lock_and_key,_rassi'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 82,\n",
       "  'stop_token_fr': 91,\n",
       "  'history_tokens': ['if,_to_anticipate_everything',\n",
       "   'to_anticipate_everything_while',\n",
       "   'anticipate_everything_while_he',\n",
       "   'everything_while_he_is'],\n",
       "  'span_tokens': ['while_he_is_under',\n",
       "   'he_is_under_lock',\n",
       "   'is_under_lock_and',\n",
       "   'under_lock_and_key,',\n",
       "   'lock_and_key,_rassi',\n",
       "   'and_key,_rassi_should',\n",
       "   'key,_rassi_should_grow',\n",
       "   'rassi_should_grow_too',\n",
       "   'should_grow_too_impatient'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 87,\n",
       "  'stop_token_fr': 96,\n",
       "  'history_tokens': ['he_is_under_lock',\n",
       "   'is_under_lock_and',\n",
       "   'under_lock_and_key,',\n",
       "   'lock_and_key,_rassi'],\n",
       "  'span_tokens': ['and_key,_rassi_should',\n",
       "   'key,_rassi_should_grow',\n",
       "   'rassi_should_grow_too',\n",
       "   'should_grow_too_impatient',\n",
       "   'grow_too_impatient_and',\n",
       "   'too_impatient_and_have',\n",
       "   'impatient_and_have_me',\n",
       "   'and_have_me_poisoned,',\n",
       "   'have_me_poisoned,_fabrizio'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 24,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 91,\n",
       "  'stop_token_fr': 101,\n",
       "  'history_tokens': ['and_key,_rassi_should',\n",
       "   'key,_rassi_should_grow',\n",
       "   'rassi_should_grow_too',\n",
       "   'should_grow_too_impatient'],\n",
       "  'span_tokens': ['grow_too_impatient_and',\n",
       "   'too_impatient_and_have',\n",
       "   'impatient_and_have_me',\n",
       "   'and_have_me_poisoned,',\n",
       "   'have_me_poisoned,_fabrizio',\n",
       "   'me_poisoned,_fabrizio_may',\n",
       "   'poisoned,_fabrizio_may_run',\n",
       "   'fabrizio_may_run_a',\n",
       "   'may_run_a_certain',\n",
       "   'run_a_certain_risk.'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['vous',\n",
       "   'avez',\n",
       "   'beau',\n",
       "   'siffler',\n",
       "   'a',\n",
       "   'faire',\n",
       "   'éclater',\n",
       "   'la',\n",
       "   'chaudiere,',\n",
       "   'ils'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 5,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['avez', 'beau', 'siffler', 'a'],\n",
       "  'span_tokens': ['faire',\n",
       "   'éclater',\n",
       "   'la',\n",
       "   'chaudiere,',\n",
       "   'ils',\n",
       "   'ne',\n",
       "   'se',\n",
       "   'dépechent',\n",
       "   'pas'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 19,\n",
       "  'history_tokens': ['éclater', 'la', 'chaudiere,', 'ils'],\n",
       "  'span_tokens': ['ne',\n",
       "   'se',\n",
       "   'dépechent',\n",
       "   'pas',\n",
       "   'pour',\n",
       "   'autant',\n",
       "   'de',\n",
       "   'dégager',\n",
       "   '__sent_marker_0'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['ne', 'se', 'dépechent', 'pas'],\n",
       "  'span_tokens': ['pour',\n",
       "   'autant',\n",
       "   'de',\n",
       "   'dégager',\n",
       "   '__sent_marker_0',\n",
       "   'le',\n",
       "   'passage.',\n",
       "   'vous_avez',\n",
       "   'avez_beau',\n",
       "   'beau_siffler'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 19,\n",
       "  'stop_token_fr': 29,\n",
       "  'history_tokens': ['autant', 'de', 'dégager', '__sent_marker_0'],\n",
       "  'span_tokens': ['le',\n",
       "   'passage.',\n",
       "   'vous_avez',\n",
       "   'avez_beau',\n",
       "   'beau_siffler',\n",
       "   'siffler_a',\n",
       "   'a_faire',\n",
       "   'faire_éclater',\n",
       "   'éclater_la',\n",
       "   'la_chaudiere,'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 24,\n",
       "  'stop_token_fr': 34,\n",
       "  'history_tokens': ['passage.', 'vous_avez', 'avez_beau', 'beau_siffler'],\n",
       "  'span_tokens': ['siffler_a',\n",
       "   'a_faire',\n",
       "   'faire_éclater',\n",
       "   'éclater_la',\n",
       "   'la_chaudiere,',\n",
       "   'chaudiere,_ils',\n",
       "   'ils_ne',\n",
       "   'ne_se',\n",
       "   'se_dépechent',\n",
       "   'dépechent_pas'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 29,\n",
       "  'stop_token_fr': 38,\n",
       "  'history_tokens': ['a_faire',\n",
       "   'faire_éclater',\n",
       "   'éclater_la',\n",
       "   'la_chaudiere,'],\n",
       "  'span_tokens': ['chaudiere,_ils',\n",
       "   'ils_ne',\n",
       "   'ne_se',\n",
       "   'se_dépechent',\n",
       "   'dépechent_pas',\n",
       "   'pas_pour',\n",
       "   'pour_autant',\n",
       "   '__sent_marker_1',\n",
       "   'autant_de'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 34,\n",
       "  'stop_token_fr': 43,\n",
       "  'history_tokens': ['ils_ne', 'ne_se', 'se_dépechent', 'dépechent_pas'],\n",
       "  'span_tokens': ['pas_pour',\n",
       "   'pour_autant',\n",
       "   '__sent_marker_1',\n",
       "   'autant_de',\n",
       "   'de_dégager',\n",
       "   'dégager_le',\n",
       "   'le_passage.',\n",
       "   'vous_avez_beau',\n",
       "   'avez_beau_siffler'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 38,\n",
       "  'stop_token_fr': 48,\n",
       "  'history_tokens': ['pas_pour',\n",
       "   'pour_autant',\n",
       "   '__sent_marker_1',\n",
       "   'autant_de'],\n",
       "  'span_tokens': ['de_dégager',\n",
       "   'dégager_le',\n",
       "   'le_passage.',\n",
       "   'vous_avez_beau',\n",
       "   'avez_beau_siffler',\n",
       "   'beau_siffler_a',\n",
       "   'siffler_a_faire',\n",
       "   'a_faire_éclater',\n",
       "   'faire_éclater_la',\n",
       "   'éclater_la_chaudiere,'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 43,\n",
       "  'stop_token_fr': 53,\n",
       "  'history_tokens': ['dégager_le',\n",
       "   'le_passage.',\n",
       "   'vous_avez_beau',\n",
       "   'avez_beau_siffler'],\n",
       "  'span_tokens': ['beau_siffler_a',\n",
       "   'siffler_a_faire',\n",
       "   'a_faire_éclater',\n",
       "   'faire_éclater_la',\n",
       "   'éclater_la_chaudiere,',\n",
       "   'la_chaudiere,_ils',\n",
       "   'chaudiere,_ils_ne',\n",
       "   'ils_ne_se',\n",
       "   'ne_se_dépechent',\n",
       "   'se_dépechent_pas'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 48,\n",
       "  'stop_token_fr': 58,\n",
       "  'history_tokens': ['siffler_a_faire',\n",
       "   'a_faire_éclater',\n",
       "   'faire_éclater_la',\n",
       "   'éclater_la_chaudiere,'],\n",
       "  'span_tokens': ['la_chaudiere,_ils',\n",
       "   'chaudiere,_ils_ne',\n",
       "   'ils_ne_se',\n",
       "   'ne_se_dépechent',\n",
       "   'se_dépechent_pas',\n",
       "   'dépechent_pas_pour',\n",
       "   '__sent_marker_2',\n",
       "   'pas_pour_autant',\n",
       "   'pour_autant_de',\n",
       "   'autant_de_dégager'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 53,\n",
       "  'stop_token_fr': 63,\n",
       "  'history_tokens': ['chaudiere,_ils_ne',\n",
       "   'ils_ne_se',\n",
       "   'ne_se_dépechent',\n",
       "   'se_dépechent_pas'],\n",
       "  'span_tokens': ['dépechent_pas_pour',\n",
       "   '__sent_marker_2',\n",
       "   'pas_pour_autant',\n",
       "   'pour_autant_de',\n",
       "   'autant_de_dégager',\n",
       "   'de_dégager_le',\n",
       "   'dégager_le_passage.',\n",
       "   'vous_avez_beau_siffler',\n",
       "   'avez_beau_siffler_a',\n",
       "   'beau_siffler_a_faire'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 58,\n",
       "  'stop_token_fr': 67,\n",
       "  'history_tokens': ['__sent_marker_2',\n",
       "   'pas_pour_autant',\n",
       "   'pour_autant_de',\n",
       "   'autant_de_dégager'],\n",
       "  'span_tokens': ['de_dégager_le',\n",
       "   'dégager_le_passage.',\n",
       "   'vous_avez_beau_siffler',\n",
       "   'avez_beau_siffler_a',\n",
       "   'beau_siffler_a_faire',\n",
       "   'siffler_a_faire_éclater',\n",
       "   'a_faire_éclater_la',\n",
       "   'faire_éclater_la_chaudiere,',\n",
       "   'éclater_la_chaudiere,_ils'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 63,\n",
       "  'stop_token_fr': 72,\n",
       "  'history_tokens': ['dégager_le_passage.',\n",
       "   'vous_avez_beau_siffler',\n",
       "   'avez_beau_siffler_a',\n",
       "   'beau_siffler_a_faire'],\n",
       "  'span_tokens': ['siffler_a_faire_éclater',\n",
       "   'a_faire_éclater_la',\n",
       "   'faire_éclater_la_chaudiere,',\n",
       "   'éclater_la_chaudiere,_ils',\n",
       "   'la_chaudiere,_ils_ne',\n",
       "   'chaudiere,_ils_ne_se',\n",
       "   'ils_ne_se_dépechent',\n",
       "   'ne_se_dépechent_pas',\n",
       "   'se_dépechent_pas_pour'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 25,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 67,\n",
       "  'stop_token_fr': 77,\n",
       "  'history_tokens': ['siffler_a_faire_éclater',\n",
       "   'a_faire_éclater_la',\n",
       "   'faire_éclater_la_chaudiere,',\n",
       "   'éclater_la_chaudiere,_ils'],\n",
       "  'span_tokens': ['la_chaudiere,_ils_ne',\n",
       "   'chaudiere,_ils_ne_se',\n",
       "   'ils_ne_se_dépechent',\n",
       "   'ne_se_dépechent_pas',\n",
       "   'se_dépechent_pas_pour',\n",
       "   'dépechent_pas_pour_autant',\n",
       "   'pas_pour_autant_de',\n",
       "   'pour_autant_de_dégager',\n",
       "   'autant_de_dégager_le',\n",
       "   'de_dégager_le_passage.'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 26,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['en',\n",
       "   'sera-t-il',\n",
       "   'de',\n",
       "   'même',\n",
       "   '__sent_marker_0',\n",
       "   'ce',\n",
       "   'soir?'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 26,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 3,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': ['en', 'sera-t-il', 'de'],\n",
       "  'span_tokens': ['même',\n",
       "   '__sent_marker_0',\n",
       "   'ce',\n",
       "   'soir?',\n",
       "   'en_sera-t-il',\n",
       "   '__sent_marker_1',\n",
       "   'sera-t-il_de'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 26,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['même', '__sent_marker_0', 'ce', 'soir?'],\n",
       "  'span_tokens': ['en_sera-t-il',\n",
       "   '__sent_marker_1',\n",
       "   'sera-t-il_de',\n",
       "   'de_même',\n",
       "   'même_ce',\n",
       "   '__sent_marker_2',\n",
       "   'ce_soir?'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 26,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 17,\n",
       "  'history_tokens': ['soir?',\n",
       "   'en_sera-t-il',\n",
       "   '__sent_marker_1',\n",
       "   'sera-t-il_de'],\n",
       "  'span_tokens': ['de_même',\n",
       "   'même_ce',\n",
       "   '__sent_marker_2',\n",
       "   'ce_soir?',\n",
       "   'en_sera-t-il_de',\n",
       "   'sera-t-il_de_même',\n",
       "   'de_même_ce'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 26,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['de_même', 'même_ce', '__sent_marker_2', 'ce_soir?'],\n",
       "  'span_tokens': ['en_sera-t-il_de',\n",
       "   'sera-t-il_de_même',\n",
       "   'de_même_ce',\n",
       "   'même_ce_soir?',\n",
       "   'en_sera-t-il_de_même',\n",
       "   'sera-t-il_de_même_ce',\n",
       "   'de_même_ce_soir?'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 26,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 17,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['ce_soir?',\n",
       "   'en_sera-t-il_de',\n",
       "   'sera-t-il_de_même',\n",
       "   'de_même_ce'],\n",
       "  'span_tokens': ['même_ce_soir?',\n",
       "   'en_sera-t-il_de_même',\n",
       "   'sera-t-il_de_même_ce',\n",
       "   'de_même_ce_soir?',\n",
       "   'en_dup0',\n",
       "   'sera-t-il_dup1',\n",
       "   'de_dup2'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['–', 'son', 'oncle', 'a', 'dix', 'ou', 'douze'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['–', 'son', 'oncle', 'a'],\n",
       "  'span_tokens': ['dix', 'ou', 'douze', 'ans', 'de', 'service', 'dans'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['a', 'dix', 'ou', 'douze'],\n",
       "  'span_tokens': ['ans', 'de', 'service', 'dans', 'ce', 'salon,', 'sans'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['ans', 'de', 'service', 'dans'],\n",
       "  'span_tokens': ['ce', 'salon,', 'sans', 'quoi', 'je', 'le', 'ferais'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['dans', 'ce', 'salon,', 'sans'],\n",
       "  'span_tokens': ['quoi',\n",
       "   'je',\n",
       "   'le',\n",
       "   'ferais',\n",
       "   'chasser',\n",
       "   '__sent_marker_0',\n",
       "   'à'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 18,\n",
       "  'stop_token_fr': 25,\n",
       "  'history_tokens': ['quoi', 'je', 'le', 'ferais'],\n",
       "  'span_tokens': ['chasser',\n",
       "   '__sent_marker_0',\n",
       "   'à',\n",
       "   'l’instant.',\n",
       "   '–_son',\n",
       "   'son_oncle',\n",
       "   'oncle_a'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 21,\n",
       "  'stop_token_fr': 28,\n",
       "  'history_tokens': ['ferais', 'chasser', '__sent_marker_0', 'à'],\n",
       "  'span_tokens': ['l’instant.',\n",
       "   '–_son',\n",
       "   'son_oncle',\n",
       "   'oncle_a',\n",
       "   'a_dix',\n",
       "   'dix_ou',\n",
       "   'ou_douze'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 25,\n",
       "  'stop_token_fr': 32,\n",
       "  'history_tokens': ['l’instant.', '–_son', 'son_oncle', 'oncle_a'],\n",
       "  'span_tokens': ['a_dix',\n",
       "   'dix_ou',\n",
       "   'ou_douze',\n",
       "   'douze_ans',\n",
       "   'ans_de',\n",
       "   'de_service',\n",
       "   'service_dans'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 28,\n",
       "  'stop_token_fr': 35,\n",
       "  'history_tokens': ['oncle_a', 'a_dix', 'dix_ou', 'ou_douze'],\n",
       "  'span_tokens': ['douze_ans',\n",
       "   'ans_de',\n",
       "   'de_service',\n",
       "   'service_dans',\n",
       "   'dans_ce',\n",
       "   'ce_salon,',\n",
       "   'salon,_sans'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 32,\n",
       "  'stop_token_fr': 39,\n",
       "  'history_tokens': ['douze_ans', 'ans_de', 'de_service', 'service_dans'],\n",
       "  'span_tokens': ['dans_ce',\n",
       "   'ce_salon,',\n",
       "   'salon,_sans',\n",
       "   'sans_quoi',\n",
       "   'quoi_je',\n",
       "   'je_le',\n",
       "   '__sent_marker_1'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 35,\n",
       "  'stop_token_fr': 42,\n",
       "  'history_tokens': ['service_dans', 'dans_ce', 'ce_salon,', 'salon,_sans'],\n",
       "  'span_tokens': ['sans_quoi',\n",
       "   'quoi_je',\n",
       "   'je_le',\n",
       "   '__sent_marker_1',\n",
       "   'le_ferais',\n",
       "   'ferais_chasser',\n",
       "   'chasser_à'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 39,\n",
       "  'stop_token_fr': 46,\n",
       "  'history_tokens': ['sans_quoi', 'quoi_je', 'je_le', '__sent_marker_1'],\n",
       "  'span_tokens': ['le_ferais',\n",
       "   'ferais_chasser',\n",
       "   'chasser_à',\n",
       "   'à_l’instant.',\n",
       "   '–_son_oncle',\n",
       "   'son_oncle_a',\n",
       "   'oncle_a_dix'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 42,\n",
       "  'stop_token_fr': 49,\n",
       "  'history_tokens': ['__sent_marker_1',\n",
       "   'le_ferais',\n",
       "   'ferais_chasser',\n",
       "   'chasser_à'],\n",
       "  'span_tokens': ['à_l’instant.',\n",
       "   '–_son_oncle',\n",
       "   'son_oncle_a',\n",
       "   'oncle_a_dix',\n",
       "   'a_dix_ou',\n",
       "   'dix_ou_douze',\n",
       "   'ou_douze_ans'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 46,\n",
       "  'stop_token_fr': 53,\n",
       "  'history_tokens': ['à_l’instant.',\n",
       "   '–_son_oncle',\n",
       "   'son_oncle_a',\n",
       "   'oncle_a_dix'],\n",
       "  'span_tokens': ['a_dix_ou',\n",
       "   'dix_ou_douze',\n",
       "   'ou_douze_ans',\n",
       "   'douze_ans_de',\n",
       "   'ans_de_service',\n",
       "   'de_service_dans',\n",
       "   'service_dans_ce'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 49,\n",
       "  'stop_token_fr': 56,\n",
       "  'history_tokens': ['oncle_a_dix',\n",
       "   'a_dix_ou',\n",
       "   'dix_ou_douze',\n",
       "   'ou_douze_ans'],\n",
       "  'span_tokens': ['douze_ans_de',\n",
       "   'ans_de_service',\n",
       "   'de_service_dans',\n",
       "   'service_dans_ce',\n",
       "   'dans_ce_salon,',\n",
       "   'ce_salon,_sans',\n",
       "   'salon,_sans_quoi'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 53,\n",
       "  'stop_token_fr': 60,\n",
       "  'history_tokens': ['douze_ans_de',\n",
       "   'ans_de_service',\n",
       "   'de_service_dans',\n",
       "   'service_dans_ce'],\n",
       "  'span_tokens': ['dans_ce_salon,',\n",
       "   'ce_salon,_sans',\n",
       "   'salon,_sans_quoi',\n",
       "   'sans_quoi_je',\n",
       "   '__sent_marker_2',\n",
       "   'quoi_je_le',\n",
       "   'je_le_ferais'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 56,\n",
       "  'stop_token_fr': 63,\n",
       "  'history_tokens': ['service_dans_ce',\n",
       "   'dans_ce_salon,',\n",
       "   'ce_salon,_sans',\n",
       "   'salon,_sans_quoi'],\n",
       "  'span_tokens': ['sans_quoi_je',\n",
       "   '__sent_marker_2',\n",
       "   'quoi_je_le',\n",
       "   'je_le_ferais',\n",
       "   'le_ferais_chasser',\n",
       "   'ferais_chasser_à',\n",
       "   'chasser_à_l’instant.'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 60,\n",
       "  'stop_token_fr': 67,\n",
       "  'history_tokens': ['sans_quoi_je',\n",
       "   '__sent_marker_2',\n",
       "   'quoi_je_le',\n",
       "   'je_le_ferais'],\n",
       "  'span_tokens': ['le_ferais_chasser',\n",
       "   'ferais_chasser_à',\n",
       "   'chasser_à_l’instant.',\n",
       "   '–_son_oncle_a',\n",
       "   'son_oncle_a_dix',\n",
       "   'oncle_a_dix_ou',\n",
       "   'a_dix_ou_douze'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 63,\n",
       "  'stop_token_fr': 70,\n",
       "  'history_tokens': ['je_le_ferais',\n",
       "   'le_ferais_chasser',\n",
       "   'ferais_chasser_à',\n",
       "   'chasser_à_l’instant.'],\n",
       "  'span_tokens': ['–_son_oncle_a',\n",
       "   'son_oncle_a_dix',\n",
       "   'oncle_a_dix_ou',\n",
       "   'a_dix_ou_douze',\n",
       "   'dix_ou_douze_ans',\n",
       "   'ou_douze_ans_de',\n",
       "   'douze_ans_de_service'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 67,\n",
       "  'stop_token_fr': 74,\n",
       "  'history_tokens': ['–_son_oncle_a',\n",
       "   'son_oncle_a_dix',\n",
       "   'oncle_a_dix_ou',\n",
       "   'a_dix_ou_douze'],\n",
       "  'span_tokens': ['dix_ou_douze_ans',\n",
       "   'ou_douze_ans_de',\n",
       "   'douze_ans_de_service',\n",
       "   'ans_de_service_dans',\n",
       "   'de_service_dans_ce',\n",
       "   'service_dans_ce_salon,',\n",
       "   'dans_ce_salon,_sans'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 70,\n",
       "  'stop_token_fr': 77,\n",
       "  'history_tokens': ['a_dix_ou_douze',\n",
       "   'dix_ou_douze_ans',\n",
       "   'ou_douze_ans_de',\n",
       "   'douze_ans_de_service'],\n",
       "  'span_tokens': ['ans_de_service_dans',\n",
       "   'de_service_dans_ce',\n",
       "   'service_dans_ce_salon,',\n",
       "   'dans_ce_salon,_sans',\n",
       "   'ce_salon,_sans_quoi',\n",
       "   'salon,_sans_quoi_je',\n",
       "   'sans_quoi_je_le'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 27,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 74,\n",
       "  'stop_token_fr': 81,\n",
       "  'history_tokens': ['ans_de_service_dans',\n",
       "   'de_service_dans_ce',\n",
       "   'service_dans_ce_salon,',\n",
       "   'dans_ce_salon,_sans'],\n",
       "  'span_tokens': ['ce_salon,_sans_quoi',\n",
       "   'salon,_sans_quoi_je',\n",
       "   'sans_quoi_je_le',\n",
       "   'quoi_je_le_ferais',\n",
       "   'je_le_ferais_chasser',\n",
       "   'le_ferais_chasser_à',\n",
       "   'ferais_chasser_à_l’instant.'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 28,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 9,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['je',\n",
       "   'vais',\n",
       "   'lui',\n",
       "   'dire',\n",
       "   'ce',\n",
       "   'que',\n",
       "   '__sent_marker_0',\n",
       "   'j’en',\n",
       "   'pense.'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 28,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 13,\n",
       "  'history_tokens': ['je', 'vais', 'lui', 'dire'],\n",
       "  'span_tokens': ['ce',\n",
       "   'que',\n",
       "   '__sent_marker_0',\n",
       "   'j’en',\n",
       "   'pense.',\n",
       "   'je_vais',\n",
       "   'vais_lui',\n",
       "   'lui_dire',\n",
       "   '__sent_marker_1'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 28,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 9,\n",
       "  'stop_token_fr': 17,\n",
       "  'history_tokens': ['que', '__sent_marker_0', 'j’en', 'pense.'],\n",
       "  'span_tokens': ['je_vais',\n",
       "   'vais_lui',\n",
       "   'lui_dire',\n",
       "   '__sent_marker_1',\n",
       "   'dire_ce',\n",
       "   'ce_que',\n",
       "   'que_j’en',\n",
       "   'j’en_pense.'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 28,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 13,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['je_vais', 'vais_lui', 'lui_dire', '__sent_marker_1'],\n",
       "  'span_tokens': ['dire_ce',\n",
       "   'ce_que',\n",
       "   'que_j’en',\n",
       "   'j’en_pense.',\n",
       "   'je_vais_lui',\n",
       "   '__sent_marker_2',\n",
       "   'vais_lui_dire',\n",
       "   'lui_dire_ce'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 28,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 17,\n",
       "  'stop_token_fr': 26,\n",
       "  'history_tokens': ['dire_ce', 'ce_que', 'que_j’en', 'j’en_pense.'],\n",
       "  'span_tokens': ['je_vais_lui',\n",
       "   '__sent_marker_2',\n",
       "   'vais_lui_dire',\n",
       "   'lui_dire_ce',\n",
       "   'dire_ce_que',\n",
       "   'ce_que_j’en',\n",
       "   'que_j’en_pense.',\n",
       "   'je_vais_lui_dire',\n",
       "   'vais_lui_dire_ce'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 12,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['après',\n",
       "   'ces',\n",
       "   'exclamations,',\n",
       "   'on',\n",
       "   'remit',\n",
       "   'tout',\n",
       "   'en',\n",
       "   'état.',\n",
       "   'lorsque',\n",
       "   'je',\n",
       "   'descendis',\n",
       "   'pour'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 6,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['exclamations,', 'on', 'remit', 'tout'],\n",
       "  'span_tokens': ['en',\n",
       "   'état.',\n",
       "   'lorsque',\n",
       "   'je',\n",
       "   'descendis',\n",
       "   'pour',\n",
       "   'dîner,',\n",
       "   'la',\n",
       "   'porte',\n",
       "   'de',\n",
       "   'la',\n",
       "   'chambre'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 12,\n",
       "  'stop_token_fr': 23,\n",
       "  'history_tokens': ['lorsque', 'je', 'descendis', 'pour'],\n",
       "  'span_tokens': ['dîner,',\n",
       "   'la',\n",
       "   'porte',\n",
       "   'de',\n",
       "   'la',\n",
       "   'chambre',\n",
       "   'était',\n",
       "   'ouverte',\n",
       "   'et',\n",
       "   'je',\n",
       "   'vis'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 18,\n",
       "  'stop_token_fr': 29,\n",
       "  'history_tokens': ['porte', 'de', 'la', 'chambre'],\n",
       "  'span_tokens': ['était',\n",
       "   'ouverte',\n",
       "   'et',\n",
       "   'je',\n",
       "   'vis',\n",
       "   'que',\n",
       "   'le',\n",
       "   'dégât',\n",
       "   'avait',\n",
       "   'été',\n",
       "   'réparé;'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 23,\n",
       "  'stop_token_fr': 35,\n",
       "  'history_tokens': ['ouverte', 'et', 'je', 'vis'],\n",
       "  'span_tokens': ['que',\n",
       "   'le',\n",
       "   'dégât',\n",
       "   'avait',\n",
       "   'été',\n",
       "   'réparé;',\n",
       "   'le',\n",
       "   'lit',\n",
       "   'seul',\n",
       "   'restait',\n",
       "   'encore',\n",
       "   'dépouillé'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 29,\n",
       "  'stop_token_fr': 41,\n",
       "  'history_tokens': ['dégât', 'avait', 'été', 'réparé;'],\n",
       "  'span_tokens': ['le',\n",
       "   'lit',\n",
       "   'seul',\n",
       "   'restait',\n",
       "   'encore',\n",
       "   'dépouillé',\n",
       "   'de',\n",
       "   'ses',\n",
       "   'rideaux;',\n",
       "   'leah',\n",
       "   'était',\n",
       "   'occupée'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 35,\n",
       "  'stop_token_fr': 47,\n",
       "  'history_tokens': ['seul', 'restait', 'encore', 'dépouillé'],\n",
       "  'span_tokens': ['de',\n",
       "   'ses',\n",
       "   'rideaux;',\n",
       "   'leah',\n",
       "   'était',\n",
       "   'occupée',\n",
       "   'à',\n",
       "   'laver',\n",
       "   'le',\n",
       "   'bord',\n",
       "   'des',\n",
       "   'fenêtres'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 41,\n",
       "  'stop_token_fr': 53,\n",
       "  'history_tokens': ['rideaux;', 'leah', 'était', 'occupée'],\n",
       "  'span_tokens': ['à',\n",
       "   'laver',\n",
       "   'le',\n",
       "   'bord',\n",
       "   'des',\n",
       "   'fenêtres',\n",
       "   'noirci',\n",
       "   'par',\n",
       "   'la',\n",
       "   'fumée;',\n",
       "   'je',\n",
       "   \"m'avançai\"],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 47,\n",
       "  'stop_token_fr': 58,\n",
       "  'history_tokens': ['le', 'bord', 'des', 'fenêtres'],\n",
       "  'span_tokens': ['noirci',\n",
       "   'par',\n",
       "   'la',\n",
       "   'fumée;',\n",
       "   'je',\n",
       "   \"m'avançai\",\n",
       "   'pour',\n",
       "   'lui',\n",
       "   'parler,',\n",
       "   'car',\n",
       "   'je'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 53,\n",
       "  'stop_token_fr': 64,\n",
       "  'history_tokens': ['la', 'fumée;', 'je', \"m'avançai\"],\n",
       "  'span_tokens': ['pour',\n",
       "   'lui',\n",
       "   'parler,',\n",
       "   'car',\n",
       "   'je',\n",
       "   'désirais',\n",
       "   'connaître',\n",
       "   \"l'explication\",\n",
       "   'donnée',\n",
       "   'par',\n",
       "   'm.'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 58,\n",
       "  'stop_token_fr': 70,\n",
       "  'history_tokens': ['lui', 'parler,', 'car', 'je'],\n",
       "  'span_tokens': ['désirais',\n",
       "   'connaître',\n",
       "   \"l'explication\",\n",
       "   'donnée',\n",
       "   'par',\n",
       "   'm.',\n",
       "   'rochester;',\n",
       "   'mais',\n",
       "   'en',\n",
       "   'approchant',\n",
       "   \"j'aperçus\",\n",
       "   'une'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 64,\n",
       "  'stop_token_fr': 76,\n",
       "  'history_tokens': [\"l'explication\", 'donnée', 'par', 'm.'],\n",
       "  'span_tokens': ['rochester;',\n",
       "   'mais',\n",
       "   'en',\n",
       "   'approchant',\n",
       "   \"j'aperçus\",\n",
       "   'une',\n",
       "   'seconde',\n",
       "   'personne:',\n",
       "   'elle',\n",
       "   'était',\n",
       "   'assise',\n",
       "   'près'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 70,\n",
       "  'stop_token_fr': 82,\n",
       "  'history_tokens': ['en', 'approchant', \"j'aperçus\", 'une'],\n",
       "  'span_tokens': ['seconde',\n",
       "   'personne:',\n",
       "   'elle',\n",
       "   'était',\n",
       "   'assise',\n",
       "   'près',\n",
       "   'du',\n",
       "   'lit,',\n",
       "   'et',\n",
       "   'occupée',\n",
       "   'à',\n",
       "   'coudre'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 76,\n",
       "  'stop_token_fr': 88,\n",
       "  'history_tokens': ['elle', 'était', 'assise', 'près'],\n",
       "  'span_tokens': ['du',\n",
       "   'lit,',\n",
       "   'et',\n",
       "   'occupée',\n",
       "   'à',\n",
       "   'coudre',\n",
       "   'des',\n",
       "   'anneaux',\n",
       "   'à',\n",
       "   '__sent_marker_0',\n",
       "   'des',\n",
       "   'rideaux.'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 82,\n",
       "  'stop_token_fr': 94,\n",
       "  'history_tokens': ['et', 'occupée', 'à', 'coudre'],\n",
       "  'span_tokens': ['des',\n",
       "   'anneaux',\n",
       "   'à',\n",
       "   '__sent_marker_0',\n",
       "   'des',\n",
       "   'rideaux.',\n",
       "   'après_ces',\n",
       "   'ces_exclamations,',\n",
       "   'exclamations,_on',\n",
       "   'on_remit',\n",
       "   'remit_tout',\n",
       "   'tout_en'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 88,\n",
       "  'stop_token_fr': 99,\n",
       "  'history_tokens': ['à', '__sent_marker_0', 'des', 'rideaux.'],\n",
       "  'span_tokens': ['après_ces',\n",
       "   'ces_exclamations,',\n",
       "   'exclamations,_on',\n",
       "   'on_remit',\n",
       "   'remit_tout',\n",
       "   'tout_en',\n",
       "   'en_état.',\n",
       "   'état._lorsque',\n",
       "   'lorsque_je',\n",
       "   'je_descendis',\n",
       "   'descendis_pour'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 94,\n",
       "  'stop_token_fr': 105,\n",
       "  'history_tokens': ['exclamations,_on', 'on_remit', 'remit_tout', 'tout_en'],\n",
       "  'span_tokens': ['en_état.',\n",
       "   'état._lorsque',\n",
       "   'lorsque_je',\n",
       "   'je_descendis',\n",
       "   'descendis_pour',\n",
       "   'pour_dîner,',\n",
       "   'dîner,_la',\n",
       "   'la_porte',\n",
       "   'porte_de',\n",
       "   'de_la',\n",
       "   'la_chambre'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 99,\n",
       "  'stop_token_fr': 111,\n",
       "  'history_tokens': ['état._lorsque',\n",
       "   'lorsque_je',\n",
       "   'je_descendis',\n",
       "   'descendis_pour'],\n",
       "  'span_tokens': ['pour_dîner,',\n",
       "   'dîner,_la',\n",
       "   'la_porte',\n",
       "   'porte_de',\n",
       "   'de_la',\n",
       "   'la_chambre',\n",
       "   'chambre_était',\n",
       "   'était_ouverte',\n",
       "   'ouverte_et',\n",
       "   'et_je',\n",
       "   'je_vis',\n",
       "   'vis_que'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 105,\n",
       "  'stop_token_fr': 117,\n",
       "  'history_tokens': ['la_porte', 'porte_de', 'de_la', 'la_chambre'],\n",
       "  'span_tokens': ['chambre_était',\n",
       "   'était_ouverte',\n",
       "   'ouverte_et',\n",
       "   'et_je',\n",
       "   'je_vis',\n",
       "   'vis_que',\n",
       "   'que_le',\n",
       "   'le_dégât',\n",
       "   'dégât_avait',\n",
       "   'avait_été',\n",
       "   'été_réparé;',\n",
       "   'réparé;_le'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 111,\n",
       "  'stop_token_fr': 123,\n",
       "  'history_tokens': ['ouverte_et', 'et_je', 'je_vis', 'vis_que'],\n",
       "  'span_tokens': ['que_le',\n",
       "   'le_dégât',\n",
       "   'dégât_avait',\n",
       "   'avait_été',\n",
       "   'été_réparé;',\n",
       "   'réparé;_le',\n",
       "   'le_lit',\n",
       "   'lit_seul',\n",
       "   'seul_restait',\n",
       "   'restait_encore',\n",
       "   'encore_dépouillé',\n",
       "   'dépouillé_de'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 117,\n",
       "  'stop_token_fr': 129,\n",
       "  'history_tokens': ['dégât_avait', 'avait_été', 'été_réparé;', 'réparé;_le'],\n",
       "  'span_tokens': ['le_lit',\n",
       "   'lit_seul',\n",
       "   'seul_restait',\n",
       "   'restait_encore',\n",
       "   'encore_dépouillé',\n",
       "   'dépouillé_de',\n",
       "   'de_ses',\n",
       "   'ses_rideaux;',\n",
       "   'rideaux;_leah',\n",
       "   'leah_était',\n",
       "   'était_occupée',\n",
       "   'occupée_à'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 123,\n",
       "  'stop_token_fr': 134,\n",
       "  'history_tokens': ['seul_restait',\n",
       "   'restait_encore',\n",
       "   'encore_dépouillé',\n",
       "   'dépouillé_de'],\n",
       "  'span_tokens': ['de_ses',\n",
       "   'ses_rideaux;',\n",
       "   'rideaux;_leah',\n",
       "   'leah_était',\n",
       "   'était_occupée',\n",
       "   'occupée_à',\n",
       "   'à_laver',\n",
       "   'laver_le',\n",
       "   'le_bord',\n",
       "   'bord_des',\n",
       "   'des_fenêtres'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 129,\n",
       "  'stop_token_fr': 140,\n",
       "  'history_tokens': ['rideaux;_leah',\n",
       "   'leah_était',\n",
       "   'était_occupée',\n",
       "   'occupée_à'],\n",
       "  'span_tokens': ['à_laver',\n",
       "   'laver_le',\n",
       "   'le_bord',\n",
       "   'bord_des',\n",
       "   'des_fenêtres',\n",
       "   'fenêtres_noirci',\n",
       "   'noirci_par',\n",
       "   'par_la',\n",
       "   'la_fumée;',\n",
       "   'fumée;_je',\n",
       "   \"je_m'avançai\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 134,\n",
       "  'stop_token_fr': 146,\n",
       "  'history_tokens': ['laver_le', 'le_bord', 'bord_des', 'des_fenêtres'],\n",
       "  'span_tokens': ['fenêtres_noirci',\n",
       "   'noirci_par',\n",
       "   'par_la',\n",
       "   'la_fumée;',\n",
       "   'fumée;_je',\n",
       "   \"je_m'avançai\",\n",
       "   \"m'avançai_pour\",\n",
       "   'pour_lui',\n",
       "   'lui_parler,',\n",
       "   'parler,_car',\n",
       "   'car_je',\n",
       "   'je_désirais'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 140,\n",
       "  'stop_token_fr': 152,\n",
       "  'history_tokens': ['par_la', 'la_fumée;', 'fumée;_je', \"je_m'avançai\"],\n",
       "  'span_tokens': [\"m'avançai_pour\",\n",
       "   'pour_lui',\n",
       "   'lui_parler,',\n",
       "   'parler,_car',\n",
       "   'car_je',\n",
       "   'je_désirais',\n",
       "   'désirais_connaître',\n",
       "   \"connaître_l'explication\",\n",
       "   \"l'explication_donnée\",\n",
       "   'donnée_par',\n",
       "   'par_m.',\n",
       "   'm._rochester;'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 146,\n",
       "  'stop_token_fr': 158,\n",
       "  'history_tokens': ['lui_parler,', 'parler,_car', 'car_je', 'je_désirais'],\n",
       "  'span_tokens': ['désirais_connaître',\n",
       "   \"connaître_l'explication\",\n",
       "   \"l'explication_donnée\",\n",
       "   'donnée_par',\n",
       "   'par_m.',\n",
       "   'm._rochester;',\n",
       "   'rochester;_mais',\n",
       "   'mais_en',\n",
       "   'en_approchant',\n",
       "   \"approchant_j'aperçus\",\n",
       "   \"j'aperçus_une\",\n",
       "   'une_seconde'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 152,\n",
       "  'stop_token_fr': 164,\n",
       "  'history_tokens': [\"l'explication_donnée\",\n",
       "   'donnée_par',\n",
       "   'par_m.',\n",
       "   'm._rochester;'],\n",
       "  'span_tokens': ['rochester;_mais',\n",
       "   'mais_en',\n",
       "   'en_approchant',\n",
       "   \"approchant_j'aperçus\",\n",
       "   \"j'aperçus_une\",\n",
       "   'une_seconde',\n",
       "   'seconde_personne:',\n",
       "   'personne:_elle',\n",
       "   'elle_était',\n",
       "   'était_assise',\n",
       "   'assise_près',\n",
       "   'près_du'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 158,\n",
       "  'stop_token_fr': 170,\n",
       "  'history_tokens': ['en_approchant',\n",
       "   \"approchant_j'aperçus\",\n",
       "   \"j'aperçus_une\",\n",
       "   'une_seconde'],\n",
       "  'span_tokens': ['seconde_personne:',\n",
       "   'personne:_elle',\n",
       "   'elle_était',\n",
       "   'était_assise',\n",
       "   'assise_près',\n",
       "   'près_du',\n",
       "   'du_lit,',\n",
       "   'lit,_et',\n",
       "   'et_occupée',\n",
       "   'occupée_à',\n",
       "   'à_coudre',\n",
       "   'coudre_des'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 164,\n",
       "  'stop_token_fr': 175,\n",
       "  'history_tokens': ['elle_était', 'était_assise', 'assise_près', 'près_du'],\n",
       "  'span_tokens': ['du_lit,',\n",
       "   'lit,_et',\n",
       "   'et_occupée',\n",
       "   'occupée_à',\n",
       "   'à_coudre',\n",
       "   'coudre_des',\n",
       "   '__sent_marker_1',\n",
       "   'des_anneaux',\n",
       "   'anneaux_à',\n",
       "   'à_des',\n",
       "   'des_rideaux.'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 170,\n",
       "  'stop_token_fr': 181,\n",
       "  'history_tokens': ['et_occupée', 'occupée_à', 'à_coudre', 'coudre_des'],\n",
       "  'span_tokens': ['__sent_marker_1',\n",
       "   'des_anneaux',\n",
       "   'anneaux_à',\n",
       "   'à_des',\n",
       "   'des_rideaux.',\n",
       "   'après_ces_exclamations,',\n",
       "   'ces_exclamations,_on',\n",
       "   'exclamations,_on_remit',\n",
       "   'on_remit_tout',\n",
       "   'remit_tout_en',\n",
       "   'tout_en_état.'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 120,\n",
       "  'stop': 128,\n",
       "  'start_token_fr': 175,\n",
       "  'stop_token_fr': 187,\n",
       "  'history_tokens': ['des_anneaux', 'anneaux_à', 'à_des', 'des_rideaux.'],\n",
       "  'span_tokens': ['après_ces_exclamations,',\n",
       "   'ces_exclamations,_on',\n",
       "   'exclamations,_on_remit',\n",
       "   'on_remit_tout',\n",
       "   'remit_tout_en',\n",
       "   'tout_en_état.',\n",
       "   'en_état._lorsque',\n",
       "   'état._lorsque_je',\n",
       "   'lorsque_je_descendis',\n",
       "   'je_descendis_pour',\n",
       "   'descendis_pour_dîner,',\n",
       "   'pour_dîner,_la'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 124,\n",
       "  'stop': 132,\n",
       "  'start_token_fr': 181,\n",
       "  'stop_token_fr': 193,\n",
       "  'history_tokens': ['exclamations,_on_remit',\n",
       "   'on_remit_tout',\n",
       "   'remit_tout_en',\n",
       "   'tout_en_état.'],\n",
       "  'span_tokens': ['en_état._lorsque',\n",
       "   'état._lorsque_je',\n",
       "   'lorsque_je_descendis',\n",
       "   'je_descendis_pour',\n",
       "   'descendis_pour_dîner,',\n",
       "   'pour_dîner,_la',\n",
       "   'dîner,_la_porte',\n",
       "   'la_porte_de',\n",
       "   'porte_de_la',\n",
       "   'de_la_chambre',\n",
       "   'la_chambre_était',\n",
       "   'chambre_était_ouverte'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 128,\n",
       "  'stop': 136,\n",
       "  'start_token_fr': 187,\n",
       "  'stop_token_fr': 199,\n",
       "  'history_tokens': ['lorsque_je_descendis',\n",
       "   'je_descendis_pour',\n",
       "   'descendis_pour_dîner,',\n",
       "   'pour_dîner,_la'],\n",
       "  'span_tokens': ['dîner,_la_porte',\n",
       "   'la_porte_de',\n",
       "   'porte_de_la',\n",
       "   'de_la_chambre',\n",
       "   'la_chambre_était',\n",
       "   'chambre_était_ouverte',\n",
       "   'était_ouverte_et',\n",
       "   'ouverte_et_je',\n",
       "   'et_je_vis',\n",
       "   'je_vis_que',\n",
       "   'vis_que_le',\n",
       "   'que_le_dégât'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 132,\n",
       "  'stop': 140,\n",
       "  'start_token_fr': 193,\n",
       "  'stop_token_fr': 205,\n",
       "  'history_tokens': ['porte_de_la',\n",
       "   'de_la_chambre',\n",
       "   'la_chambre_était',\n",
       "   'chambre_était_ouverte'],\n",
       "  'span_tokens': ['était_ouverte_et',\n",
       "   'ouverte_et_je',\n",
       "   'et_je_vis',\n",
       "   'je_vis_que',\n",
       "   'vis_que_le',\n",
       "   'que_le_dégât',\n",
       "   'le_dégât_avait',\n",
       "   'dégât_avait_été',\n",
       "   'avait_été_réparé;',\n",
       "   'été_réparé;_le',\n",
       "   'réparé;_le_lit',\n",
       "   'le_lit_seul'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 136,\n",
       "  'stop': 144,\n",
       "  'start_token_fr': 199,\n",
       "  'stop_token_fr': 211,\n",
       "  'history_tokens': ['et_je_vis', 'je_vis_que', 'vis_que_le', 'que_le_dégât'],\n",
       "  'span_tokens': ['le_dégât_avait',\n",
       "   'dégât_avait_été',\n",
       "   'avait_été_réparé;',\n",
       "   'été_réparé;_le',\n",
       "   'réparé;_le_lit',\n",
       "   'le_lit_seul',\n",
       "   'lit_seul_restait',\n",
       "   'seul_restait_encore',\n",
       "   'restait_encore_dépouillé',\n",
       "   'encore_dépouillé_de',\n",
       "   'dépouillé_de_ses',\n",
       "   'de_ses_rideaux;'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 140,\n",
       "  'stop': 148,\n",
       "  'start_token_fr': 205,\n",
       "  'stop_token_fr': 216,\n",
       "  'history_tokens': ['avait_été_réparé;',\n",
       "   'été_réparé;_le',\n",
       "   'réparé;_le_lit',\n",
       "   'le_lit_seul'],\n",
       "  'span_tokens': ['lit_seul_restait',\n",
       "   'seul_restait_encore',\n",
       "   'restait_encore_dépouillé',\n",
       "   'encore_dépouillé_de',\n",
       "   'dépouillé_de_ses',\n",
       "   'de_ses_rideaux;',\n",
       "   'ses_rideaux;_leah',\n",
       "   'rideaux;_leah_était',\n",
       "   'leah_était_occupée',\n",
       "   'était_occupée_à',\n",
       "   'occupée_à_laver'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 144,\n",
       "  'stop': 152,\n",
       "  'start_token_fr': 211,\n",
       "  'stop_token_fr': 222,\n",
       "  'history_tokens': ['restait_encore_dépouillé',\n",
       "   'encore_dépouillé_de',\n",
       "   'dépouillé_de_ses',\n",
       "   'de_ses_rideaux;'],\n",
       "  'span_tokens': ['ses_rideaux;_leah',\n",
       "   'rideaux;_leah_était',\n",
       "   'leah_était_occupée',\n",
       "   'était_occupée_à',\n",
       "   'occupée_à_laver',\n",
       "   'à_laver_le',\n",
       "   'laver_le_bord',\n",
       "   'le_bord_des',\n",
       "   'bord_des_fenêtres',\n",
       "   'des_fenêtres_noirci',\n",
       "   'fenêtres_noirci_par'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 148,\n",
       "  'stop': 156,\n",
       "  'start_token_fr': 216,\n",
       "  'stop_token_fr': 228,\n",
       "  'history_tokens': ['rideaux;_leah_était',\n",
       "   'leah_était_occupée',\n",
       "   'était_occupée_à',\n",
       "   'occupée_à_laver'],\n",
       "  'span_tokens': ['à_laver_le',\n",
       "   'laver_le_bord',\n",
       "   'le_bord_des',\n",
       "   'bord_des_fenêtres',\n",
       "   'des_fenêtres_noirci',\n",
       "   'fenêtres_noirci_par',\n",
       "   'noirci_par_la',\n",
       "   'par_la_fumée;',\n",
       "   'la_fumée;_je',\n",
       "   \"fumée;_je_m'avançai\",\n",
       "   \"je_m'avançai_pour\",\n",
       "   \"m'avançai_pour_lui\"],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 152,\n",
       "  'stop': 160,\n",
       "  'start_token_fr': 222,\n",
       "  'stop_token_fr': 234,\n",
       "  'history_tokens': ['le_bord_des',\n",
       "   'bord_des_fenêtres',\n",
       "   'des_fenêtres_noirci',\n",
       "   'fenêtres_noirci_par'],\n",
       "  'span_tokens': ['noirci_par_la',\n",
       "   'par_la_fumée;',\n",
       "   'la_fumée;_je',\n",
       "   \"fumée;_je_m'avançai\",\n",
       "   \"je_m'avançai_pour\",\n",
       "   \"m'avançai_pour_lui\",\n",
       "   'pour_lui_parler,',\n",
       "   'lui_parler,_car',\n",
       "   'parler,_car_je',\n",
       "   'car_je_désirais',\n",
       "   'je_désirais_connaître',\n",
       "   \"désirais_connaître_l'explication\"],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 156,\n",
       "  'stop': 164,\n",
       "  'start_token_fr': 228,\n",
       "  'stop_token_fr': 240,\n",
       "  'history_tokens': ['la_fumée;_je',\n",
       "   \"fumée;_je_m'avançai\",\n",
       "   \"je_m'avançai_pour\",\n",
       "   \"m'avançai_pour_lui\"],\n",
       "  'span_tokens': ['pour_lui_parler,',\n",
       "   'lui_parler,_car',\n",
       "   'parler,_car_je',\n",
       "   'car_je_désirais',\n",
       "   'je_désirais_connaître',\n",
       "   \"désirais_connaître_l'explication\",\n",
       "   \"connaître_l'explication_donnée\",\n",
       "   \"l'explication_donnée_par\",\n",
       "   'donnée_par_m.',\n",
       "   'par_m._rochester;',\n",
       "   'm._rochester;_mais',\n",
       "   'rochester;_mais_en'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 160,\n",
       "  'stop': 168,\n",
       "  'start_token_fr': 234,\n",
       "  'stop_token_fr': 246,\n",
       "  'history_tokens': ['parler,_car_je',\n",
       "   'car_je_désirais',\n",
       "   'je_désirais_connaître',\n",
       "   \"désirais_connaître_l'explication\"],\n",
       "  'span_tokens': [\"connaître_l'explication_donnée\",\n",
       "   \"l'explication_donnée_par\",\n",
       "   'donnée_par_m.',\n",
       "   'par_m._rochester;',\n",
       "   'm._rochester;_mais',\n",
       "   'rochester;_mais_en',\n",
       "   'mais_en_approchant',\n",
       "   \"en_approchant_j'aperçus\",\n",
       "   \"approchant_j'aperçus_une\",\n",
       "   \"j'aperçus_une_seconde\",\n",
       "   'une_seconde_personne:',\n",
       "   'seconde_personne:_elle'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 164,\n",
       "  'stop': 172,\n",
       "  'start_token_fr': 240,\n",
       "  'stop_token_fr': 251,\n",
       "  'history_tokens': ['donnée_par_m.',\n",
       "   'par_m._rochester;',\n",
       "   'm._rochester;_mais',\n",
       "   'rochester;_mais_en'],\n",
       "  'span_tokens': ['mais_en_approchant',\n",
       "   \"en_approchant_j'aperçus\",\n",
       "   \"approchant_j'aperçus_une\",\n",
       "   \"j'aperçus_une_seconde\",\n",
       "   'une_seconde_personne:',\n",
       "   'seconde_personne:_elle',\n",
       "   'personne:_elle_était',\n",
       "   'elle_était_assise',\n",
       "   'était_assise_près',\n",
       "   'assise_près_du',\n",
       "   'près_du_lit,'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 168,\n",
       "  'stop': 176,\n",
       "  'start_token_fr': 246,\n",
       "  'stop_token_fr': 257,\n",
       "  'history_tokens': [\"approchant_j'aperçus_une\",\n",
       "   \"j'aperçus_une_seconde\",\n",
       "   'une_seconde_personne:',\n",
       "   'seconde_personne:_elle'],\n",
       "  'span_tokens': ['personne:_elle_était',\n",
       "   'elle_était_assise',\n",
       "   'était_assise_près',\n",
       "   'assise_près_du',\n",
       "   'près_du_lit,',\n",
       "   'du_lit,_et',\n",
       "   'lit,_et_occupée',\n",
       "   'et_occupée_à',\n",
       "   'occupée_à_coudre',\n",
       "   '__sent_marker_2',\n",
       "   'à_coudre_des'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 172,\n",
       "  'stop': 180,\n",
       "  'start_token_fr': 251,\n",
       "  'stop_token_fr': 263,\n",
       "  'history_tokens': ['elle_était_assise',\n",
       "   'était_assise_près',\n",
       "   'assise_près_du',\n",
       "   'près_du_lit,'],\n",
       "  'span_tokens': ['du_lit,_et',\n",
       "   'lit,_et_occupée',\n",
       "   'et_occupée_à',\n",
       "   'occupée_à_coudre',\n",
       "   '__sent_marker_2',\n",
       "   'à_coudre_des',\n",
       "   'coudre_des_anneaux',\n",
       "   'des_anneaux_à',\n",
       "   'anneaux_à_des',\n",
       "   'à_des_rideaux.',\n",
       "   'après_ces_exclamations,_on',\n",
       "   'ces_exclamations,_on_remit'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 176,\n",
       "  'stop': 184,\n",
       "  'start_token_fr': 257,\n",
       "  'stop_token_fr': 269,\n",
       "  'history_tokens': ['et_occupée_à',\n",
       "   'occupée_à_coudre',\n",
       "   '__sent_marker_2',\n",
       "   'à_coudre_des'],\n",
       "  'span_tokens': ['coudre_des_anneaux',\n",
       "   'des_anneaux_à',\n",
       "   'anneaux_à_des',\n",
       "   'à_des_rideaux.',\n",
       "   'après_ces_exclamations,_on',\n",
       "   'ces_exclamations,_on_remit',\n",
       "   'exclamations,_on_remit_tout',\n",
       "   'on_remit_tout_en',\n",
       "   'remit_tout_en_état.',\n",
       "   'tout_en_état._lorsque',\n",
       "   'en_état._lorsque_je',\n",
       "   'état._lorsque_je_descendis'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 180,\n",
       "  'stop': 188,\n",
       "  'start_token_fr': 263,\n",
       "  'stop_token_fr': 275,\n",
       "  'history_tokens': ['anneaux_à_des',\n",
       "   'à_des_rideaux.',\n",
       "   'après_ces_exclamations,_on',\n",
       "   'ces_exclamations,_on_remit'],\n",
       "  'span_tokens': ['exclamations,_on_remit_tout',\n",
       "   'on_remit_tout_en',\n",
       "   'remit_tout_en_état.',\n",
       "   'tout_en_état._lorsque',\n",
       "   'en_état._lorsque_je',\n",
       "   'état._lorsque_je_descendis',\n",
       "   'lorsque_je_descendis_pour',\n",
       "   'je_descendis_pour_dîner,',\n",
       "   'descendis_pour_dîner,_la',\n",
       "   'pour_dîner,_la_porte',\n",
       "   'dîner,_la_porte_de',\n",
       "   'la_porte_de_la'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 184,\n",
       "  'stop': 192,\n",
       "  'start_token_fr': 269,\n",
       "  'stop_token_fr': 281,\n",
       "  'history_tokens': ['remit_tout_en_état.',\n",
       "   'tout_en_état._lorsque',\n",
       "   'en_état._lorsque_je',\n",
       "   'état._lorsque_je_descendis'],\n",
       "  'span_tokens': ['lorsque_je_descendis_pour',\n",
       "   'je_descendis_pour_dîner,',\n",
       "   'descendis_pour_dîner,_la',\n",
       "   'pour_dîner,_la_porte',\n",
       "   'dîner,_la_porte_de',\n",
       "   'la_porte_de_la',\n",
       "   'porte_de_la_chambre',\n",
       "   'de_la_chambre_était',\n",
       "   'la_chambre_était_ouverte',\n",
       "   'chambre_était_ouverte_et',\n",
       "   'était_ouverte_et_je',\n",
       "   'ouverte_et_je_vis'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 188,\n",
       "  'stop': 196,\n",
       "  'start_token_fr': 275,\n",
       "  'stop_token_fr': 287,\n",
       "  'history_tokens': ['descendis_pour_dîner,_la',\n",
       "   'pour_dîner,_la_porte',\n",
       "   'dîner,_la_porte_de',\n",
       "   'la_porte_de_la'],\n",
       "  'span_tokens': ['porte_de_la_chambre',\n",
       "   'de_la_chambre_était',\n",
       "   'la_chambre_était_ouverte',\n",
       "   'chambre_était_ouverte_et',\n",
       "   'était_ouverte_et_je',\n",
       "   'ouverte_et_je_vis',\n",
       "   'et_je_vis_que',\n",
       "   'je_vis_que_le',\n",
       "   'vis_que_le_dégât',\n",
       "   'que_le_dégât_avait',\n",
       "   'le_dégât_avait_été',\n",
       "   'dégât_avait_été_réparé;'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 192,\n",
       "  'stop': 200,\n",
       "  'start_token_fr': 281,\n",
       "  'stop_token_fr': 292,\n",
       "  'history_tokens': ['la_chambre_était_ouverte',\n",
       "   'chambre_était_ouverte_et',\n",
       "   'était_ouverte_et_je',\n",
       "   'ouverte_et_je_vis'],\n",
       "  'span_tokens': ['et_je_vis_que',\n",
       "   'je_vis_que_le',\n",
       "   'vis_que_le_dégât',\n",
       "   'que_le_dégât_avait',\n",
       "   'le_dégât_avait_été',\n",
       "   'dégât_avait_été_réparé;',\n",
       "   'avait_été_réparé;_le',\n",
       "   'été_réparé;_le_lit',\n",
       "   'réparé;_le_lit_seul',\n",
       "   'le_lit_seul_restait',\n",
       "   'lit_seul_restait_encore'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 196,\n",
       "  'stop': 204,\n",
       "  'start_token_fr': 287,\n",
       "  'stop_token_fr': 298,\n",
       "  'history_tokens': ['vis_que_le_dégât',\n",
       "   'que_le_dégât_avait',\n",
       "   'le_dégât_avait_été',\n",
       "   'dégât_avait_été_réparé;'],\n",
       "  'span_tokens': ['avait_été_réparé;_le',\n",
       "   'été_réparé;_le_lit',\n",
       "   'réparé;_le_lit_seul',\n",
       "   'le_lit_seul_restait',\n",
       "   'lit_seul_restait_encore',\n",
       "   'seul_restait_encore_dépouillé',\n",
       "   'restait_encore_dépouillé_de',\n",
       "   'encore_dépouillé_de_ses',\n",
       "   'dépouillé_de_ses_rideaux;',\n",
       "   'de_ses_rideaux;_leah',\n",
       "   'ses_rideaux;_leah_était'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 200,\n",
       "  'stop': 208,\n",
       "  'start_token_fr': 292,\n",
       "  'stop_token_fr': 304,\n",
       "  'history_tokens': ['été_réparé;_le_lit',\n",
       "   'réparé;_le_lit_seul',\n",
       "   'le_lit_seul_restait',\n",
       "   'lit_seul_restait_encore'],\n",
       "  'span_tokens': ['seul_restait_encore_dépouillé',\n",
       "   'restait_encore_dépouillé_de',\n",
       "   'encore_dépouillé_de_ses',\n",
       "   'dépouillé_de_ses_rideaux;',\n",
       "   'de_ses_rideaux;_leah',\n",
       "   'ses_rideaux;_leah_était',\n",
       "   'rideaux;_leah_était_occupée',\n",
       "   'leah_était_occupée_à',\n",
       "   'était_occupée_à_laver',\n",
       "   'occupée_à_laver_le',\n",
       "   'à_laver_le_bord',\n",
       "   'laver_le_bord_des'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 204,\n",
       "  'stop': 212,\n",
       "  'start_token_fr': 298,\n",
       "  'stop_token_fr': 310,\n",
       "  'history_tokens': ['encore_dépouillé_de_ses',\n",
       "   'dépouillé_de_ses_rideaux;',\n",
       "   'de_ses_rideaux;_leah',\n",
       "   'ses_rideaux;_leah_était'],\n",
       "  'span_tokens': ['rideaux;_leah_était_occupée',\n",
       "   'leah_était_occupée_à',\n",
       "   'était_occupée_à_laver',\n",
       "   'occupée_à_laver_le',\n",
       "   'à_laver_le_bord',\n",
       "   'laver_le_bord_des',\n",
       "   'le_bord_des_fenêtres',\n",
       "   'bord_des_fenêtres_noirci',\n",
       "   'des_fenêtres_noirci_par',\n",
       "   'fenêtres_noirci_par_la',\n",
       "   'noirci_par_la_fumée;',\n",
       "   'par_la_fumée;_je'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 208,\n",
       "  'stop': 216,\n",
       "  'start_token_fr': 304,\n",
       "  'stop_token_fr': 316,\n",
       "  'history_tokens': ['était_occupée_à_laver',\n",
       "   'occupée_à_laver_le',\n",
       "   'à_laver_le_bord',\n",
       "   'laver_le_bord_des'],\n",
       "  'span_tokens': ['le_bord_des_fenêtres',\n",
       "   'bord_des_fenêtres_noirci',\n",
       "   'des_fenêtres_noirci_par',\n",
       "   'fenêtres_noirci_par_la',\n",
       "   'noirci_par_la_fumée;',\n",
       "   'par_la_fumée;_je',\n",
       "   \"la_fumée;_je_m'avançai\",\n",
       "   \"fumée;_je_m'avançai_pour\",\n",
       "   \"je_m'avançai_pour_lui\",\n",
       "   \"m'avançai_pour_lui_parler,\",\n",
       "   'pour_lui_parler,_car',\n",
       "   'lui_parler,_car_je'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 212,\n",
       "  'stop': 220,\n",
       "  'start_token_fr': 310,\n",
       "  'stop_token_fr': 322,\n",
       "  'history_tokens': ['des_fenêtres_noirci_par',\n",
       "   'fenêtres_noirci_par_la',\n",
       "   'noirci_par_la_fumée;',\n",
       "   'par_la_fumée;_je'],\n",
       "  'span_tokens': [\"la_fumée;_je_m'avançai\",\n",
       "   \"fumée;_je_m'avançai_pour\",\n",
       "   \"je_m'avançai_pour_lui\",\n",
       "   \"m'avançai_pour_lui_parler,\",\n",
       "   'pour_lui_parler,_car',\n",
       "   'lui_parler,_car_je',\n",
       "   'parler,_car_je_désirais',\n",
       "   'car_je_désirais_connaître',\n",
       "   \"je_désirais_connaître_l'explication\",\n",
       "   \"désirais_connaître_l'explication_donnée\",\n",
       "   \"connaître_l'explication_donnée_par\",\n",
       "   \"l'explication_donnée_par_m.\"],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 216,\n",
       "  'stop': 224,\n",
       "  'start_token_fr': 316,\n",
       "  'stop_token_fr': 327,\n",
       "  'history_tokens': [\"je_m'avançai_pour_lui\",\n",
       "   \"m'avançai_pour_lui_parler,\",\n",
       "   'pour_lui_parler,_car',\n",
       "   'lui_parler,_car_je'],\n",
       "  'span_tokens': ['parler,_car_je_désirais',\n",
       "   'car_je_désirais_connaître',\n",
       "   \"je_désirais_connaître_l'explication\",\n",
       "   \"désirais_connaître_l'explication_donnée\",\n",
       "   \"connaître_l'explication_donnée_par\",\n",
       "   \"l'explication_donnée_par_m.\",\n",
       "   'donnée_par_m._rochester;',\n",
       "   'par_m._rochester;_mais',\n",
       "   'm._rochester;_mais_en',\n",
       "   'rochester;_mais_en_approchant',\n",
       "   \"mais_en_approchant_j'aperçus\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 220,\n",
       "  'stop': 228,\n",
       "  'start_token_fr': 322,\n",
       "  'stop_token_fr': 333,\n",
       "  'history_tokens': [\"je_désirais_connaître_l'explication\",\n",
       "   \"désirais_connaître_l'explication_donnée\",\n",
       "   \"connaître_l'explication_donnée_par\",\n",
       "   \"l'explication_donnée_par_m.\"],\n",
       "  'span_tokens': ['donnée_par_m._rochester;',\n",
       "   'par_m._rochester;_mais',\n",
       "   'm._rochester;_mais_en',\n",
       "   'rochester;_mais_en_approchant',\n",
       "   \"mais_en_approchant_j'aperçus\",\n",
       "   \"en_approchant_j'aperçus_une\",\n",
       "   \"approchant_j'aperçus_une_seconde\",\n",
       "   \"j'aperçus_une_seconde_personne:\",\n",
       "   'une_seconde_personne:_elle',\n",
       "   'seconde_personne:_elle_était',\n",
       "   'personne:_elle_était_assise'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 224,\n",
       "  'stop': 232,\n",
       "  'start_token_fr': 327,\n",
       "  'stop_token_fr': 339,\n",
       "  'history_tokens': ['par_m._rochester;_mais',\n",
       "   'm._rochester;_mais_en',\n",
       "   'rochester;_mais_en_approchant',\n",
       "   \"mais_en_approchant_j'aperçus\"],\n",
       "  'span_tokens': [\"en_approchant_j'aperçus_une\",\n",
       "   \"approchant_j'aperçus_une_seconde\",\n",
       "   \"j'aperçus_une_seconde_personne:\",\n",
       "   'une_seconde_personne:_elle',\n",
       "   'seconde_personne:_elle_était',\n",
       "   'personne:_elle_était_assise',\n",
       "   'elle_était_assise_près',\n",
       "   'était_assise_près_du',\n",
       "   'assise_près_du_lit,',\n",
       "   'près_du_lit,_et',\n",
       "   'du_lit,_et_occupée',\n",
       "   'lit,_et_occupée_à'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 29,\n",
       "  'start': 228,\n",
       "  'stop': 236,\n",
       "  'start_token_fr': 333,\n",
       "  'stop_token_fr': 345,\n",
       "  'history_tokens': [\"j'aperçus_une_seconde_personne:\",\n",
       "   'une_seconde_personne:_elle',\n",
       "   'seconde_personne:_elle_était',\n",
       "   'personne:_elle_était_assise'],\n",
       "  'span_tokens': ['elle_était_assise_près',\n",
       "   'était_assise_près_du',\n",
       "   'assise_près_du_lit,',\n",
       "   'près_du_lit,_et',\n",
       "   'du_lit,_et_occupée',\n",
       "   'lit,_et_occupée_à',\n",
       "   'et_occupée_à_coudre',\n",
       "   'occupée_à_coudre_des',\n",
       "   'à_coudre_des_anneaux',\n",
       "   'coudre_des_anneaux_à',\n",
       "   'des_anneaux_à_des',\n",
       "   'anneaux_à_des_rideaux.'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 30,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['bien.',\n",
       "   '__sent_marker_0',\n",
       "   '__sent_marker_1',\n",
       "   '__sent_marker_2',\n",
       "   'bien._dup0',\n",
       "   'bien._dup1',\n",
       "   'bien._dup2'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 30,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['bien.',\n",
       "   '__sent_marker_0',\n",
       "   '__sent_marker_1',\n",
       "   '__sent_marker_2'],\n",
       "  'span_tokens': ['bien._dup0',\n",
       "   'bien._dup1',\n",
       "   'bien._dup2',\n",
       "   'bien._dup3',\n",
       "   'bien._dup4',\n",
       "   'bien._dup5',\n",
       "   'bien._dup6'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 30,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['__sent_marker_2',\n",
       "   'bien._dup0',\n",
       "   'bien._dup1',\n",
       "   'bien._dup2'],\n",
       "  'span_tokens': ['bien._dup3',\n",
       "   'bien._dup4',\n",
       "   'bien._dup5',\n",
       "   'bien._dup6',\n",
       "   'bien._dup7',\n",
       "   'bien._dup8',\n",
       "   'bien._dup9'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 30,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['bien._dup3', 'bien._dup4', 'bien._dup5', 'bien._dup6'],\n",
       "  'span_tokens': ['bien._dup7',\n",
       "   'bien._dup8',\n",
       "   'bien._dup9',\n",
       "   'bien._dup10',\n",
       "   'bien._dup11',\n",
       "   'bien._dup12',\n",
       "   'bien._dup13'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 30,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['bien._dup6', 'bien._dup7', 'bien._dup8', 'bien._dup9'],\n",
       "  'span_tokens': ['bien._dup10',\n",
       "   'bien._dup11',\n",
       "   'bien._dup12',\n",
       "   'bien._dup13',\n",
       "   'bien._dup14',\n",
       "   'bien._dup15',\n",
       "   'bien._dup16'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 31,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['ilfrappa.',\n",
       "   '__sent_marker_0',\n",
       "   '__sent_marker_1',\n",
       "   '__sent_marker_2',\n",
       "   'ilfrappa._dup0',\n",
       "   'ilfrappa._dup1',\n",
       "   'ilfrappa._dup2'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 31,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['ilfrappa.',\n",
       "   '__sent_marker_0',\n",
       "   '__sent_marker_1',\n",
       "   '__sent_marker_2'],\n",
       "  'span_tokens': ['ilfrappa._dup0',\n",
       "   'ilfrappa._dup1',\n",
       "   'ilfrappa._dup2',\n",
       "   'ilfrappa._dup3',\n",
       "   'ilfrappa._dup4',\n",
       "   'ilfrappa._dup5',\n",
       "   'ilfrappa._dup6'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 31,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['__sent_marker_2',\n",
       "   'ilfrappa._dup0',\n",
       "   'ilfrappa._dup1',\n",
       "   'ilfrappa._dup2'],\n",
       "  'span_tokens': ['ilfrappa._dup3',\n",
       "   'ilfrappa._dup4',\n",
       "   'ilfrappa._dup5',\n",
       "   'ilfrappa._dup6',\n",
       "   'ilfrappa._dup7',\n",
       "   'ilfrappa._dup8',\n",
       "   'ilfrappa._dup9'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 31,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['ilfrappa._dup3',\n",
       "   'ilfrappa._dup4',\n",
       "   'ilfrappa._dup5',\n",
       "   'ilfrappa._dup6'],\n",
       "  'span_tokens': ['ilfrappa._dup7',\n",
       "   'ilfrappa._dup8',\n",
       "   'ilfrappa._dup9',\n",
       "   'ilfrappa._dup10',\n",
       "   'ilfrappa._dup11',\n",
       "   'ilfrappa._dup12',\n",
       "   'ilfrappa._dup13'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 31,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['ilfrappa._dup6',\n",
       "   'ilfrappa._dup7',\n",
       "   'ilfrappa._dup8',\n",
       "   'ilfrappa._dup9'],\n",
       "  'span_tokens': ['ilfrappa._dup10',\n",
       "   'ilfrappa._dup11',\n",
       "   'ilfrappa._dup12',\n",
       "   'ilfrappa._dup13',\n",
       "   'ilfrappa._dup14',\n",
       "   'ilfrappa._dup15',\n",
       "   'ilfrappa._dup16'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['je', 'voudrais', 'bien', 'savoir', 'si', 'c’est', 'la'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['je', 'voudrais', 'bien', 'savoir'],\n",
       "  'span_tokens': ['si', 'c’est', 'la', 'grâce', 'de', 'dieu', 'qu’il'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['savoir', 'si', 'c’est', 'la'],\n",
       "  'span_tokens': ['grâce', 'de', 'dieu', 'qu’il', 'y', 'ait', 'à'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['grâce', 'de', 'dieu', 'qu’il'],\n",
       "  'span_tokens': ['y', 'ait', 'à', 'paris', 'un', 'autre', 'voyer'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 22,\n",
       "  'history_tokens': ['qu’il', 'y', 'ait', 'à'],\n",
       "  'span_tokens': ['paris', 'un', 'autre', 'voyer', 'que', 'le', 'roi,', 'une'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 18,\n",
       "  'stop_token_fr': 25,\n",
       "  'history_tokens': ['paris', 'un', 'autre', 'voyer'],\n",
       "  'span_tokens': ['que', 'le', 'roi,', 'une', 'autre', 'justice', 'que'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 22,\n",
       "  'stop_token_fr': 29,\n",
       "  'history_tokens': ['que', 'le', 'roi,', 'une'],\n",
       "  'span_tokens': ['autre',\n",
       "   'justice',\n",
       "   'que',\n",
       "   'notre',\n",
       "   'parlement,',\n",
       "   'un',\n",
       "   'autre'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 25,\n",
       "  'stop_token_fr': 33,\n",
       "  'history_tokens': ['une', 'autre', 'justice', 'que'],\n",
       "  'span_tokens': ['notre',\n",
       "   'parlement,',\n",
       "   'un',\n",
       "   'autre',\n",
       "   'empereur',\n",
       "   'que',\n",
       "   'nous',\n",
       "   'dans'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 29,\n",
       "  'stop_token_fr': 36,\n",
       "  'history_tokens': ['notre', 'parlement,', 'un', 'autre'],\n",
       "  'span_tokens': ['empereur',\n",
       "   'que',\n",
       "   'nous',\n",
       "   'dans',\n",
       "   'cet',\n",
       "   '__sent_marker_0',\n",
       "   'empire'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 33,\n",
       "  'stop_token_fr': 40,\n",
       "  'history_tokens': ['empereur', 'que', 'nous', 'dans'],\n",
       "  'span_tokens': ['cet',\n",
       "   '__sent_marker_0',\n",
       "   'empire',\n",
       "   '!',\n",
       "   'je_voudrais',\n",
       "   'voudrais_bien',\n",
       "   'bien_savoir'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 36,\n",
       "  'stop_token_fr': 43,\n",
       "  'history_tokens': ['dans', 'cet', '__sent_marker_0', 'empire'],\n",
       "  'span_tokens': ['!',\n",
       "   'je_voudrais',\n",
       "   'voudrais_bien',\n",
       "   'bien_savoir',\n",
       "   'savoir_si',\n",
       "   'si_c’est',\n",
       "   'c’est_la'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 40,\n",
       "  'stop_token_fr': 47,\n",
       "  'history_tokens': ['!', 'je_voudrais', 'voudrais_bien', 'bien_savoir'],\n",
       "  'span_tokens': ['savoir_si',\n",
       "   'si_c’est',\n",
       "   'c’est_la',\n",
       "   'la_grâce',\n",
       "   'grâce_de',\n",
       "   'de_dieu',\n",
       "   'dieu_qu’il'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 43,\n",
       "  'stop_token_fr': 51,\n",
       "  'history_tokens': ['bien_savoir', 'savoir_si', 'si_c’est', 'c’est_la'],\n",
       "  'span_tokens': ['la_grâce',\n",
       "   'grâce_de',\n",
       "   'de_dieu',\n",
       "   'dieu_qu’il',\n",
       "   'qu’il_y',\n",
       "   'y_ait',\n",
       "   'ait_à',\n",
       "   'à_paris'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 47,\n",
       "  'stop_token_fr': 54,\n",
       "  'history_tokens': ['la_grâce', 'grâce_de', 'de_dieu', 'dieu_qu’il'],\n",
       "  'span_tokens': ['qu’il_y',\n",
       "   'y_ait',\n",
       "   'ait_à',\n",
       "   'à_paris',\n",
       "   'paris_un',\n",
       "   'un_autre',\n",
       "   'autre_voyer'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 51,\n",
       "  'stop_token_fr': 58,\n",
       "  'history_tokens': ['qu’il_y', 'y_ait', 'ait_à', 'à_paris'],\n",
       "  'span_tokens': ['paris_un',\n",
       "   'un_autre',\n",
       "   'autre_voyer',\n",
       "   'voyer_que',\n",
       "   'que_le',\n",
       "   'le_roi,',\n",
       "   'roi,_une'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 54,\n",
       "  'stop_token_fr': 61,\n",
       "  'history_tokens': ['à_paris', 'paris_un', 'un_autre', 'autre_voyer'],\n",
       "  'span_tokens': ['voyer_que',\n",
       "   'que_le',\n",
       "   'le_roi,',\n",
       "   'roi,_une',\n",
       "   'une_autre',\n",
       "   'autre_justice',\n",
       "   'justice_que'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 58,\n",
       "  'stop_token_fr': 65,\n",
       "  'history_tokens': ['voyer_que', 'que_le', 'le_roi,', 'roi,_une'],\n",
       "  'span_tokens': ['une_autre',\n",
       "   'autre_justice',\n",
       "   'justice_que',\n",
       "   'que_notre',\n",
       "   'notre_parlement,',\n",
       "   'parlement,_un',\n",
       "   'un_autre'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 61,\n",
       "  'stop_token_fr': 69,\n",
       "  'history_tokens': ['roi,_une', 'une_autre', 'autre_justice', 'justice_que'],\n",
       "  'span_tokens': ['que_notre',\n",
       "   'notre_parlement,',\n",
       "   'parlement,_un',\n",
       "   'un_autre',\n",
       "   'autre_empereur',\n",
       "   'empereur_que',\n",
       "   'que_nous',\n",
       "   '__sent_marker_1'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 65,\n",
       "  'stop_token_fr': 72,\n",
       "  'history_tokens': ['que_notre',\n",
       "   'notre_parlement,',\n",
       "   'parlement,_un',\n",
       "   'un_autre'],\n",
       "  'span_tokens': ['autre_empereur',\n",
       "   'empereur_que',\n",
       "   'que_nous',\n",
       "   '__sent_marker_1',\n",
       "   'nous_dans',\n",
       "   'dans_cet',\n",
       "   'cet_empire'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 69,\n",
       "  'stop_token_fr': 76,\n",
       "  'history_tokens': ['autre_empereur',\n",
       "   'empereur_que',\n",
       "   'que_nous',\n",
       "   '__sent_marker_1'],\n",
       "  'span_tokens': ['nous_dans',\n",
       "   'dans_cet',\n",
       "   'cet_empire',\n",
       "   'empire_!',\n",
       "   'je_voudrais_bien',\n",
       "   'voudrais_bien_savoir',\n",
       "   'bien_savoir_si'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 72,\n",
       "  'stop_token_fr': 80,\n",
       "  'history_tokens': ['__sent_marker_1', 'nous_dans', 'dans_cet', 'cet_empire'],\n",
       "  'span_tokens': ['empire_!',\n",
       "   'je_voudrais_bien',\n",
       "   'voudrais_bien_savoir',\n",
       "   'bien_savoir_si',\n",
       "   'savoir_si_c’est',\n",
       "   'si_c’est_la',\n",
       "   'c’est_la_grâce',\n",
       "   'la_grâce_de'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 76,\n",
       "  'stop_token_fr': 83,\n",
       "  'history_tokens': ['empire_!',\n",
       "   'je_voudrais_bien',\n",
       "   'voudrais_bien_savoir',\n",
       "   'bien_savoir_si'],\n",
       "  'span_tokens': ['savoir_si_c’est',\n",
       "   'si_c’est_la',\n",
       "   'c’est_la_grâce',\n",
       "   'la_grâce_de',\n",
       "   'grâce_de_dieu',\n",
       "   'de_dieu_qu’il',\n",
       "   'dieu_qu’il_y'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 80,\n",
       "  'stop_token_fr': 87,\n",
       "  'history_tokens': ['savoir_si_c’est',\n",
       "   'si_c’est_la',\n",
       "   'c’est_la_grâce',\n",
       "   'la_grâce_de'],\n",
       "  'span_tokens': ['grâce_de_dieu',\n",
       "   'de_dieu_qu’il',\n",
       "   'dieu_qu’il_y',\n",
       "   'qu’il_y_ait',\n",
       "   'y_ait_à',\n",
       "   'ait_à_paris',\n",
       "   'à_paris_un'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 83,\n",
       "  'stop_token_fr': 90,\n",
       "  'history_tokens': ['la_grâce_de',\n",
       "   'grâce_de_dieu',\n",
       "   'de_dieu_qu’il',\n",
       "   'dieu_qu’il_y'],\n",
       "  'span_tokens': ['qu’il_y_ait',\n",
       "   'y_ait_à',\n",
       "   'ait_à_paris',\n",
       "   'à_paris_un',\n",
       "   'paris_un_autre',\n",
       "   'un_autre_voyer',\n",
       "   'autre_voyer_que'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 87,\n",
       "  'stop_token_fr': 94,\n",
       "  'history_tokens': ['qu’il_y_ait', 'y_ait_à', 'ait_à_paris', 'à_paris_un'],\n",
       "  'span_tokens': ['paris_un_autre',\n",
       "   'un_autre_voyer',\n",
       "   'autre_voyer_que',\n",
       "   'voyer_que_le',\n",
       "   'que_le_roi,',\n",
       "   'le_roi,_une',\n",
       "   'roi,_une_autre'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 90,\n",
       "  'stop_token_fr': 98,\n",
       "  'history_tokens': ['à_paris_un',\n",
       "   'paris_un_autre',\n",
       "   'un_autre_voyer',\n",
       "   'autre_voyer_que'],\n",
       "  'span_tokens': ['voyer_que_le',\n",
       "   'que_le_roi,',\n",
       "   'le_roi,_une',\n",
       "   'roi,_une_autre',\n",
       "   'une_autre_justice',\n",
       "   'autre_justice_que',\n",
       "   'justice_que_notre',\n",
       "   'que_notre_parlement,'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 94,\n",
       "  'stop_token_fr': 101,\n",
       "  'history_tokens': ['voyer_que_le',\n",
       "   'que_le_roi,',\n",
       "   'le_roi,_une',\n",
       "   'roi,_une_autre'],\n",
       "  'span_tokens': ['une_autre_justice',\n",
       "   'autre_justice_que',\n",
       "   'justice_que_notre',\n",
       "   'que_notre_parlement,',\n",
       "   'notre_parlement,_un',\n",
       "   'parlement,_un_autre',\n",
       "   'un_autre_empereur'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 98,\n",
       "  'stop_token_fr': 105,\n",
       "  'history_tokens': ['une_autre_justice',\n",
       "   'autre_justice_que',\n",
       "   'justice_que_notre',\n",
       "   'que_notre_parlement,'],\n",
       "  'span_tokens': ['notre_parlement,_un',\n",
       "   'parlement,_un_autre',\n",
       "   'un_autre_empereur',\n",
       "   'autre_empereur_que',\n",
       "   '__sent_marker_2',\n",
       "   'empereur_que_nous',\n",
       "   'que_nous_dans'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 101,\n",
       "  'stop_token_fr': 108,\n",
       "  'history_tokens': ['que_notre_parlement,',\n",
       "   'notre_parlement,_un',\n",
       "   'parlement,_un_autre',\n",
       "   'un_autre_empereur'],\n",
       "  'span_tokens': ['autre_empereur_que',\n",
       "   '__sent_marker_2',\n",
       "   'empereur_que_nous',\n",
       "   'que_nous_dans',\n",
       "   'nous_dans_cet',\n",
       "   'dans_cet_empire',\n",
       "   'cet_empire_!'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 105,\n",
       "  'stop_token_fr': 112,\n",
       "  'history_tokens': ['autre_empereur_que',\n",
       "   '__sent_marker_2',\n",
       "   'empereur_que_nous',\n",
       "   'que_nous_dans'],\n",
       "  'span_tokens': ['nous_dans_cet',\n",
       "   'dans_cet_empire',\n",
       "   'cet_empire_!',\n",
       "   'je_voudrais_bien_savoir',\n",
       "   'voudrais_bien_savoir_si',\n",
       "   'bien_savoir_si_c’est',\n",
       "   'savoir_si_c’est_la'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 120,\n",
       "  'stop': 128,\n",
       "  'start_token_fr': 108,\n",
       "  'stop_token_fr': 116,\n",
       "  'history_tokens': ['que_nous_dans',\n",
       "   'nous_dans_cet',\n",
       "   'dans_cet_empire',\n",
       "   'cet_empire_!'],\n",
       "  'span_tokens': ['je_voudrais_bien_savoir',\n",
       "   'voudrais_bien_savoir_si',\n",
       "   'bien_savoir_si_c’est',\n",
       "   'savoir_si_c’est_la',\n",
       "   'si_c’est_la_grâce',\n",
       "   'c’est_la_grâce_de',\n",
       "   'la_grâce_de_dieu',\n",
       "   'grâce_de_dieu_qu’il'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 124,\n",
       "  'stop': 132,\n",
       "  'start_token_fr': 112,\n",
       "  'stop_token_fr': 119,\n",
       "  'history_tokens': ['je_voudrais_bien_savoir',\n",
       "   'voudrais_bien_savoir_si',\n",
       "   'bien_savoir_si_c’est',\n",
       "   'savoir_si_c’est_la'],\n",
       "  'span_tokens': ['si_c’est_la_grâce',\n",
       "   'c’est_la_grâce_de',\n",
       "   'la_grâce_de_dieu',\n",
       "   'grâce_de_dieu_qu’il',\n",
       "   'de_dieu_qu’il_y',\n",
       "   'dieu_qu’il_y_ait',\n",
       "   'qu’il_y_ait_à'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 128,\n",
       "  'stop': 136,\n",
       "  'start_token_fr': 116,\n",
       "  'stop_token_fr': 123,\n",
       "  'history_tokens': ['si_c’est_la_grâce',\n",
       "   'c’est_la_grâce_de',\n",
       "   'la_grâce_de_dieu',\n",
       "   'grâce_de_dieu_qu’il'],\n",
       "  'span_tokens': ['de_dieu_qu’il_y',\n",
       "   'dieu_qu’il_y_ait',\n",
       "   'qu’il_y_ait_à',\n",
       "   'y_ait_à_paris',\n",
       "   'ait_à_paris_un',\n",
       "   'à_paris_un_autre',\n",
       "   'paris_un_autre_voyer'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 132,\n",
       "  'stop': 140,\n",
       "  'start_token_fr': 119,\n",
       "  'stop_token_fr': 127,\n",
       "  'history_tokens': ['grâce_de_dieu_qu’il',\n",
       "   'de_dieu_qu’il_y',\n",
       "   'dieu_qu’il_y_ait',\n",
       "   'qu’il_y_ait_à'],\n",
       "  'span_tokens': ['y_ait_à_paris',\n",
       "   'ait_à_paris_un',\n",
       "   'à_paris_un_autre',\n",
       "   'paris_un_autre_voyer',\n",
       "   'un_autre_voyer_que',\n",
       "   'autre_voyer_que_le',\n",
       "   'voyer_que_le_roi,',\n",
       "   'que_le_roi,_une'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 136,\n",
       "  'stop': 144,\n",
       "  'start_token_fr': 123,\n",
       "  'stop_token_fr': 130,\n",
       "  'history_tokens': ['y_ait_à_paris',\n",
       "   'ait_à_paris_un',\n",
       "   'à_paris_un_autre',\n",
       "   'paris_un_autre_voyer'],\n",
       "  'span_tokens': ['un_autre_voyer_que',\n",
       "   'autre_voyer_que_le',\n",
       "   'voyer_que_le_roi,',\n",
       "   'que_le_roi,_une',\n",
       "   'le_roi,_une_autre',\n",
       "   'roi,_une_autre_justice',\n",
       "   'une_autre_justice_que'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 140,\n",
       "  'stop': 148,\n",
       "  'start_token_fr': 127,\n",
       "  'stop_token_fr': 134,\n",
       "  'history_tokens': ['un_autre_voyer_que',\n",
       "   'autre_voyer_que_le',\n",
       "   'voyer_que_le_roi,',\n",
       "   'que_le_roi,_une'],\n",
       "  'span_tokens': ['le_roi,_une_autre',\n",
       "   'roi,_une_autre_justice',\n",
       "   'une_autre_justice_que',\n",
       "   'autre_justice_que_notre',\n",
       "   'justice_que_notre_parlement,',\n",
       "   'que_notre_parlement,_un',\n",
       "   'notre_parlement,_un_autre'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 144,\n",
       "  'stop': 152,\n",
       "  'start_token_fr': 130,\n",
       "  'stop_token_fr': 137,\n",
       "  'history_tokens': ['que_le_roi,_une',\n",
       "   'le_roi,_une_autre',\n",
       "   'roi,_une_autre_justice',\n",
       "   'une_autre_justice_que'],\n",
       "  'span_tokens': ['autre_justice_que_notre',\n",
       "   'justice_que_notre_parlement,',\n",
       "   'que_notre_parlement,_un',\n",
       "   'notre_parlement,_un_autre',\n",
       "   'parlement,_un_autre_empereur',\n",
       "   'un_autre_empereur_que',\n",
       "   'autre_empereur_que_nous'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 32,\n",
       "  'start': 148,\n",
       "  'stop': 156,\n",
       "  'start_token_fr': 134,\n",
       "  'stop_token_fr': 141,\n",
       "  'history_tokens': ['autre_justice_que_notre',\n",
       "   'justice_que_notre_parlement,',\n",
       "   'que_notre_parlement,_un',\n",
       "   'notre_parlement,_un_autre'],\n",
       "  'span_tokens': ['parlement,_un_autre_empereur',\n",
       "   'un_autre_empereur_que',\n",
       "   'autre_empereur_que_nous',\n",
       "   'empereur_que_nous_dans',\n",
       "   'que_nous_dans_cet',\n",
       "   'nous_dans_cet_empire',\n",
       "   'dans_cet_empire_!'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['aussi,',\n",
       "   'mon',\n",
       "   'cher',\n",
       "   'spilett,',\n",
       "   'une',\n",
       "   'éruption',\n",
       "   'serait-elle'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 3,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': ['aussi,', 'mon', 'cher'],\n",
       "  'span_tokens': ['spilett,',\n",
       "   'une',\n",
       "   'éruption',\n",
       "   'serait-elle',\n",
       "   'pour',\n",
       "   'nous',\n",
       "   'un'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 13,\n",
       "  'history_tokens': ['spilett,', 'une', 'éruption', 'serait-elle'],\n",
       "  'span_tokens': ['pour', 'nous', 'un', 'fait', 'grave,', 'et'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 17,\n",
       "  'history_tokens': ['serait-elle', 'pour', 'nous', 'un'],\n",
       "  'span_tokens': ['fait',\n",
       "   'grave,',\n",
       "   'et',\n",
       "   'vaudrait-il',\n",
       "   'beaucoup',\n",
       "   'mieux',\n",
       "   'que'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 13,\n",
       "  'stop_token_fr': 20,\n",
       "  'history_tokens': ['un', 'fait', 'grave,', 'et'],\n",
       "  'span_tokens': ['vaudrait-il',\n",
       "   'beaucoup',\n",
       "   'mieux',\n",
       "   'que',\n",
       "   'ce',\n",
       "   'volcan',\n",
       "   \"n'eût\"],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 17,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['vaudrait-il', 'beaucoup', 'mieux', 'que'],\n",
       "  'span_tokens': ['ce', 'volcan', \"n'eût\", 'pas', 'la', 'velléité', 'de'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 20,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['que', 'ce', 'volcan', \"n'eût\"],\n",
       "  'span_tokens': ['pas',\n",
       "   'la',\n",
       "   'velléité',\n",
       "   'de',\n",
       "   '__sent_marker_0',\n",
       "   'se',\n",
       "   'réveiller?'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 24,\n",
       "  'stop_token_fr': 30,\n",
       "  'history_tokens': ['pas', 'la', 'velléité', 'de'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'se',\n",
       "   'réveiller?',\n",
       "   'aussi,_mon',\n",
       "   'mon_cher',\n",
       "   'cher_spilett,'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 34,\n",
       "  'history_tokens': ['de', '__sent_marker_0', 'se', 'réveiller?'],\n",
       "  'span_tokens': ['aussi,_mon',\n",
       "   'mon_cher',\n",
       "   'cher_spilett,',\n",
       "   'spilett,_une',\n",
       "   'une_éruption',\n",
       "   'éruption_serait-elle',\n",
       "   'serait-elle_pour'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 30,\n",
       "  'stop_token_fr': 37,\n",
       "  'history_tokens': ['réveiller?', 'aussi,_mon', 'mon_cher', 'cher_spilett,'],\n",
       "  'span_tokens': ['spilett,_une',\n",
       "   'une_éruption',\n",
       "   'éruption_serait-elle',\n",
       "   'serait-elle_pour',\n",
       "   'pour_nous',\n",
       "   'nous_un',\n",
       "   'un_fait'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 34,\n",
       "  'stop_token_fr': 40,\n",
       "  'history_tokens': ['spilett,_une',\n",
       "   'une_éruption',\n",
       "   'éruption_serait-elle',\n",
       "   'serait-elle_pour'],\n",
       "  'span_tokens': ['pour_nous',\n",
       "   'nous_un',\n",
       "   'un_fait',\n",
       "   'fait_grave,',\n",
       "   'grave,_et',\n",
       "   'et_vaudrait-il'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 37,\n",
       "  'stop_token_fr': 44,\n",
       "  'history_tokens': ['serait-elle_pour', 'pour_nous', 'nous_un', 'un_fait'],\n",
       "  'span_tokens': ['fait_grave,',\n",
       "   'grave,_et',\n",
       "   'et_vaudrait-il',\n",
       "   'vaudrait-il_beaucoup',\n",
       "   'beaucoup_mieux',\n",
       "   'mieux_que',\n",
       "   'que_ce'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 40,\n",
       "  'stop_token_fr': 47,\n",
       "  'history_tokens': ['un_fait', 'fait_grave,', 'grave,_et', 'et_vaudrait-il'],\n",
       "  'span_tokens': ['vaudrait-il_beaucoup',\n",
       "   'beaucoup_mieux',\n",
       "   'mieux_que',\n",
       "   'que_ce',\n",
       "   'ce_volcan',\n",
       "   \"volcan_n'eût\",\n",
       "   \"n'eût_pas\"],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 44,\n",
       "  'stop_token_fr': 50,\n",
       "  'history_tokens': ['vaudrait-il_beaucoup',\n",
       "   'beaucoup_mieux',\n",
       "   'mieux_que',\n",
       "   'que_ce'],\n",
       "  'span_tokens': ['ce_volcan',\n",
       "   \"volcan_n'eût\",\n",
       "   \"n'eût_pas\",\n",
       "   'pas_la',\n",
       "   '__sent_marker_1',\n",
       "   'la_velléité'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 47,\n",
       "  'stop_token_fr': 54,\n",
       "  'history_tokens': ['que_ce', 'ce_volcan', \"volcan_n'eût\", \"n'eût_pas\"],\n",
       "  'span_tokens': ['pas_la',\n",
       "   '__sent_marker_1',\n",
       "   'la_velléité',\n",
       "   'velléité_de',\n",
       "   'de_se',\n",
       "   'se_réveiller?',\n",
       "   'aussi,_mon_cher'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 50,\n",
       "  'stop_token_fr': 57,\n",
       "  'history_tokens': [\"n'eût_pas\", 'pas_la', '__sent_marker_1', 'la_velléité'],\n",
       "  'span_tokens': ['velléité_de',\n",
       "   'de_se',\n",
       "   'se_réveiller?',\n",
       "   'aussi,_mon_cher',\n",
       "   'mon_cher_spilett,',\n",
       "   'cher_spilett,_une',\n",
       "   'spilett,_une_éruption'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 54,\n",
       "  'stop_token_fr': 61,\n",
       "  'history_tokens': ['velléité_de',\n",
       "   'de_se',\n",
       "   'se_réveiller?',\n",
       "   'aussi,_mon_cher'],\n",
       "  'span_tokens': ['mon_cher_spilett,',\n",
       "   'cher_spilett,_une',\n",
       "   'spilett,_une_éruption',\n",
       "   'une_éruption_serait-elle',\n",
       "   'éruption_serait-elle_pour',\n",
       "   'serait-elle_pour_nous',\n",
       "   'pour_nous_un'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 57,\n",
       "  'stop_token_fr': 64,\n",
       "  'history_tokens': ['aussi,_mon_cher',\n",
       "   'mon_cher_spilett,',\n",
       "   'cher_spilett,_une',\n",
       "   'spilett,_une_éruption'],\n",
       "  'span_tokens': ['une_éruption_serait-elle',\n",
       "   'éruption_serait-elle_pour',\n",
       "   'serait-elle_pour_nous',\n",
       "   'pour_nous_un',\n",
       "   'nous_un_fait',\n",
       "   'un_fait_grave,',\n",
       "   'fait_grave,_et'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 61,\n",
       "  'stop_token_fr': 67,\n",
       "  'history_tokens': ['une_éruption_serait-elle',\n",
       "   'éruption_serait-elle_pour',\n",
       "   'serait-elle_pour_nous',\n",
       "   'pour_nous_un'],\n",
       "  'span_tokens': ['nous_un_fait',\n",
       "   'un_fait_grave,',\n",
       "   'fait_grave,_et',\n",
       "   'grave,_et_vaudrait-il',\n",
       "   'et_vaudrait-il_beaucoup',\n",
       "   'vaudrait-il_beaucoup_mieux'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 64,\n",
       "  'stop_token_fr': 71,\n",
       "  'history_tokens': ['pour_nous_un',\n",
       "   'nous_un_fait',\n",
       "   'un_fait_grave,',\n",
       "   'fait_grave,_et'],\n",
       "  'span_tokens': ['grave,_et_vaudrait-il',\n",
       "   'et_vaudrait-il_beaucoup',\n",
       "   'vaudrait-il_beaucoup_mieux',\n",
       "   'beaucoup_mieux_que',\n",
       "   'mieux_que_ce',\n",
       "   'que_ce_volcan',\n",
       "   \"ce_volcan_n'eût\"],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 67,\n",
       "  'stop_token_fr': 74,\n",
       "  'history_tokens': ['fait_grave,_et',\n",
       "   'grave,_et_vaudrait-il',\n",
       "   'et_vaudrait-il_beaucoup',\n",
       "   'vaudrait-il_beaucoup_mieux'],\n",
       "  'span_tokens': ['beaucoup_mieux_que',\n",
       "   'mieux_que_ce',\n",
       "   'que_ce_volcan',\n",
       "   \"ce_volcan_n'eût\",\n",
       "   \"volcan_n'eût_pas\",\n",
       "   '__sent_marker_2',\n",
       "   \"n'eût_pas_la\"],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 71,\n",
       "  'stop_token_fr': 77,\n",
       "  'history_tokens': ['beaucoup_mieux_que',\n",
       "   'mieux_que_ce',\n",
       "   'que_ce_volcan',\n",
       "   \"ce_volcan_n'eût\"],\n",
       "  'span_tokens': [\"volcan_n'eût_pas\",\n",
       "   '__sent_marker_2',\n",
       "   \"n'eût_pas_la\",\n",
       "   'pas_la_velléité',\n",
       "   'la_velléité_de',\n",
       "   'velléité_de_se'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 74,\n",
       "  'stop_token_fr': 81,\n",
       "  'history_tokens': [\"ce_volcan_n'eût\",\n",
       "   \"volcan_n'eût_pas\",\n",
       "   '__sent_marker_2',\n",
       "   \"n'eût_pas_la\"],\n",
       "  'span_tokens': ['pas_la_velléité',\n",
       "   'la_velléité_de',\n",
       "   'velléité_de_se',\n",
       "   'de_se_réveiller?',\n",
       "   'aussi,_mon_cher_spilett,',\n",
       "   'mon_cher_spilett,_une',\n",
       "   'cher_spilett,_une_éruption'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 77,\n",
       "  'stop_token_fr': 84,\n",
       "  'history_tokens': [\"n'eût_pas_la\",\n",
       "   'pas_la_velléité',\n",
       "   'la_velléité_de',\n",
       "   'velléité_de_se'],\n",
       "  'span_tokens': ['de_se_réveiller?',\n",
       "   'aussi,_mon_cher_spilett,',\n",
       "   'mon_cher_spilett,_une',\n",
       "   'cher_spilett,_une_éruption',\n",
       "   'spilett,_une_éruption_serait-elle',\n",
       "   'une_éruption_serait-elle_pour',\n",
       "   'éruption_serait-elle_pour_nous'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 81,\n",
       "  'stop_token_fr': 88,\n",
       "  'history_tokens': ['de_se_réveiller?',\n",
       "   'aussi,_mon_cher_spilett,',\n",
       "   'mon_cher_spilett,_une',\n",
       "   'cher_spilett,_une_éruption'],\n",
       "  'span_tokens': ['spilett,_une_éruption_serait-elle',\n",
       "   'une_éruption_serait-elle_pour',\n",
       "   'éruption_serait-elle_pour_nous',\n",
       "   'serait-elle_pour_nous_un',\n",
       "   'pour_nous_un_fait',\n",
       "   'nous_un_fait_grave,',\n",
       "   'un_fait_grave,_et'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 84,\n",
       "  'stop_token_fr': 91,\n",
       "  'history_tokens': ['cher_spilett,_une_éruption',\n",
       "   'spilett,_une_éruption_serait-elle',\n",
       "   'une_éruption_serait-elle_pour',\n",
       "   'éruption_serait-elle_pour_nous'],\n",
       "  'span_tokens': ['serait-elle_pour_nous_un',\n",
       "   'pour_nous_un_fait',\n",
       "   'nous_un_fait_grave,',\n",
       "   'un_fait_grave,_et',\n",
       "   'fait_grave,_et_vaudrait-il',\n",
       "   'grave,_et_vaudrait-il_beaucoup',\n",
       "   'et_vaudrait-il_beaucoup_mieux'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 88,\n",
       "  'stop_token_fr': 94,\n",
       "  'history_tokens': ['serait-elle_pour_nous_un',\n",
       "   'pour_nous_un_fait',\n",
       "   'nous_un_fait_grave,',\n",
       "   'un_fait_grave,_et'],\n",
       "  'span_tokens': ['fait_grave,_et_vaudrait-il',\n",
       "   'grave,_et_vaudrait-il_beaucoup',\n",
       "   'et_vaudrait-il_beaucoup_mieux',\n",
       "   'vaudrait-il_beaucoup_mieux_que',\n",
       "   'beaucoup_mieux_que_ce',\n",
       "   'mieux_que_ce_volcan'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 91,\n",
       "  'stop_token_fr': 98,\n",
       "  'history_tokens': ['un_fait_grave,_et',\n",
       "   'fait_grave,_et_vaudrait-il',\n",
       "   'grave,_et_vaudrait-il_beaucoup',\n",
       "   'et_vaudrait-il_beaucoup_mieux'],\n",
       "  'span_tokens': ['vaudrait-il_beaucoup_mieux_que',\n",
       "   'beaucoup_mieux_que_ce',\n",
       "   'mieux_que_ce_volcan',\n",
       "   \"que_ce_volcan_n'eût\",\n",
       "   \"ce_volcan_n'eût_pas\",\n",
       "   \"volcan_n'eût_pas_la\",\n",
       "   \"n'eût_pas_la_velléité\"],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 33,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 94,\n",
       "  'stop_token_fr': 101,\n",
       "  'history_tokens': ['et_vaudrait-il_beaucoup_mieux',\n",
       "   'vaudrait-il_beaucoup_mieux_que',\n",
       "   'beaucoup_mieux_que_ce',\n",
       "   'mieux_que_ce_volcan'],\n",
       "  'span_tokens': [\"que_ce_volcan_n'eût\",\n",
       "   \"ce_volcan_n'eût_pas\",\n",
       "   \"volcan_n'eût_pas_la\",\n",
       "   \"n'eût_pas_la_velléité\",\n",
       "   'pas_la_velléité_de',\n",
       "   'la_velléité_de_se',\n",
       "   'velléité_de_se_réveiller?'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['malgré',\n",
       "   'la',\n",
       "   'douceur',\n",
       "   'inaltérable',\n",
       "   'de',\n",
       "   'son',\n",
       "   'caractère,'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 3,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': ['malgré', 'la', 'douceur'],\n",
       "  'span_tokens': ['inaltérable',\n",
       "   'de',\n",
       "   'son',\n",
       "   'caractère,',\n",
       "   'elle',\n",
       "   'fut',\n",
       "   'plusieurs'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 13,\n",
       "  'history_tokens': ['inaltérable', 'de', 'son', 'caractère,'],\n",
       "  'span_tokens': ['elle', 'fut', 'plusieurs', 'fois', 'sur', 'le'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 17,\n",
       "  'history_tokens': ['caractère,', 'elle', 'fut', 'plusieurs'],\n",
       "  'span_tokens': ['fois', 'sur', 'le', 'point', 'de', 'faire', 'entendre'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 13,\n",
       "  'stop_token_fr': 20,\n",
       "  'history_tokens': ['plusieurs', 'fois', 'sur', 'le'],\n",
       "  'span_tokens': ['point', 'de', 'faire', 'entendre', 'à', 'son', 'amie'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 17,\n",
       "  'stop_token_fr': 23,\n",
       "  'history_tokens': ['point', 'de', 'faire', 'entendre'],\n",
       "  'span_tokens': ['à', 'son', 'amie', 'combien', 'elle', '__sent_marker_0'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 20,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['entendre', 'à', 'son', 'amie'],\n",
       "  'span_tokens': ['combien',\n",
       "   'elle',\n",
       "   '__sent_marker_0',\n",
       "   'était',\n",
       "   'importune.',\n",
       "   'malgré_la',\n",
       "   'la_douceur'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 23,\n",
       "  'stop_token_fr': 30,\n",
       "  'history_tokens': ['amie', 'combien', 'elle', '__sent_marker_0'],\n",
       "  'span_tokens': ['était',\n",
       "   'importune.',\n",
       "   'malgré_la',\n",
       "   'la_douceur',\n",
       "   'douceur_inaltérable',\n",
       "   'inaltérable_de',\n",
       "   'de_son'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 33,\n",
       "  'history_tokens': ['était', 'importune.', 'malgré_la', 'la_douceur'],\n",
       "  'span_tokens': ['douceur_inaltérable',\n",
       "   'inaltérable_de',\n",
       "   'de_son',\n",
       "   'son_caractère,',\n",
       "   'caractère,_elle',\n",
       "   'elle_fut'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 30,\n",
       "  'stop_token_fr': 37,\n",
       "  'history_tokens': ['la_douceur',\n",
       "   'douceur_inaltérable',\n",
       "   'inaltérable_de',\n",
       "   'de_son'],\n",
       "  'span_tokens': ['son_caractère,',\n",
       "   'caractère,_elle',\n",
       "   'elle_fut',\n",
       "   'fut_plusieurs',\n",
       "   'plusieurs_fois',\n",
       "   'fois_sur',\n",
       "   'sur_le'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 33,\n",
       "  'stop_token_fr': 40,\n",
       "  'history_tokens': ['de_son',\n",
       "   'son_caractère,',\n",
       "   'caractère,_elle',\n",
       "   'elle_fut'],\n",
       "  'span_tokens': ['fut_plusieurs',\n",
       "   'plusieurs_fois',\n",
       "   'fois_sur',\n",
       "   'sur_le',\n",
       "   'le_point',\n",
       "   'point_de',\n",
       "   'de_faire'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 37,\n",
       "  'stop_token_fr': 43,\n",
       "  'history_tokens': ['fut_plusieurs', 'plusieurs_fois', 'fois_sur', 'sur_le'],\n",
       "  'span_tokens': ['le_point',\n",
       "   'point_de',\n",
       "   'de_faire',\n",
       "   'faire_entendre',\n",
       "   'entendre_à',\n",
       "   'à_son'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 40,\n",
       "  'stop_token_fr': 46,\n",
       "  'history_tokens': ['sur_le', 'le_point', 'point_de', 'de_faire'],\n",
       "  'span_tokens': ['faire_entendre',\n",
       "   'entendre_à',\n",
       "   'à_son',\n",
       "   'son_amie',\n",
       "   '__sent_marker_1',\n",
       "   'amie_combien'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 43,\n",
       "  'stop_token_fr': 50,\n",
       "  'history_tokens': ['de_faire', 'faire_entendre', 'entendre_à', 'à_son'],\n",
       "  'span_tokens': ['son_amie',\n",
       "   '__sent_marker_1',\n",
       "   'amie_combien',\n",
       "   'combien_elle',\n",
       "   'elle_était',\n",
       "   'était_importune.',\n",
       "   'malgré_la_douceur'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 46,\n",
       "  'stop_token_fr': 53,\n",
       "  'history_tokens': ['à_son', 'son_amie', '__sent_marker_1', 'amie_combien'],\n",
       "  'span_tokens': ['combien_elle',\n",
       "   'elle_était',\n",
       "   'était_importune.',\n",
       "   'malgré_la_douceur',\n",
       "   'la_douceur_inaltérable',\n",
       "   'douceur_inaltérable_de',\n",
       "   'inaltérable_de_son'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 50,\n",
       "  'stop_token_fr': 56,\n",
       "  'history_tokens': ['combien_elle',\n",
       "   'elle_était',\n",
       "   'était_importune.',\n",
       "   'malgré_la_douceur'],\n",
       "  'span_tokens': ['la_douceur_inaltérable',\n",
       "   'douceur_inaltérable_de',\n",
       "   'inaltérable_de_son',\n",
       "   'de_son_caractère,',\n",
       "   'son_caractère,_elle',\n",
       "   'caractère,_elle_fut'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 53,\n",
       "  'stop_token_fr': 60,\n",
       "  'history_tokens': ['malgré_la_douceur',\n",
       "   'la_douceur_inaltérable',\n",
       "   'douceur_inaltérable_de',\n",
       "   'inaltérable_de_son'],\n",
       "  'span_tokens': ['de_son_caractère,',\n",
       "   'son_caractère,_elle',\n",
       "   'caractère,_elle_fut',\n",
       "   'elle_fut_plusieurs',\n",
       "   'fut_plusieurs_fois',\n",
       "   'plusieurs_fois_sur',\n",
       "   'fois_sur_le'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 56,\n",
       "  'stop_token_fr': 63,\n",
       "  'history_tokens': ['inaltérable_de_son',\n",
       "   'de_son_caractère,',\n",
       "   'son_caractère,_elle',\n",
       "   'caractère,_elle_fut'],\n",
       "  'span_tokens': ['elle_fut_plusieurs',\n",
       "   'fut_plusieurs_fois',\n",
       "   'plusieurs_fois_sur',\n",
       "   'fois_sur_le',\n",
       "   'sur_le_point',\n",
       "   'le_point_de',\n",
       "   'point_de_faire'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 60,\n",
       "  'stop_token_fr': 66,\n",
       "  'history_tokens': ['elle_fut_plusieurs',\n",
       "   'fut_plusieurs_fois',\n",
       "   'plusieurs_fois_sur',\n",
       "   'fois_sur_le'],\n",
       "  'span_tokens': ['sur_le_point',\n",
       "   'le_point_de',\n",
       "   'point_de_faire',\n",
       "   'de_faire_entendre',\n",
       "   'faire_entendre_à',\n",
       "   'entendre_à_son'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 63,\n",
       "  'stop_token_fr': 70,\n",
       "  'history_tokens': ['fois_sur_le',\n",
       "   'sur_le_point',\n",
       "   'le_point_de',\n",
       "   'point_de_faire'],\n",
       "  'span_tokens': ['de_faire_entendre',\n",
       "   'faire_entendre_à',\n",
       "   'entendre_à_son',\n",
       "   '__sent_marker_2',\n",
       "   'à_son_amie',\n",
       "   'son_amie_combien',\n",
       "   'amie_combien_elle'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 66,\n",
       "  'stop_token_fr': 73,\n",
       "  'history_tokens': ['point_de_faire',\n",
       "   'de_faire_entendre',\n",
       "   'faire_entendre_à',\n",
       "   'entendre_à_son'],\n",
       "  'span_tokens': ['__sent_marker_2',\n",
       "   'à_son_amie',\n",
       "   'son_amie_combien',\n",
       "   'amie_combien_elle',\n",
       "   'combien_elle_était',\n",
       "   'elle_était_importune.',\n",
       "   'malgré_la_douceur_inaltérable'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 70,\n",
       "  'stop_token_fr': 76,\n",
       "  'history_tokens': ['__sent_marker_2',\n",
       "   'à_son_amie',\n",
       "   'son_amie_combien',\n",
       "   'amie_combien_elle'],\n",
       "  'span_tokens': ['combien_elle_était',\n",
       "   'elle_était_importune.',\n",
       "   'malgré_la_douceur_inaltérable',\n",
       "   'la_douceur_inaltérable_de',\n",
       "   'douceur_inaltérable_de_son',\n",
       "   'inaltérable_de_son_caractère,'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 73,\n",
       "  'stop_token_fr': 80,\n",
       "  'history_tokens': ['amie_combien_elle',\n",
       "   'combien_elle_était',\n",
       "   'elle_était_importune.',\n",
       "   'malgré_la_douceur_inaltérable'],\n",
       "  'span_tokens': ['la_douceur_inaltérable_de',\n",
       "   'douceur_inaltérable_de_son',\n",
       "   'inaltérable_de_son_caractère,',\n",
       "   'de_son_caractère,_elle',\n",
       "   'son_caractère,_elle_fut',\n",
       "   'caractère,_elle_fut_plusieurs',\n",
       "   'elle_fut_plusieurs_fois'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 76,\n",
       "  'stop_token_fr': 83,\n",
       "  'history_tokens': ['malgré_la_douceur_inaltérable',\n",
       "   'la_douceur_inaltérable_de',\n",
       "   'douceur_inaltérable_de_son',\n",
       "   'inaltérable_de_son_caractère,'],\n",
       "  'span_tokens': ['de_son_caractère,_elle',\n",
       "   'son_caractère,_elle_fut',\n",
       "   'caractère,_elle_fut_plusieurs',\n",
       "   'elle_fut_plusieurs_fois',\n",
       "   'fut_plusieurs_fois_sur',\n",
       "   'plusieurs_fois_sur_le',\n",
       "   'fois_sur_le_point'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 80,\n",
       "  'stop_token_fr': 86,\n",
       "  'history_tokens': ['de_son_caractère,_elle',\n",
       "   'son_caractère,_elle_fut',\n",
       "   'caractère,_elle_fut_plusieurs',\n",
       "   'elle_fut_plusieurs_fois'],\n",
       "  'span_tokens': ['fut_plusieurs_fois_sur',\n",
       "   'plusieurs_fois_sur_le',\n",
       "   'fois_sur_le_point',\n",
       "   'sur_le_point_de',\n",
       "   'le_point_de_faire',\n",
       "   'point_de_faire_entendre'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 83,\n",
       "  'stop_token_fr': 90,\n",
       "  'history_tokens': ['elle_fut_plusieurs_fois',\n",
       "   'fut_plusieurs_fois_sur',\n",
       "   'plusieurs_fois_sur_le',\n",
       "   'fois_sur_le_point'],\n",
       "  'span_tokens': ['sur_le_point_de',\n",
       "   'le_point_de_faire',\n",
       "   'point_de_faire_entendre',\n",
       "   'de_faire_entendre_à',\n",
       "   'faire_entendre_à_son',\n",
       "   'entendre_à_son_amie',\n",
       "   'à_son_amie_combien'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 34,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 86,\n",
       "  'stop_token_fr': 93,\n",
       "  'history_tokens': ['fois_sur_le_point',\n",
       "   'sur_le_point_de',\n",
       "   'le_point_de_faire',\n",
       "   'point_de_faire_entendre'],\n",
       "  'span_tokens': ['de_faire_entendre_à',\n",
       "   'faire_entendre_à_son',\n",
       "   'entendre_à_son_amie',\n",
       "   'à_son_amie_combien',\n",
       "   'son_amie_combien_elle',\n",
       "   'amie_combien_elle_était',\n",
       "   'combien_elle_était_importune.'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 35,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['elle', 'aurait', 'de', 'plus', 'l’occasion', 'de', 'voir'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 35,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['elle', 'aurait', 'de', 'plus'],\n",
       "  'span_tokens': ['l’occasion',\n",
       "   'de',\n",
       "   'voir',\n",
       "   'jane',\n",
       "   '__sent_marker_0',\n",
       "   'au',\n",
       "   'passage.'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 35,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 15,\n",
       "  'history_tokens': ['plus', 'l’occasion', 'de', 'voir'],\n",
       "  'span_tokens': ['jane',\n",
       "   '__sent_marker_0',\n",
       "   'au',\n",
       "   'passage.',\n",
       "   'elle_aurait',\n",
       "   'aurait_de',\n",
       "   'de_plus',\n",
       "   'plus_l’occasion'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 35,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['jane', '__sent_marker_0', 'au', 'passage.'],\n",
       "  'span_tokens': ['elle_aurait',\n",
       "   'aurait_de',\n",
       "   'de_plus',\n",
       "   'plus_l’occasion',\n",
       "   'l’occasion_de',\n",
       "   '__sent_marker_1',\n",
       "   'de_voir'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 35,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 15,\n",
       "  'stop_token_fr': 22,\n",
       "  'history_tokens': ['elle_aurait', 'aurait_de', 'de_plus', 'plus_l’occasion'],\n",
       "  'span_tokens': ['l’occasion_de',\n",
       "   '__sent_marker_1',\n",
       "   'de_voir',\n",
       "   'voir_jane',\n",
       "   'jane_au',\n",
       "   'au_passage.',\n",
       "   'elle_aurait_de'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 35,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 18,\n",
       "  'stop_token_fr': 26,\n",
       "  'history_tokens': ['plus_l’occasion',\n",
       "   'l’occasion_de',\n",
       "   '__sent_marker_1',\n",
       "   'de_voir'],\n",
       "  'span_tokens': ['voir_jane',\n",
       "   'jane_au',\n",
       "   'au_passage.',\n",
       "   'elle_aurait_de',\n",
       "   'aurait_de_plus',\n",
       "   'de_plus_l’occasion',\n",
       "   '__sent_marker_2',\n",
       "   'plus_l’occasion_de'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 35,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 22,\n",
       "  'stop_token_fr': 30,\n",
       "  'history_tokens': ['voir_jane', 'jane_au', 'au_passage.', 'elle_aurait_de'],\n",
       "  'span_tokens': ['aurait_de_plus',\n",
       "   'de_plus_l’occasion',\n",
       "   '__sent_marker_2',\n",
       "   'plus_l’occasion_de',\n",
       "   'l’occasion_de_voir',\n",
       "   'de_voir_jane',\n",
       "   'voir_jane_au',\n",
       "   'jane_au_passage.'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 35,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 26,\n",
       "  'stop_token_fr': 33,\n",
       "  'history_tokens': ['aurait_de_plus',\n",
       "   'de_plus_l’occasion',\n",
       "   '__sent_marker_2',\n",
       "   'plus_l’occasion_de'],\n",
       "  'span_tokens': ['l’occasion_de_voir',\n",
       "   'de_voir_jane',\n",
       "   'voir_jane_au',\n",
       "   'jane_au_passage.',\n",
       "   'elle_aurait_de_plus',\n",
       "   'aurait_de_plus_l’occasion',\n",
       "   'de_plus_l’occasion_de'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 35,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 30,\n",
       "  'stop_token_fr': 37,\n",
       "  'history_tokens': ['l’occasion_de_voir',\n",
       "   'de_voir_jane',\n",
       "   'voir_jane_au',\n",
       "   'jane_au_passage.'],\n",
       "  'span_tokens': ['elle_aurait_de_plus',\n",
       "   'aurait_de_plus_l’occasion',\n",
       "   'de_plus_l’occasion_de',\n",
       "   'plus_l’occasion_de_voir',\n",
       "   'l’occasion_de_voir_jane',\n",
       "   'de_voir_jane_au',\n",
       "   'voir_jane_au_passage.'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 36,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['dieu',\n",
       "   'm’en',\n",
       "   'garde',\n",
       "   '!',\n",
       "   'elle',\n",
       "   'me',\n",
       "   'volerait',\n",
       "   'mon',\n",
       "   'enfant',\n",
       "   '!'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 36,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 5,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['m’en', 'garde', '!', 'elle'],\n",
       "  'span_tokens': ['me',\n",
       "   'volerait',\n",
       "   'mon',\n",
       "   'enfant',\n",
       "   '!',\n",
       "   '–',\n",
       "   'viens,',\n",
       "   'eustache',\n",
       "   '__sent_marker_0'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 36,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 19,\n",
       "  'history_tokens': ['volerait', 'mon', 'enfant', '!'],\n",
       "  'span_tokens': ['–',\n",
       "   'viens,',\n",
       "   'eustache',\n",
       "   '__sent_marker_0',\n",
       "   '!',\n",
       "   '»',\n",
       "   'dieu_m’en',\n",
       "   'm’en_garde',\n",
       "   'garde_!'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 36,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['–', 'viens,', 'eustache', '__sent_marker_0'],\n",
       "  'span_tokens': ['!',\n",
       "   '»',\n",
       "   'dieu_m’en',\n",
       "   'm’en_garde',\n",
       "   'garde_!',\n",
       "   '!_elle',\n",
       "   'elle_me',\n",
       "   'me_volerait',\n",
       "   'volerait_mon',\n",
       "   'mon_enfant'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 36,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 19,\n",
       "  'stop_token_fr': 28,\n",
       "  'history_tokens': ['»', 'dieu_m’en', 'm’en_garde', 'garde_!'],\n",
       "  'span_tokens': ['!_elle',\n",
       "   'elle_me',\n",
       "   'me_volerait',\n",
       "   'volerait_mon',\n",
       "   'mon_enfant',\n",
       "   'enfant_!',\n",
       "   '!_–',\n",
       "   '__sent_marker_1',\n",
       "   '–_viens,'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 36,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 24,\n",
       "  'stop_token_fr': 33,\n",
       "  'history_tokens': ['elle_me', 'me_volerait', 'volerait_mon', 'mon_enfant'],\n",
       "  'span_tokens': ['enfant_!',\n",
       "   '!_–',\n",
       "   '__sent_marker_1',\n",
       "   '–_viens,',\n",
       "   'viens,_eustache',\n",
       "   'eustache_!',\n",
       "   '!_»',\n",
       "   'dieu_m’en_garde',\n",
       "   'm’en_garde_!'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 36,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 28,\n",
       "  'stop_token_fr': 38,\n",
       "  'history_tokens': ['enfant_!', '!_–', '__sent_marker_1', '–_viens,'],\n",
       "  'span_tokens': ['viens,_eustache',\n",
       "   'eustache_!',\n",
       "   '!_»',\n",
       "   'dieu_m’en_garde',\n",
       "   'm’en_garde_!',\n",
       "   'garde_!_elle',\n",
       "   '!_elle_me',\n",
       "   'elle_me_volerait',\n",
       "   'me_volerait_mon',\n",
       "   'volerait_mon_enfant'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 36,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 33,\n",
       "  'stop_token_fr': 43,\n",
       "  'history_tokens': ['eustache_!', '!_»', 'dieu_m’en_garde', 'm’en_garde_!'],\n",
       "  'span_tokens': ['garde_!_elle',\n",
       "   '!_elle_me',\n",
       "   'elle_me_volerait',\n",
       "   'me_volerait_mon',\n",
       "   'volerait_mon_enfant',\n",
       "   'mon_enfant_!',\n",
       "   '__sent_marker_2',\n",
       "   'enfant_!_–',\n",
       "   '!_–_viens,',\n",
       "   '–_viens,_eustache'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 36,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 38,\n",
       "  'stop_token_fr': 48,\n",
       "  'history_tokens': ['!_elle_me',\n",
       "   'elle_me_volerait',\n",
       "   'me_volerait_mon',\n",
       "   'volerait_mon_enfant'],\n",
       "  'span_tokens': ['mon_enfant_!',\n",
       "   '__sent_marker_2',\n",
       "   'enfant_!_–',\n",
       "   '!_–_viens,',\n",
       "   '–_viens,_eustache',\n",
       "   'viens,_eustache_!',\n",
       "   'eustache_!_»',\n",
       "   'dieu_m’en_garde_!',\n",
       "   'm’en_garde_!_elle',\n",
       "   'garde_!_elle_me'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 36,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 43,\n",
       "  'stop_token_fr': 52,\n",
       "  'history_tokens': ['__sent_marker_2',\n",
       "   'enfant_!_–',\n",
       "   '!_–_viens,',\n",
       "   '–_viens,_eustache'],\n",
       "  'span_tokens': ['viens,_eustache_!',\n",
       "   'eustache_!_»',\n",
       "   'dieu_m’en_garde_!',\n",
       "   'm’en_garde_!_elle',\n",
       "   'garde_!_elle_me',\n",
       "   '!_elle_me_volerait',\n",
       "   'elle_me_volerait_mon',\n",
       "   'me_volerait_mon_enfant',\n",
       "   'volerait_mon_enfant_!'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 36,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 48,\n",
       "  'stop_token_fr': 57,\n",
       "  'history_tokens': ['eustache_!_»',\n",
       "   'dieu_m’en_garde_!',\n",
       "   'm’en_garde_!_elle',\n",
       "   'garde_!_elle_me'],\n",
       "  'span_tokens': ['!_elle_me_volerait',\n",
       "   'elle_me_volerait_mon',\n",
       "   'me_volerait_mon_enfant',\n",
       "   'volerait_mon_enfant_!',\n",
       "   'mon_enfant_!_–',\n",
       "   'enfant_!_–_viens,',\n",
       "   '!_–_viens,_eustache',\n",
       "   '–_viens,_eustache_!',\n",
       "   'viens,_eustache_!_»'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 37,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': [\"j'allai\",\n",
       "   '__sent_marker_0',\n",
       "   '__sent_marker_1',\n",
       "   '__sent_marker_2',\n",
       "   'à',\n",
       "   'lui.',\n",
       "   \"j'allai_à\"],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 37,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': [\"j'allai\",\n",
       "   '__sent_marker_0',\n",
       "   '__sent_marker_1',\n",
       "   '__sent_marker_2'],\n",
       "  'span_tokens': ['à',\n",
       "   'lui.',\n",
       "   \"j'allai_à\",\n",
       "   'à_lui.',\n",
       "   \"j'allai_à_lui.\",\n",
       "   \"j'allai_dup0\",\n",
       "   'à_dup1'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 37,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['__sent_marker_2', 'à', 'lui.', \"j'allai_à\"],\n",
       "  'span_tokens': ['à_lui.',\n",
       "   \"j'allai_à_lui.\",\n",
       "   \"j'allai_dup0\",\n",
       "   'à_dup1',\n",
       "   'lui._dup2',\n",
       "   \"j'allai_dup3\",\n",
       "   'à_dup4'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 37,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['à_lui.', \"j'allai_à_lui.\", \"j'allai_dup0\", 'à_dup1'],\n",
       "  'span_tokens': ['lui._dup2',\n",
       "   \"j'allai_dup3\",\n",
       "   'à_dup4',\n",
       "   'lui._dup5',\n",
       "   \"j'allai_dup6\",\n",
       "   'à_dup7',\n",
       "   'lui._dup8'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 37,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['à_dup1', 'lui._dup2', \"j'allai_dup3\", 'à_dup4'],\n",
       "  'span_tokens': ['lui._dup5',\n",
       "   \"j'allai_dup6\",\n",
       "   'à_dup7',\n",
       "   'lui._dup8',\n",
       "   \"j'allai_dup9\",\n",
       "   'à_dup10',\n",
       "   'lui._dup11'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 8,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['de',\n",
       "   'leur',\n",
       "   'côté,',\n",
       "   'les',\n",
       "   'trois',\n",
       "   'amis',\n",
       "   'avaient',\n",
       "   'chargé'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 12,\n",
       "  'history_tokens': ['de', 'leur', 'côté,', 'les'],\n",
       "  'span_tokens': ['trois',\n",
       "   'amis',\n",
       "   'avaient',\n",
       "   'chargé',\n",
       "   'leurs',\n",
       "   'armes;',\n",
       "   'une',\n",
       "   'seconde'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 8,\n",
       "  'stop_token_fr': 16,\n",
       "  'history_tokens': ['trois', 'amis', 'avaient', 'chargé'],\n",
       "  'span_tokens': ['leurs',\n",
       "   'armes;',\n",
       "   'une',\n",
       "   'seconde',\n",
       "   'décharge',\n",
       "   'suivit',\n",
       "   'la',\n",
       "   'première:'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 12,\n",
       "  'stop_token_fr': 20,\n",
       "  'history_tokens': ['leurs', 'armes;', 'une', 'seconde'],\n",
       "  'span_tokens': ['décharge',\n",
       "   'suivit',\n",
       "   'la',\n",
       "   'première:',\n",
       "   'le',\n",
       "   'brigadier',\n",
       "   'et',\n",
       "   'deux'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 16,\n",
       "  'stop_token_fr': 23,\n",
       "  'history_tokens': ['décharge', 'suivit', 'la', 'première:'],\n",
       "  'span_tokens': ['le',\n",
       "   'brigadier',\n",
       "   'et',\n",
       "   'deux',\n",
       "   'pionniers',\n",
       "   'tombèrent',\n",
       "   'morts,'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 20,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['le', 'brigadier', 'et', 'deux'],\n",
       "  'span_tokens': ['pionniers',\n",
       "   'tombèrent',\n",
       "   'morts,',\n",
       "   'le',\n",
       "   'reste',\n",
       "   'de',\n",
       "   'la'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 23,\n",
       "  'stop_token_fr': 31,\n",
       "  'history_tokens': ['deux', 'pionniers', 'tombèrent', 'morts,'],\n",
       "  'span_tokens': ['le',\n",
       "   'reste',\n",
       "   'de',\n",
       "   'la',\n",
       "   'troupe',\n",
       "   'prit',\n",
       "   '__sent_marker_0',\n",
       "   'la'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 35,\n",
       "  'history_tokens': ['le', 'reste', 'de', 'la'],\n",
       "  'span_tokens': ['troupe',\n",
       "   'prit',\n",
       "   '__sent_marker_0',\n",
       "   'la',\n",
       "   'fuite.',\n",
       "   'de_leur',\n",
       "   'leur_côté,',\n",
       "   'côté,_les'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 31,\n",
       "  'stop_token_fr': 39,\n",
       "  'history_tokens': ['troupe', 'prit', '__sent_marker_0', 'la'],\n",
       "  'span_tokens': ['fuite.',\n",
       "   'de_leur',\n",
       "   'leur_côté,',\n",
       "   'côté,_les',\n",
       "   'les_trois',\n",
       "   'trois_amis',\n",
       "   'amis_avaient',\n",
       "   'avaient_chargé'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 35,\n",
       "  'stop_token_fr': 43,\n",
       "  'history_tokens': ['fuite.', 'de_leur', 'leur_côté,', 'côté,_les'],\n",
       "  'span_tokens': ['les_trois',\n",
       "   'trois_amis',\n",
       "   'amis_avaient',\n",
       "   'avaient_chargé',\n",
       "   'chargé_leurs',\n",
       "   'leurs_armes;',\n",
       "   'armes;_une',\n",
       "   'une_seconde'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 39,\n",
       "  'stop_token_fr': 47,\n",
       "  'history_tokens': ['les_trois',\n",
       "   'trois_amis',\n",
       "   'amis_avaient',\n",
       "   'avaient_chargé'],\n",
       "  'span_tokens': ['chargé_leurs',\n",
       "   'leurs_armes;',\n",
       "   'armes;_une',\n",
       "   'une_seconde',\n",
       "   'seconde_décharge',\n",
       "   'décharge_suivit',\n",
       "   'suivit_la',\n",
       "   'la_première:'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 43,\n",
       "  'stop_token_fr': 51,\n",
       "  'history_tokens': ['chargé_leurs',\n",
       "   'leurs_armes;',\n",
       "   'armes;_une',\n",
       "   'une_seconde'],\n",
       "  'span_tokens': ['seconde_décharge',\n",
       "   'décharge_suivit',\n",
       "   'suivit_la',\n",
       "   'la_première:',\n",
       "   'première:_le',\n",
       "   'le_brigadier',\n",
       "   'brigadier_et',\n",
       "   'et_deux'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 47,\n",
       "  'stop_token_fr': 55,\n",
       "  'history_tokens': ['seconde_décharge',\n",
       "   'décharge_suivit',\n",
       "   'suivit_la',\n",
       "   'la_première:'],\n",
       "  'span_tokens': ['première:_le',\n",
       "   'le_brigadier',\n",
       "   'brigadier_et',\n",
       "   'et_deux',\n",
       "   'deux_pionniers',\n",
       "   'pionniers_tombèrent',\n",
       "   'tombèrent_morts,',\n",
       "   'morts,_le'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 51,\n",
       "  'stop_token_fr': 59,\n",
       "  'history_tokens': ['première:_le',\n",
       "   'le_brigadier',\n",
       "   'brigadier_et',\n",
       "   'et_deux'],\n",
       "  'span_tokens': ['deux_pionniers',\n",
       "   'pionniers_tombèrent',\n",
       "   'tombèrent_morts,',\n",
       "   'morts,_le',\n",
       "   'le_reste',\n",
       "   'reste_de',\n",
       "   'de_la',\n",
       "   '__sent_marker_1'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 55,\n",
       "  'stop_token_fr': 62,\n",
       "  'history_tokens': ['deux_pionniers',\n",
       "   'pionniers_tombèrent',\n",
       "   'tombèrent_morts,',\n",
       "   'morts,_le'],\n",
       "  'span_tokens': ['le_reste',\n",
       "   'reste_de',\n",
       "   'de_la',\n",
       "   '__sent_marker_1',\n",
       "   'la_troupe',\n",
       "   'troupe_prit',\n",
       "   'prit_la'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 59,\n",
       "  'stop_token_fr': 66,\n",
       "  'history_tokens': ['le_reste', 'reste_de', 'de_la', '__sent_marker_1'],\n",
       "  'span_tokens': ['la_troupe',\n",
       "   'troupe_prit',\n",
       "   'prit_la',\n",
       "   'la_fuite.',\n",
       "   'de_leur_côté,',\n",
       "   'leur_côté,_les',\n",
       "   'côté,_les_trois'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 62,\n",
       "  'stop_token_fr': 70,\n",
       "  'history_tokens': ['__sent_marker_1', 'la_troupe', 'troupe_prit', 'prit_la'],\n",
       "  'span_tokens': ['la_fuite.',\n",
       "   'de_leur_côté,',\n",
       "   'leur_côté,_les',\n",
       "   'côté,_les_trois',\n",
       "   'les_trois_amis',\n",
       "   'trois_amis_avaient',\n",
       "   'amis_avaient_chargé',\n",
       "   'avaient_chargé_leurs'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 66,\n",
       "  'stop_token_fr': 74,\n",
       "  'history_tokens': ['la_fuite.',\n",
       "   'de_leur_côté,',\n",
       "   'leur_côté,_les',\n",
       "   'côté,_les_trois'],\n",
       "  'span_tokens': ['les_trois_amis',\n",
       "   'trois_amis_avaient',\n",
       "   'amis_avaient_chargé',\n",
       "   'avaient_chargé_leurs',\n",
       "   'chargé_leurs_armes;',\n",
       "   'leurs_armes;_une',\n",
       "   'armes;_une_seconde',\n",
       "   'une_seconde_décharge'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 72,\n",
       "  'stop': 80,\n",
       "  'start_token_fr': 70,\n",
       "  'stop_token_fr': 78,\n",
       "  'history_tokens': ['les_trois_amis',\n",
       "   'trois_amis_avaient',\n",
       "   'amis_avaient_chargé',\n",
       "   'avaient_chargé_leurs'],\n",
       "  'span_tokens': ['chargé_leurs_armes;',\n",
       "   'leurs_armes;_une',\n",
       "   'armes;_une_seconde',\n",
       "   'une_seconde_décharge',\n",
       "   'seconde_décharge_suivit',\n",
       "   'décharge_suivit_la',\n",
       "   'suivit_la_première:',\n",
       "   'la_première:_le'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 76,\n",
       "  'stop': 84,\n",
       "  'start_token_fr': 74,\n",
       "  'stop_token_fr': 82,\n",
       "  'history_tokens': ['chargé_leurs_armes;',\n",
       "   'leurs_armes;_une',\n",
       "   'armes;_une_seconde',\n",
       "   'une_seconde_décharge'],\n",
       "  'span_tokens': ['seconde_décharge_suivit',\n",
       "   'décharge_suivit_la',\n",
       "   'suivit_la_première:',\n",
       "   'la_première:_le',\n",
       "   'première:_le_brigadier',\n",
       "   'le_brigadier_et',\n",
       "   'brigadier_et_deux',\n",
       "   'et_deux_pionniers'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 80,\n",
       "  'stop': 88,\n",
       "  'start_token_fr': 78,\n",
       "  'stop_token_fr': 86,\n",
       "  'history_tokens': ['seconde_décharge_suivit',\n",
       "   'décharge_suivit_la',\n",
       "   'suivit_la_première:',\n",
       "   'la_première:_le'],\n",
       "  'span_tokens': ['première:_le_brigadier',\n",
       "   'le_brigadier_et',\n",
       "   'brigadier_et_deux',\n",
       "   'et_deux_pionniers',\n",
       "   'deux_pionniers_tombèrent',\n",
       "   'pionniers_tombèrent_morts,',\n",
       "   'tombèrent_morts,_le',\n",
       "   'morts,_le_reste'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 84,\n",
       "  'stop': 92,\n",
       "  'start_token_fr': 82,\n",
       "  'stop_token_fr': 90,\n",
       "  'history_tokens': ['première:_le_brigadier',\n",
       "   'le_brigadier_et',\n",
       "   'brigadier_et_deux',\n",
       "   'et_deux_pionniers'],\n",
       "  'span_tokens': ['deux_pionniers_tombèrent',\n",
       "   'pionniers_tombèrent_morts,',\n",
       "   'tombèrent_morts,_le',\n",
       "   'morts,_le_reste',\n",
       "   'le_reste_de',\n",
       "   '__sent_marker_2',\n",
       "   'reste_de_la',\n",
       "   'de_la_troupe'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 88,\n",
       "  'stop': 96,\n",
       "  'start_token_fr': 86,\n",
       "  'stop_token_fr': 94,\n",
       "  'history_tokens': ['deux_pionniers_tombèrent',\n",
       "   'pionniers_tombèrent_morts,',\n",
       "   'tombèrent_morts,_le',\n",
       "   'morts,_le_reste'],\n",
       "  'span_tokens': ['le_reste_de',\n",
       "   '__sent_marker_2',\n",
       "   'reste_de_la',\n",
       "   'de_la_troupe',\n",
       "   'la_troupe_prit',\n",
       "   'troupe_prit_la',\n",
       "   'prit_la_fuite.',\n",
       "   'de_leur_côté,_les'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 92,\n",
       "  'stop': 100,\n",
       "  'start_token_fr': 90,\n",
       "  'stop_token_fr': 98,\n",
       "  'history_tokens': ['le_reste_de',\n",
       "   '__sent_marker_2',\n",
       "   'reste_de_la',\n",
       "   'de_la_troupe'],\n",
       "  'span_tokens': ['la_troupe_prit',\n",
       "   'troupe_prit_la',\n",
       "   'prit_la_fuite.',\n",
       "   'de_leur_côté,_les',\n",
       "   'leur_côté,_les_trois',\n",
       "   'côté,_les_trois_amis',\n",
       "   'les_trois_amis_avaient',\n",
       "   'trois_amis_avaient_chargé'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 96,\n",
       "  'stop': 104,\n",
       "  'start_token_fr': 94,\n",
       "  'stop_token_fr': 101,\n",
       "  'history_tokens': ['la_troupe_prit',\n",
       "   'troupe_prit_la',\n",
       "   'prit_la_fuite.',\n",
       "   'de_leur_côté,_les'],\n",
       "  'span_tokens': ['leur_côté,_les_trois',\n",
       "   'côté,_les_trois_amis',\n",
       "   'les_trois_amis_avaient',\n",
       "   'trois_amis_avaient_chargé',\n",
       "   'amis_avaient_chargé_leurs',\n",
       "   'avaient_chargé_leurs_armes;',\n",
       "   'chargé_leurs_armes;_une'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 100,\n",
       "  'stop': 108,\n",
       "  'start_token_fr': 98,\n",
       "  'stop_token_fr': 105,\n",
       "  'history_tokens': ['leur_côté,_les_trois',\n",
       "   'côté,_les_trois_amis',\n",
       "   'les_trois_amis_avaient',\n",
       "   'trois_amis_avaient_chargé'],\n",
       "  'span_tokens': ['amis_avaient_chargé_leurs',\n",
       "   'avaient_chargé_leurs_armes;',\n",
       "   'chargé_leurs_armes;_une',\n",
       "   'leurs_armes;_une_seconde',\n",
       "   'armes;_une_seconde_décharge',\n",
       "   'une_seconde_décharge_suivit',\n",
       "   'seconde_décharge_suivit_la'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 104,\n",
       "  'stop': 112,\n",
       "  'start_token_fr': 101,\n",
       "  'stop_token_fr': 109,\n",
       "  'history_tokens': ['trois_amis_avaient_chargé',\n",
       "   'amis_avaient_chargé_leurs',\n",
       "   'avaient_chargé_leurs_armes;',\n",
       "   'chargé_leurs_armes;_une'],\n",
       "  'span_tokens': ['leurs_armes;_une_seconde',\n",
       "   'armes;_une_seconde_décharge',\n",
       "   'une_seconde_décharge_suivit',\n",
       "   'seconde_décharge_suivit_la',\n",
       "   'décharge_suivit_la_première:',\n",
       "   'suivit_la_première:_le',\n",
       "   'la_première:_le_brigadier',\n",
       "   'première:_le_brigadier_et'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 108,\n",
       "  'stop': 116,\n",
       "  'start_token_fr': 105,\n",
       "  'stop_token_fr': 113,\n",
       "  'history_tokens': ['leurs_armes;_une_seconde',\n",
       "   'armes;_une_seconde_décharge',\n",
       "   'une_seconde_décharge_suivit',\n",
       "   'seconde_décharge_suivit_la'],\n",
       "  'span_tokens': ['décharge_suivit_la_première:',\n",
       "   'suivit_la_première:_le',\n",
       "   'la_première:_le_brigadier',\n",
       "   'première:_le_brigadier_et',\n",
       "   'le_brigadier_et_deux',\n",
       "   'brigadier_et_deux_pionniers',\n",
       "   'et_deux_pionniers_tombèrent',\n",
       "   'deux_pionniers_tombèrent_morts,'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 112,\n",
       "  'stop': 120,\n",
       "  'start_token_fr': 109,\n",
       "  'stop_token_fr': 117,\n",
       "  'history_tokens': ['décharge_suivit_la_première:',\n",
       "   'suivit_la_première:_le',\n",
       "   'la_première:_le_brigadier',\n",
       "   'première:_le_brigadier_et'],\n",
       "  'span_tokens': ['le_brigadier_et_deux',\n",
       "   'brigadier_et_deux_pionniers',\n",
       "   'et_deux_pionniers_tombèrent',\n",
       "   'deux_pionniers_tombèrent_morts,',\n",
       "   'pionniers_tombèrent_morts,_le',\n",
       "   'tombèrent_morts,_le_reste',\n",
       "   'morts,_le_reste_de',\n",
       "   'le_reste_de_la'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 38,\n",
       "  'start': 116,\n",
       "  'stop': 124,\n",
       "  'start_token_fr': 113,\n",
       "  'stop_token_fr': 121,\n",
       "  'history_tokens': ['le_brigadier_et_deux',\n",
       "   'brigadier_et_deux_pionniers',\n",
       "   'et_deux_pionniers_tombèrent',\n",
       "   'deux_pionniers_tombèrent_morts,'],\n",
       "  'span_tokens': ['pionniers_tombèrent_morts,_le',\n",
       "   'tombèrent_morts,_le_reste',\n",
       "   'morts,_le_reste_de',\n",
       "   'le_reste_de_la',\n",
       "   'reste_de_la_troupe',\n",
       "   'de_la_troupe_prit',\n",
       "   'la_troupe_prit_la',\n",
       "   'troupe_prit_la_fuite.'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['–',\n",
       "   'pardon,',\n",
       "   'bon',\n",
       "   'camarade',\n",
       "   'jehan,',\n",
       "   's’écria',\n",
       "   'phœbus'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['–', 'pardon,', 'bon', 'camarade'],\n",
       "  'span_tokens': ['jehan,',\n",
       "   's’écria',\n",
       "   'phœbus',\n",
       "   'en',\n",
       "   'lui',\n",
       "   'secouant',\n",
       "   'la'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 15,\n",
       "  'history_tokens': ['camarade', 'jehan,', 's’écria', 'phœbus'],\n",
       "  'span_tokens': ['en',\n",
       "   'lui',\n",
       "   'secouant',\n",
       "   'la',\n",
       "   'main,',\n",
       "   'cheval',\n",
       "   'lancé',\n",
       "   'ne'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['en', 'lui', 'secouant', 'la'],\n",
       "  'span_tokens': ['main,',\n",
       "   'cheval',\n",
       "   'lancé',\n",
       "   'ne',\n",
       "   's’arrête',\n",
       "   '__sent_marker_0',\n",
       "   'pas'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 15,\n",
       "  'stop_token_fr': 22,\n",
       "  'history_tokens': ['main,', 'cheval', 'lancé', 'ne'],\n",
       "  'span_tokens': ['s’arrête',\n",
       "   '__sent_marker_0',\n",
       "   'pas',\n",
       "   'court.',\n",
       "   '–_pardon,',\n",
       "   'pardon,_bon',\n",
       "   'bon_camarade'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 18,\n",
       "  'stop_token_fr': 25,\n",
       "  'history_tokens': ['ne', 's’arrête', '__sent_marker_0', 'pas'],\n",
       "  'span_tokens': ['court.',\n",
       "   '–_pardon,',\n",
       "   'pardon,_bon',\n",
       "   'bon_camarade',\n",
       "   'camarade_jehan,',\n",
       "   'jehan,_s’écria',\n",
       "   's’écria_phœbus'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 22,\n",
       "  'stop_token_fr': 29,\n",
       "  'history_tokens': ['court.', '–_pardon,', 'pardon,_bon', 'bon_camarade'],\n",
       "  'span_tokens': ['camarade_jehan,',\n",
       "   'jehan,_s’écria',\n",
       "   's’écria_phœbus',\n",
       "   'phœbus_en',\n",
       "   'en_lui',\n",
       "   'lui_secouant',\n",
       "   'secouant_la'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 25,\n",
       "  'stop_token_fr': 33,\n",
       "  'history_tokens': ['bon_camarade',\n",
       "   'camarade_jehan,',\n",
       "   'jehan,_s’écria',\n",
       "   's’écria_phœbus'],\n",
       "  'span_tokens': ['phœbus_en',\n",
       "   'en_lui',\n",
       "   'lui_secouant',\n",
       "   'secouant_la',\n",
       "   'la_main,',\n",
       "   'main,_cheval',\n",
       "   'cheval_lancé',\n",
       "   '__sent_marker_1'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 29,\n",
       "  'stop_token_fr': 36,\n",
       "  'history_tokens': ['phœbus_en', 'en_lui', 'lui_secouant', 'secouant_la'],\n",
       "  'span_tokens': ['la_main,',\n",
       "   'main,_cheval',\n",
       "   'cheval_lancé',\n",
       "   '__sent_marker_1',\n",
       "   'lancé_ne',\n",
       "   'ne_s’arrête',\n",
       "   's’arrête_pas'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 33,\n",
       "  'stop_token_fr': 40,\n",
       "  'history_tokens': ['la_main,',\n",
       "   'main,_cheval',\n",
       "   'cheval_lancé',\n",
       "   '__sent_marker_1'],\n",
       "  'span_tokens': ['lancé_ne',\n",
       "   'ne_s’arrête',\n",
       "   's’arrête_pas',\n",
       "   'pas_court.',\n",
       "   '–_pardon,_bon',\n",
       "   'pardon,_bon_camarade',\n",
       "   'bon_camarade_jehan,'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 36,\n",
       "  'stop_token_fr': 44,\n",
       "  'history_tokens': ['__sent_marker_1',\n",
       "   'lancé_ne',\n",
       "   'ne_s’arrête',\n",
       "   's’arrête_pas'],\n",
       "  'span_tokens': ['pas_court.',\n",
       "   '–_pardon,_bon',\n",
       "   'pardon,_bon_camarade',\n",
       "   'bon_camarade_jehan,',\n",
       "   'camarade_jehan,_s’écria',\n",
       "   'jehan,_s’écria_phœbus',\n",
       "   's’écria_phœbus_en',\n",
       "   'phœbus_en_lui'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 40,\n",
       "  'stop_token_fr': 47,\n",
       "  'history_tokens': ['pas_court.',\n",
       "   '–_pardon,_bon',\n",
       "   'pardon,_bon_camarade',\n",
       "   'bon_camarade_jehan,'],\n",
       "  'span_tokens': ['camarade_jehan,_s’écria',\n",
       "   'jehan,_s’écria_phœbus',\n",
       "   's’écria_phœbus_en',\n",
       "   'phœbus_en_lui',\n",
       "   'en_lui_secouant',\n",
       "   'lui_secouant_la',\n",
       "   'secouant_la_main,'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 44,\n",
       "  'stop_token_fr': 51,\n",
       "  'history_tokens': ['camarade_jehan,_s’écria',\n",
       "   'jehan,_s’écria_phœbus',\n",
       "   's’écria_phœbus_en',\n",
       "   'phœbus_en_lui'],\n",
       "  'span_tokens': ['en_lui_secouant',\n",
       "   'lui_secouant_la',\n",
       "   'secouant_la_main,',\n",
       "   'la_main,_cheval',\n",
       "   '__sent_marker_2',\n",
       "   'main,_cheval_lancé',\n",
       "   'cheval_lancé_ne'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 47,\n",
       "  'stop_token_fr': 54,\n",
       "  'history_tokens': ['phœbus_en_lui',\n",
       "   'en_lui_secouant',\n",
       "   'lui_secouant_la',\n",
       "   'secouant_la_main,'],\n",
       "  'span_tokens': ['la_main,_cheval',\n",
       "   '__sent_marker_2',\n",
       "   'main,_cheval_lancé',\n",
       "   'cheval_lancé_ne',\n",
       "   'lancé_ne_s’arrête',\n",
       "   'ne_s’arrête_pas',\n",
       "   's’arrête_pas_court.'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 51,\n",
       "  'stop_token_fr': 58,\n",
       "  'history_tokens': ['la_main,_cheval',\n",
       "   '__sent_marker_2',\n",
       "   'main,_cheval_lancé',\n",
       "   'cheval_lancé_ne'],\n",
       "  'span_tokens': ['lancé_ne_s’arrête',\n",
       "   'ne_s’arrête_pas',\n",
       "   's’arrête_pas_court.',\n",
       "   '–_pardon,_bon_camarade',\n",
       "   'pardon,_bon_camarade_jehan,',\n",
       "   'bon_camarade_jehan,_s’écria',\n",
       "   'camarade_jehan,_s’écria_phœbus'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 60,\n",
       "  'stop': 68,\n",
       "  'start_token_fr': 54,\n",
       "  'stop_token_fr': 62,\n",
       "  'history_tokens': ['cheval_lancé_ne',\n",
       "   'lancé_ne_s’arrête',\n",
       "   'ne_s’arrête_pas',\n",
       "   's’arrête_pas_court.'],\n",
       "  'span_tokens': ['–_pardon,_bon_camarade',\n",
       "   'pardon,_bon_camarade_jehan,',\n",
       "   'bon_camarade_jehan,_s’écria',\n",
       "   'camarade_jehan,_s’écria_phœbus',\n",
       "   'jehan,_s’écria_phœbus_en',\n",
       "   's’écria_phœbus_en_lui',\n",
       "   'phœbus_en_lui_secouant',\n",
       "   'en_lui_secouant_la'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 64,\n",
       "  'stop': 72,\n",
       "  'start_token_fr': 58,\n",
       "  'stop_token_fr': 65,\n",
       "  'history_tokens': ['–_pardon,_bon_camarade',\n",
       "   'pardon,_bon_camarade_jehan,',\n",
       "   'bon_camarade_jehan,_s’écria',\n",
       "   'camarade_jehan,_s’écria_phœbus'],\n",
       "  'span_tokens': ['jehan,_s’écria_phœbus_en',\n",
       "   's’écria_phœbus_en_lui',\n",
       "   'phœbus_en_lui_secouant',\n",
       "   'en_lui_secouant_la',\n",
       "   'lui_secouant_la_main,',\n",
       "   'secouant_la_main,_cheval',\n",
       "   'la_main,_cheval_lancé'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 39,\n",
       "  'start': 68,\n",
       "  'stop': 76,\n",
       "  'start_token_fr': 62,\n",
       "  'stop_token_fr': 69,\n",
       "  'history_tokens': ['jehan,_s’écria_phœbus_en',\n",
       "   's’écria_phœbus_en_lui',\n",
       "   'phœbus_en_lui_secouant',\n",
       "   'en_lui_secouant_la'],\n",
       "  'span_tokens': ['lui_secouant_la_main,',\n",
       "   'secouant_la_main,_cheval',\n",
       "   'la_main,_cheval_lancé',\n",
       "   'main,_cheval_lancé_ne',\n",
       "   'cheval_lancé_ne_s’arrête',\n",
       "   'lancé_ne_s’arrête_pas',\n",
       "   'ne_s’arrête_pas_court.'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 40,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['--oui!',\n",
       "   '__sent_marker_0',\n",
       "   '__sent_marker_1',\n",
       "   '__sent_marker_2',\n",
       "   '--oui!_dup0',\n",
       "   '--oui!_dup1',\n",
       "   '--oui!_dup2'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 40,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['--oui!',\n",
       "   '__sent_marker_0',\n",
       "   '__sent_marker_1',\n",
       "   '__sent_marker_2'],\n",
       "  'span_tokens': ['--oui!_dup0',\n",
       "   '--oui!_dup1',\n",
       "   '--oui!_dup2',\n",
       "   '--oui!_dup3',\n",
       "   '--oui!_dup4',\n",
       "   '--oui!_dup5',\n",
       "   '--oui!_dup6'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 40,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['__sent_marker_2',\n",
       "   '--oui!_dup0',\n",
       "   '--oui!_dup1',\n",
       "   '--oui!_dup2'],\n",
       "  'span_tokens': ['--oui!_dup3',\n",
       "   '--oui!_dup4',\n",
       "   '--oui!_dup5',\n",
       "   '--oui!_dup6',\n",
       "   '--oui!_dup7',\n",
       "   '--oui!_dup8',\n",
       "   '--oui!_dup9'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 40,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['--oui!_dup3',\n",
       "   '--oui!_dup4',\n",
       "   '--oui!_dup5',\n",
       "   '--oui!_dup6'],\n",
       "  'span_tokens': ['--oui!_dup7',\n",
       "   '--oui!_dup8',\n",
       "   '--oui!_dup9',\n",
       "   '--oui!_dup10',\n",
       "   '--oui!_dup11',\n",
       "   '--oui!_dup12',\n",
       "   '--oui!_dup13'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 40,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['--oui!_dup6',\n",
       "   '--oui!_dup7',\n",
       "   '--oui!_dup8',\n",
       "   '--oui!_dup9'],\n",
       "  'span_tokens': ['--oui!_dup10',\n",
       "   '--oui!_dup11',\n",
       "   '--oui!_dup12',\n",
       "   '--oui!_dup13',\n",
       "   '--oui!_dup14',\n",
       "   '--oui!_dup15',\n",
       "   '--oui!_dup16'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 41,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 5,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['--serez-vous', 'heureux', 'ici?', '__sent_marker_0', 'me'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 41,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 3,\n",
       "  'stop_token_fr': 8,\n",
       "  'history_tokens': ['--serez-vous', 'heureux', 'ici?'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'me',\n",
       "   'dit-elle.',\n",
       "   '__sent_marker_1',\n",
       "   '--serez-vous_heureux'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 41,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 5,\n",
       "  'stop_token_fr': 11,\n",
       "  'history_tokens': ['heureux', 'ici?', '__sent_marker_0', 'me'],\n",
       "  'span_tokens': ['dit-elle.',\n",
       "   '__sent_marker_1',\n",
       "   '--serez-vous_heureux',\n",
       "   'heureux_ici?',\n",
       "   '__sent_marker_2',\n",
       "   'ici?_me'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 41,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 8,\n",
       "  'stop_token_fr': 13,\n",
       "  'history_tokens': ['me',\n",
       "   'dit-elle.',\n",
       "   '__sent_marker_1',\n",
       "   '--serez-vous_heureux'],\n",
       "  'span_tokens': ['heureux_ici?',\n",
       "   '__sent_marker_2',\n",
       "   'ici?_me',\n",
       "   'me_dit-elle.',\n",
       "   '--serez-vous_heureux_ici?'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 41,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 11,\n",
       "  'stop_token_fr': 16,\n",
       "  'history_tokens': ['--serez-vous_heureux',\n",
       "   'heureux_ici?',\n",
       "   '__sent_marker_2',\n",
       "   'ici?_me'],\n",
       "  'span_tokens': ['me_dit-elle.',\n",
       "   '--serez-vous_heureux_ici?',\n",
       "   'heureux_ici?_me',\n",
       "   'ici?_me_dit-elle.',\n",
       "   '--serez-vous_heureux_ici?_me'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 41,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 13,\n",
       "  'stop_token_fr': 19,\n",
       "  'history_tokens': ['__sent_marker_2',\n",
       "   'ici?_me',\n",
       "   'me_dit-elle.',\n",
       "   '--serez-vous_heureux_ici?'],\n",
       "  'span_tokens': ['heureux_ici?_me',\n",
       "   'ici?_me_dit-elle.',\n",
       "   '--serez-vous_heureux_ici?_me',\n",
       "   'heureux_ici?_me_dit-elle.',\n",
       "   '--serez-vous_dup0',\n",
       "   'heureux_dup1'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 41,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 16,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['--serez-vous_heureux_ici?',\n",
       "   'heureux_ici?_me',\n",
       "   'ici?_me_dit-elle.',\n",
       "   '--serez-vous_heureux_ici?_me'],\n",
       "  'span_tokens': ['heureux_ici?_me_dit-elle.',\n",
       "   '--serez-vous_dup0',\n",
       "   'heureux_dup1',\n",
       "   'ici?_dup2',\n",
       "   'me_dup3'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 41,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 19,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['--serez-vous_heureux_ici?_me',\n",
       "   'heureux_ici?_me_dit-elle.',\n",
       "   '--serez-vous_dup0',\n",
       "   'heureux_dup1'],\n",
       "  'span_tokens': ['ici?_dup2',\n",
       "   'me_dup3',\n",
       "   'dit-elle._dup4',\n",
       "   '--serez-vous_dup5',\n",
       "   'heureux_dup6'],\n",
       "  'Z_en': array([-1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 5,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['il', 's’en', 'débarrassa', 'pourtant', 'et'],\n",
       "  'Z_en': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 2,\n",
       "  'stop_token_fr': 7,\n",
       "  'history_tokens': ['il', 's’en'],\n",
       "  'span_tokens': ['débarrassa', 'pourtant', 'et', 'courut', 'd’un'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 5,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': ['s’en', 'débarrassa', 'pourtant', 'et'],\n",
       "  'span_tokens': ['courut', 'd’un', 'bond', '__sent_marker_0', 'jusqu’à'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 7,\n",
       "  'stop_token_fr': 12,\n",
       "  'history_tokens': ['pourtant', 'et', 'courut', 'd’un'],\n",
       "  'span_tokens': ['bond', '__sent_marker_0', 'jusqu’à', 'l’hôtel.', 'il_s’en'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 15,\n",
       "  'history_tokens': ['d’un', 'bond', '__sent_marker_0', 'jusqu’à'],\n",
       "  'span_tokens': ['l’hôtel.',\n",
       "   'il_s’en',\n",
       "   's’en_débarrassa',\n",
       "   'débarrassa_pourtant',\n",
       "   'pourtant_et'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 12,\n",
       "  'stop_token_fr': 17,\n",
       "  'history_tokens': ['__sent_marker_0', 'jusqu’à', 'l’hôtel.', 'il_s’en'],\n",
       "  'span_tokens': ['s’en_débarrassa',\n",
       "   'débarrassa_pourtant',\n",
       "   'pourtant_et',\n",
       "   'et_courut',\n",
       "   '__sent_marker_1'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 15,\n",
       "  'stop_token_fr': 20,\n",
       "  'history_tokens': ['il_s’en',\n",
       "   's’en_débarrassa',\n",
       "   'débarrassa_pourtant',\n",
       "   'pourtant_et'],\n",
       "  'span_tokens': ['et_courut',\n",
       "   '__sent_marker_1',\n",
       "   'courut_d’un',\n",
       "   'd’un_bond',\n",
       "   'bond_jusqu’à'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 17,\n",
       "  'stop_token_fr': 22,\n",
       "  'history_tokens': ['débarrassa_pourtant',\n",
       "   'pourtant_et',\n",
       "   'et_courut',\n",
       "   '__sent_marker_1'],\n",
       "  'span_tokens': ['courut_d’un',\n",
       "   'd’un_bond',\n",
       "   'bond_jusqu’à',\n",
       "   'jusqu’à_l’hôtel.',\n",
       "   'il_s’en_débarrassa'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 20,\n",
       "  'stop_token_fr': 25,\n",
       "  'history_tokens': ['__sent_marker_1',\n",
       "   'courut_d’un',\n",
       "   'd’un_bond',\n",
       "   'bond_jusqu’à'],\n",
       "  'span_tokens': ['jusqu’à_l’hôtel.',\n",
       "   'il_s’en_débarrassa',\n",
       "   's’en_débarrassa_pourtant',\n",
       "   'débarrassa_pourtant_et',\n",
       "   '__sent_marker_2'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 22,\n",
       "  'stop_token_fr': 27,\n",
       "  'history_tokens': ['d’un_bond',\n",
       "   'bond_jusqu’à',\n",
       "   'jusqu’à_l’hôtel.',\n",
       "   'il_s’en_débarrassa'],\n",
       "  'span_tokens': ['s’en_débarrassa_pourtant',\n",
       "   'débarrassa_pourtant_et',\n",
       "   '__sent_marker_2',\n",
       "   'pourtant_et_courut',\n",
       "   'et_courut_d’un'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 25,\n",
       "  'stop_token_fr': 30,\n",
       "  'history_tokens': ['il_s’en_débarrassa',\n",
       "   's’en_débarrassa_pourtant',\n",
       "   'débarrassa_pourtant_et',\n",
       "   '__sent_marker_2'],\n",
       "  'span_tokens': ['pourtant_et_courut',\n",
       "   'et_courut_d’un',\n",
       "   'courut_d’un_bond',\n",
       "   'd’un_bond_jusqu’à',\n",
       "   'bond_jusqu’à_l’hôtel.'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 27,\n",
       "  'stop_token_fr': 32,\n",
       "  'history_tokens': ['débarrassa_pourtant_et',\n",
       "   '__sent_marker_2',\n",
       "   'pourtant_et_courut',\n",
       "   'et_courut_d’un'],\n",
       "  'span_tokens': ['courut_d’un_bond',\n",
       "   'd’un_bond_jusqu’à',\n",
       "   'bond_jusqu’à_l’hôtel.',\n",
       "   'il_s’en_débarrassa_pourtant',\n",
       "   's’en_débarrassa_pourtant_et'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 30,\n",
       "  'stop_token_fr': 35,\n",
       "  'history_tokens': ['et_courut_d’un',\n",
       "   'courut_d’un_bond',\n",
       "   'd’un_bond_jusqu’à',\n",
       "   'bond_jusqu’à_l’hôtel.'],\n",
       "  'span_tokens': ['il_s’en_débarrassa_pourtant',\n",
       "   's’en_débarrassa_pourtant_et',\n",
       "   'débarrassa_pourtant_et_courut',\n",
       "   'pourtant_et_courut_d’un',\n",
       "   'et_courut_d’un_bond'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 42,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 32,\n",
       "  'stop_token_fr': 37,\n",
       "  'history_tokens': ['d’un_bond_jusqu’à',\n",
       "   'bond_jusqu’à_l’hôtel.',\n",
       "   'il_s’en_débarrassa_pourtant',\n",
       "   's’en_débarrassa_pourtant_et'],\n",
       "  'span_tokens': ['débarrassa_pourtant_et_courut',\n",
       "   'pourtant_et_courut_d’un',\n",
       "   'et_courut_d’un_bond',\n",
       "   'courut_d’un_bond_jusqu’à',\n",
       "   'd’un_bond_jusqu’à_l’hôtel.'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 3,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['et', 'quacunque', 'viam'],\n",
       "  'Z_en': array([-1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 2,\n",
       "  'stop_token_fr': 4,\n",
       "  'history_tokens': ['et', 'quacunque'],\n",
       "  'span_tokens': ['viam', 'dederit'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 3,\n",
       "  'stop_token_fr': 6,\n",
       "  'history_tokens': ['et', 'quacunque', 'viam'],\n",
       "  'span_tokens': ['dederit', '__sent_marker_0', 'fortuna'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 4,\n",
       "  'stop_token_fr': 8,\n",
       "  'history_tokens': ['et', 'quacunque', 'viam', 'dederit'],\n",
       "  'span_tokens': ['__sent_marker_0', 'fortuna', 'sequamur.', 'et_quacunque'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 6,\n",
       "  'stop_token_fr': 9,\n",
       "  'history_tokens': ['viam', 'dederit', '__sent_marker_0', 'fortuna'],\n",
       "  'span_tokens': ['sequamur.', 'et_quacunque', '__sent_marker_1'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 8,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': ['__sent_marker_0',\n",
       "   'fortuna',\n",
       "   'sequamur.',\n",
       "   'et_quacunque'],\n",
       "  'span_tokens': ['__sent_marker_1', 'quacunque_viam'],\n",
       "  'Z_en': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 9,\n",
       "  'stop_token_fr': 12,\n",
       "  'history_tokens': ['fortuna',\n",
       "   'sequamur.',\n",
       "   'et_quacunque',\n",
       "   '__sent_marker_1'],\n",
       "  'span_tokens': ['quacunque_viam', 'viam_dederit', 'dederit_fortuna'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 14,\n",
       "  'history_tokens': ['sequamur.',\n",
       "   'et_quacunque',\n",
       "   '__sent_marker_1',\n",
       "   'quacunque_viam'],\n",
       "  'span_tokens': ['viam_dederit',\n",
       "   'dederit_fortuna',\n",
       "   '__sent_marker_2',\n",
       "   'fortuna_sequamur.'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 12,\n",
       "  'stop_token_fr': 15,\n",
       "  'history_tokens': ['__sent_marker_1',\n",
       "   'quacunque_viam',\n",
       "   'viam_dederit',\n",
       "   'dederit_fortuna'],\n",
       "  'span_tokens': ['__sent_marker_2', 'fortuna_sequamur.', 'et_quacunque_viam'],\n",
       "  'Z_en': array([-1, -1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 14,\n",
       "  'stop_token_fr': 16,\n",
       "  'history_tokens': ['viam_dederit',\n",
       "   'dederit_fortuna',\n",
       "   '__sent_marker_2',\n",
       "   'fortuna_sequamur.'],\n",
       "  'span_tokens': ['et_quacunque_viam', 'quacunque_viam_dederit'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 15,\n",
       "  'stop_token_fr': 18,\n",
       "  'history_tokens': ['dederit_fortuna',\n",
       "   '__sent_marker_2',\n",
       "   'fortuna_sequamur.',\n",
       "   'et_quacunque_viam'],\n",
       "  'span_tokens': ['quacunque_viam_dederit',\n",
       "   'viam_dederit_fortuna',\n",
       "   'dederit_fortuna_sequamur.'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 16,\n",
       "  'stop_token_fr': 20,\n",
       "  'history_tokens': ['__sent_marker_2',\n",
       "   'fortuna_sequamur.',\n",
       "   'et_quacunque_viam',\n",
       "   'quacunque_viam_dederit'],\n",
       "  'span_tokens': ['viam_dederit_fortuna',\n",
       "   'dederit_fortuna_sequamur.',\n",
       "   'et_quacunque_viam_dederit',\n",
       "   'quacunque_viam_dederit_fortuna'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 48,\n",
       "  'stop': 56,\n",
       "  'start_token_fr': 18,\n",
       "  'stop_token_fr': 21,\n",
       "  'history_tokens': ['et_quacunque_viam',\n",
       "   'quacunque_viam_dederit',\n",
       "   'viam_dederit_fortuna',\n",
       "   'dederit_fortuna_sequamur.'],\n",
       "  'span_tokens': ['et_quacunque_viam_dederit',\n",
       "   'quacunque_viam_dederit_fortuna',\n",
       "   'viam_dederit_fortuna_sequamur.'],\n",
       "  'Z_en': array([-1, -1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 52,\n",
       "  'stop': 60,\n",
       "  'start_token_fr': 20,\n",
       "  'stop_token_fr': 22,\n",
       "  'history_tokens': ['viam_dederit_fortuna',\n",
       "   'dederit_fortuna_sequamur.',\n",
       "   'et_quacunque_viam_dederit',\n",
       "   'quacunque_viam_dederit_fortuna'],\n",
       "  'span_tokens': ['viam_dederit_fortuna_sequamur.', 'et_dup0'],\n",
       "  'Z_en': array([-1, -1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ...,  1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 43,\n",
       "  'start': 56,\n",
       "  'stop': 64,\n",
       "  'start_token_fr': 21,\n",
       "  'stop_token_fr': 24,\n",
       "  'history_tokens': ['dederit_fortuna_sequamur.',\n",
       "   'et_quacunque_viam_dederit',\n",
       "   'quacunque_viam_dederit_fortuna',\n",
       "   'viam_dederit_fortuna_sequamur.'],\n",
       "  'span_tokens': ['et_dup0', 'quacunque_dup1', 'viam_dup2'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 44,\n",
       "  'start': 0,\n",
       "  'stop': 8,\n",
       "  'start_token_fr': 0,\n",
       "  'stop_token_fr': 6,\n",
       "  'history_tokens': [],\n",
       "  'span_tokens': ['à', 'quoi', 'mène', 'ce', 'raisonnement', 'infini'],\n",
       "  'Z_en': array([-1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 44,\n",
       "  'start': 4,\n",
       "  'stop': 12,\n",
       "  'start_token_fr': 3,\n",
       "  'stop_token_fr': 10,\n",
       "  'history_tokens': ['à', 'quoi', 'mène'],\n",
       "  'span_tokens': ['ce',\n",
       "   'raisonnement',\n",
       "   'infini',\n",
       "   'sur',\n",
       "   'les',\n",
       "   'saintes',\n",
       "   'écritures,'],\n",
       "  'Z_en': array([ 1, -1, -1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 44,\n",
       "  'start': 8,\n",
       "  'stop': 16,\n",
       "  'start_token_fr': 6,\n",
       "  'stop_token_fr': 13,\n",
       "  'history_tokens': ['mène', 'ce', 'raisonnement', 'infini'],\n",
       "  'span_tokens': ['sur',\n",
       "   'les',\n",
       "   'saintes',\n",
       "   'écritures,',\n",
       "   'pensa',\n",
       "   'l’abbé',\n",
       "   'pirard,'],\n",
       "  'Z_en': array([ 1,  1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 44,\n",
       "  'start': 12,\n",
       "  'stop': 20,\n",
       "  'start_token_fr': 10,\n",
       "  'stop_token_fr': 16,\n",
       "  'history_tokens': ['sur', 'les', 'saintes', 'écritures,'],\n",
       "  'span_tokens': ['pensa', 'l’abbé', 'pirard,', 'si', 'ce', 'n’est'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 44,\n",
       "  'start': 16,\n",
       "  'stop': 24,\n",
       "  'start_token_fr': 13,\n",
       "  'stop_token_fr': 19,\n",
       "  'history_tokens': ['écritures,', 'pensa', 'l’abbé', 'pirard,'],\n",
       "  'span_tokens': ['si', 'ce', 'n’est', 'à', 'l’examen', 'personnel,'],\n",
       "  'Z_en': array([ 1,  1,  1, ...,  1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([1, 1, 1, ..., 1, 1, 1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 44,\n",
       "  'start': 20,\n",
       "  'stop': 28,\n",
       "  'start_token_fr': 16,\n",
       "  'stop_token_fr': 23,\n",
       "  'history_tokens': ['pirard,', 'si', 'ce', 'n’est'],\n",
       "  'span_tokens': ['à',\n",
       "   'l’examen',\n",
       "   'personnel,',\n",
       "   'c’est-à-dire',\n",
       "   'au',\n",
       "   'plus',\n",
       "   'affreux'],\n",
       "  'Z_en': array([-1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 44,\n",
       "  'start': 24,\n",
       "  'stop': 32,\n",
       "  'start_token_fr': 19,\n",
       "  'stop_token_fr': 26,\n",
       "  'history_tokens': ['n’est', 'à', 'l’examen', 'personnel,'],\n",
       "  'span_tokens': ['c’est-à-dire',\n",
       "   'au',\n",
       "   'plus',\n",
       "   'affreux',\n",
       "   '__sent_marker_0',\n",
       "   'protestantisme',\n",
       "   '?'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1,  1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 44,\n",
       "  'start': 28,\n",
       "  'stop': 36,\n",
       "  'start_token_fr': 23,\n",
       "  'stop_token_fr': 29,\n",
       "  'history_tokens': ['c’est-à-dire', 'au', 'plus', 'affreux'],\n",
       "  'span_tokens': ['__sent_marker_0',\n",
       "   'protestantisme',\n",
       "   '?',\n",
       "   'à_quoi',\n",
       "   'quoi_mène',\n",
       "   'mène_ce'],\n",
       "  'Z_en': array([ 1, -1,  1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1,  1,  1, ...,  1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 44,\n",
       "  'start': 32,\n",
       "  'stop': 40,\n",
       "  'start_token_fr': 26,\n",
       "  'stop_token_fr': 32,\n",
       "  'history_tokens': ['affreux', '__sent_marker_0', 'protestantisme', '?'],\n",
       "  'span_tokens': ['à_quoi',\n",
       "   'quoi_mène',\n",
       "   'mène_ce',\n",
       "   'ce_raisonnement',\n",
       "   'raisonnement_infini',\n",
       "   'infini_sur'],\n",
       "  'Z_en': array([ 1, -1, -1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1,  1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 44,\n",
       "  'start': 36,\n",
       "  'stop': 44,\n",
       "  'start_token_fr': 29,\n",
       "  'stop_token_fr': 36,\n",
       "  'history_tokens': ['?', 'à_quoi', 'quoi_mène', 'mène_ce'],\n",
       "  'span_tokens': ['ce_raisonnement',\n",
       "   'raisonnement_infini',\n",
       "   'infini_sur',\n",
       "   'sur_les',\n",
       "   'les_saintes',\n",
       "   'saintes_écritures,',\n",
       "   'écritures,_pensa'],\n",
       "  'Z_en': array([ 1, -1,  1, ..., -1, -1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([-1, -1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 44,\n",
       "  'start': 40,\n",
       "  'stop': 48,\n",
       "  'start_token_fr': 32,\n",
       "  'stop_token_fr': 39,\n",
       "  'history_tokens': ['mène_ce',\n",
       "   'ce_raisonnement',\n",
       "   'raisonnement_infini',\n",
       "   'infini_sur'],\n",
       "  'span_tokens': ['sur_les',\n",
       "   'les_saintes',\n",
       "   'saintes_écritures,',\n",
       "   'écritures,_pensa',\n",
       "   'pensa_l’abbé',\n",
       "   'l’abbé_pirard,',\n",
       "   'pirard,_si'],\n",
       "  'Z_en': array([ 1,  1, -1, ...,  1, -1, -1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1,  1, -1, ..., -1, -1, -1], shape=(8192,), dtype=int8)},\n",
       " {'sentence_idx': 44,\n",
       "  'start': 44,\n",
       "  'stop': 52,\n",
       "  'start_token_fr': 36,\n",
       "  'stop_token_fr': 42,\n",
       "  'history_tokens': ['sur_les',\n",
       "   'les_saintes',\n",
       "   'saintes_écritures,',\n",
       "   'écritures,_pensa'],\n",
       "  'span_tokens': ['pensa_l’abbé',\n",
       "   'l’abbé_pirard,',\n",
       "   'pirard,_si',\n",
       "   'si_ce',\n",
       "   'ce_n’est',\n",
       "   'n’est_à'],\n",
       "  'Z_en': array([-1,  1, -1, ...,  1,  1,  1], shape=(8192,), dtype=int8),\n",
       "  'Z_fr_lex': array([ 1, -1,  1, ..., -1,  1,  1], shape=(8192,), dtype=int8)},\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd232d6",
   "metadata": {},
   "source": [
    "## 3. Entraînement MEM et diagnostic rapide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d69b4f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:39:15,108 [INFO] MEM training completed (B=128)\n",
      "MEM probe: 100%|██████████| 200/200 [00:00<00:00, 17927.06it/s]\n",
      "2025-10-07 16:39:15,123 [INFO] Probe similarities — mean: 0.1674 | median: 0.2489\n",
      "2025-10-07 16:39:15,124 [INFO] Bucket population stats — mean: 390.6 | p90: 443 | p99: 477\n"
     ]
    }
   ],
   "source": [
    "MEM_K = 16\n",
    "MEM_BUCKETS = 128\n",
    "cfg = mem_pipeline.MemConfig(D=D, B=MEM_BUCKETS, k=MEM_K, seed_lsh=10, seed_gmem=11)\n",
    "comp = mem_pipeline.make_mem_pipeline(cfg)\n",
    "mem_pipeline.train_one_pass_MEM(comp, pairs_mem)\n",
    "log.info(\"MEM training completed (B=%d)\", comp.mem.B)\n",
    "\n",
    "probe_count = min(200, len(pairs_mem))\n",
    "sim_values = []\n",
    "for Z_en_vec, Z_fr_vec in tqdm(pairs_mem[:probe_count], desc=\"MEM probe\"):\n",
    "    bucket_idx, _ = mem_pipeline.infer_map_top1(comp, Z_en_vec)\n",
    "    prototype = comp.mem.H[bucket_idx].astype(np.int32, copy=False)\n",
    "    sim = float(np.dot(prototype, Z_fr_vec.astype(np.int32, copy=False)) / D)\n",
    "    sim_values.append(sim)\n",
    "\n",
    "if sim_values:\n",
    "    log.info(\n",
    "        \"Probe similarities — mean: %.4f | median: %.4f\",\n",
    "        float(np.mean(sim_values)),\n",
    "        float(np.median(sim_values)),\n",
    "    )\n",
    "    nb = comp.mem.n\n",
    "    log.info(\n",
    "        \"Bucket population stats — mean: %.1f | p90: %d | p99: %d\",\n",
    "        float(nb.mean()),\n",
    "        int(np.quantile(nb, 0.90)),\n",
    "        int(np.quantile(nb, 0.99)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b914c",
   "metadata": {},
   "source": [
    "## 4. Dictionnaire bucket → vocabulaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ccd61b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:39:36,690 [INFO] Bucket vocab built for 128 buckets (global vocab size=12629)\n"
     ]
    }
   ],
   "source": [
    "MAX_BUCKET_VOCAB = 256\n",
    "bucket_counts: dict[int, Counter] = defaultdict(Counter)\n",
    "for meta in tqdm(span_meta, desc=\"Bucket vocab build\", leave=False):\n",
    "    bucket_idx, _ = mem_pipeline.infer_map_top1(comp, meta[\"Z_en\"])\n",
    "    bucket_idx = int(bucket_idx)\n",
    "    meta[\"bucket_idx\"] = bucket_idx\n",
    "    for tok in meta.get(\"span_tokens\", []):\n",
    "        bucket_counts[bucket_idx][tok] += 1\n",
    "\n",
    "bucket2vocab_freq = {\n",
    "    bucket: sorted(counter.items(), key=lambda kv: (-kv[1], kv[0]))[:MAX_BUCKET_VOCAB]\n",
    "    for bucket, counter in bucket_counts.items()\n",
    "}\n",
    "bucket2vocab = {bucket: [tok for tok, _ in tokens] for bucket, tokens in bucket2vocab_freq.items()}\n",
    "all_vocab = sorted({tok for tokens in bucket2vocab.values() for tok in tokens})\n",
    "log.info(\n",
    "    \"Bucket vocab built for %d buckets (global vocab size=%d)\",\n",
    "    len(bucket2vocab),\n",
    "    len(all_vocab),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af56487",
   "metadata": {},
   "source": [
    "## 5bis. Diagnostics théorie ↔ implémentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "819244a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:39:48,088 [INFO] Running DEC diagnostic suite (subsampled)...\n",
      "2025-10-07 16:39:48,466 [INFO] DX2 ok — example median norm: 1.000\n",
      "2025-10-07 16:39:48,617 [INFO] DX3 ok — mean relative error=0.0000 | p=1.000\n",
      "2025-10-07 16:39:57,447 [INFO] DX4 ok — recall@500=1.000\n",
      "2025-10-07 16:39:57,456 [INFO] DX5 ok — accuracy m=8: 1.000\n",
      "2025-10-07 16:39:57,566 [INFO] DX6 ok — lambda grid summary: {0.0: (1.0, 1.1109939911767361), 0.5: (1.0, 1.0000001843669586), 1.0: (1.0, 1.000000000000448)}\n",
      "2025-10-07 16:40:03,095 [INFO] DX7 ok — ell*=2, top1=0.926\n"
     ]
    }
   ],
   "source": [
    "log.info(\"Running DEC diagnostic suite (subsampled)...\")\n",
    "\n",
    "norms = DX2_run(D=D, trials=50, ells=(2, 4, 8), ratios=(1.0,), seed=2025)\n",
    "log.info(\"DX2 ok — example median norm: %.3f\", np.median([v[1] for v in norms.values()]))\n",
    "\n",
    "rel_err, pval = DX3_run(D=D, C=256, T=64, seed=2025, rel_tol=0.02, pmin=0.05)\n",
    "log.info(\"DX3 ok — mean relative error=%.4f | p=%.3f\", rel_err, pval)\n",
    "\n",
    "recalls = DX4_run(D=D, B=5_000, trials=40, Ks=(100, 500), seed=0)\n",
    "log.info(\"DX4 ok — recall@500=%.3f\", recalls[500])\n",
    "\n",
    "accuracies = DX5_run(D=D, trials=40, ms=(4, 8, 16), seed=0)\n",
    "log.info(\"DX5 ok — accuracy m=8: %.3f\", accuracies[8])\n",
    "\n",
    "results_dx6 = DX6_run(D=D, trials=120, lam_grid=(0.0, 0.5, 1.0), rng_seed=7031)\n",
    "log.info(\"DX6 ok — lambda grid summary: %s\", {lam: (vals['top1'], vals['ppl']) for lam, vals in results_dx6.items()})\n",
    "\n",
    "dx7_results, ell_star = DX7_run(ell_grid=(2, 4, 8), D=D, seed_pi=10_456, rng_seed=9_117)\n",
    "log.info(\"DX7 ok — ell*=%d, top1=%.3f\", ell_star, dx7_results[ell_star][\"top1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49225cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0:\n",
      "  EN   : The sudden idea of the marriage between C&amp;eacue;cile and Paul, which she was arranging with so quiet a smile, completed his exasperation.\n",
      "  REF  : l'idée brusque du mariage qu'elle poursuivait d'un sourire si tranquille entre cécile et paul, acheva de l'exaspérer.\n",
      "  PRED : <empty>\n",
      "  BLEU=0.000 | ROUGE-1 F1=0.000 | ROUGE-2 F1=0.000\n",
      "\n",
      "Sentence 1:\n",
      "  EN   : Harris said: \"Seven.\"\n",
      "  REF  : – a sept heures, dit harris.\n",
      "  PRED : <empty>\n",
      "  BLEU=0.000 | ROUGE-1 F1=0.000 | ROUGE-2 F1=0.000\n",
      "\n",
      "Sentence 2:\n",
      "  EN   : His hair was long and black, not curled like wool; his forehead very high and large; and a great vivacity and sparkling sharpness in his eyes.\n",
      "  REF  : sa chevelure était longue et noire, et non pas crépue comme de la laine. son front était haut et large, ses yeux vifs et pleins de feu.\n",
      "  PRED : <empty>\n",
      "  BLEU=0.000 | ROUGE-1 F1=0.000 | ROUGE-2 F1=0.000\n",
      "\n",
      "Sentence 3:\n",
      "  EN   : He was Des Roches le Masle, canon of Notre Dame, who had formerly been valet of a bishop, who introduced him to his Eminence as a perfectly devout man.\n",
      "  REF  : ce fut des roches le masle, chanoine à notre-dame, et qui avait été autrefois valet de chambre du cardinal, qui le proposa à son éminence comme un homme tout dévoué.\n",
      "  PRED : <empty>\n",
      "  BLEU=0.000 | ROUGE-1 F1=0.000 | ROUGE-2 F1=0.000\n",
      "\n",
      "Sentence 4:\n",
      "  EN   : Perhaps, following the examples of oyster farmers in China and India, he had even predetermined the creation of this pearl by sticking under the mollusk's folds some piece of glass or metal that was gradually covered with mother-of-pearl.\n",
      "  REF  : peut-être même, suivant l'exemple des chinois et des indiens, avait-il déterminé la production de cette perle en introduisant sous les plis du mollusque quelque morceau de verre et de métal, qui s'était peu à peu recouvert de la matière nacrée.\n",
      "  PRED : <empty>\n",
      "  BLEU=0.000 | ROUGE-1 F1=0.000 | ROUGE-2 F1=0.000\n",
      "\n",
      "=== Averages ===\n",
      "BLEU=0.000 | ROUGE-1=0.000 | ROUGE-2=0.000\n"
     ]
    }
   ],
   "source": [
    "if not span_meta:\n",
    "    raise RuntimeError(\"Aucune paire MEM disponible pour la démonstration DEC.\")\n",
    "\n",
    "G_DEC = rademacher(D, np.random.default_rng(2025))\n",
    "G_MEM = comp.Gmem\n",
    "L_fr_payload = L_FR_PAYLOAD  # MEM=DEC payload space\n",
    "L_fr_lm = L_FR_LM\n",
    "prototypes = comp.mem.H.astype(np.int8, copy=False)\n",
    "\n",
    "ELL = 4\n",
    "CAND_PER_BUCKET = 32\n",
    "STAGE1_LIMIT = 24\n",
    "STAGE2_LIMIT = 8\n",
    "V_MAX = 512\n",
    "LAM = 0.2\n",
    "\n",
    "\n",
    "def update_LM_sep(H_LM: np.ndarray, token: str) -> np.ndarray:\n",
    "    vec = L_FR_LM(token).astype(np.int8, copy=False)\n",
    "    hd_assert_pm1(vec, D)\n",
    "    inc = permute_pow(vec, pi, 1).astype(np.int16, copy=False)\n",
    "    acc = H_LM.astype(np.int16) + inc\n",
    "    return np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "\n",
    "\n",
    "def gather_candidates_from_ck(\n",
    "    C_K: Sequence[int],\n",
    "    history: Sequence[str],\n",
    "    *,\n",
    "    limit: int | None = None,\n",
    ") -> list[str]:\n",
    "    seen: set[str] = set()\n",
    "    out: list[str] = []\n",
    "    cap = limit or V_MAX\n",
    "    for c in C_K:\n",
    "        for tok, _cnt in bucket2vocab_freq.get(int(c), [])[:CAND_PER_BUCKET]:\n",
    "            if tok not in seen:\n",
    "                seen.add(tok)\n",
    "                out.append(tok)\n",
    "            if len(out) >= cap:\n",
    "                break\n",
    "        if len(out) >= cap:\n",
    "            break\n",
    "    for tok in reversed(history):\n",
    "        if tok not in seen:\n",
    "            seen.add(tok)\n",
    "            out.append(tok)\n",
    "        if len(out) >= cap:\n",
    "            break\n",
    "    if not out and all_vocab:\n",
    "        out.extend(all_vocab[:cap])\n",
    "    if not out:\n",
    "        out.append(\"<unk>\")\n",
    "    return out[:cap]\n",
    "\n",
    "\n",
    "def decode_span_staged(meta: dict, H_LM: np.ndarray, history: list[str], *, max_steps: int) -> tuple[list[str], list[str], np.ndarray]:\n",
    "    decoded: list[str] = []\n",
    "    for _ in range(max_steps):\n",
    "        stage1_token, stage1_scores, _c1, C_K1, _ = DecodeOneStep(\n",
    "            Hs=meta[\"Z_en\"],\n",
    "            H_LM=H_LM,\n",
    "            history_fr=history,\n",
    "            G_DEC=G_DEC,\n",
    "            G_MEM=G_MEM,\n",
    "            Pi=pi,\n",
    "            L_fr=L_fr_payload,\n",
    "            prototypes=prototypes,\n",
    "            K=64,\n",
    "            alpha=1.0,\n",
    "            beta=0.0,\n",
    "            ell=ELL,\n",
    "            lam=0.0,\n",
    "            bucket2vocab=bucket2vocab,\n",
    "            global_fallback_vocab=all_vocab[:512] if all_vocab else None,\n",
    "            return_ck_scores=True,\n",
    "        )\n",
    "\n",
    "        cand_stage1 = gather_candidates_from_ck(C_K1, history, limit=STAGE1_LIMIT)\n",
    "        order = np.argsort(stage1_scores)[::-1]\n",
    "        stage_tokens = [cand_stage1[i] for i in order[:STAGE2_LIMIT] if i < len(cand_stage1)]\n",
    "        if not stage_tokens:\n",
    "            stage_tokens = cand_stage1[:STAGE2_LIMIT] or [stage1_token]\n",
    "\n",
    "        bucket2vocab_stage2 = {\n",
    "            c: [tok for tok, _cnt in bucket2vocab_freq.get(c, []) if tok in stage_tokens]\n",
    "            for c in bucket2vocab_freq.keys()\n",
    "        }\n",
    "\n",
    "        token_star, _scores2, _c_star2, _C_K2, _ = DecodeOneStep(\n",
    "            Hs=meta[\"Z_en\"],\n",
    "            H_LM=H_LM,\n",
    "            history_fr=history,\n",
    "            G_DEC=G_DEC,\n",
    "            G_MEM=G_MEM,\n",
    "            Pi=pi,\n",
    "            L_fr=L_fr_payload,\n",
    "            prototypes=prototypes,\n",
    "            K=max(8, len(stage_tokens)),\n",
    "            alpha=1.0,\n",
    "            beta=0.0,\n",
    "            ell=ELL,\n",
    "            lam=LAM,\n",
    "            bucket2vocab=bucket2vocab_stage2,\n",
    "            global_fallback_vocab=stage_tokens,\n",
    "            return_ck_scores=False,\n",
    "        )\n",
    "\n",
    "        decoded.append(token_star)\n",
    "        history.append(token_star)\n",
    "        if len(history) > ELL:\n",
    "            history[:] = history[-ELL:]\n",
    "        H_LM = update_LM_sep(H_LM, token_star)\n",
    "    return decoded, history, H_LM\n",
    "\n",
    "\n",
    "def decode_sentence(sentence_idx: int, *, max_steps_per_span: int = 8) -> list[str]:\n",
    "    metas = sorted(\n",
    "        [m for m in span_meta if m[\"sentence_idx\"] == sentence_idx],\n",
    "        key=lambda m: m[\"start\"],\n",
    "    )\n",
    "    if not metas:\n",
    "        return []\n",
    "\n",
    "    history: list[str] = []\n",
    "    rng_sent = np.random.default_rng(9_999 + sentence_idx)\n",
    "    H_LM = rademacher(D, rng_sent)\n",
    "\n",
    "    decoded_sentence: list[str] = []\n",
    "    for meta in metas:\n",
    "        seed = list(meta.get(\"history_tokens\", [])[-ELL:])\n",
    "        if seed and not history:\n",
    "            history = seed.copy()\n",
    "            for tok in history:\n",
    "                H_LM = update_LM_sep(H_LM, tok)\n",
    "\n",
    "        steps = min(len(meta.get(\"span_tokens\", [])), max_steps_per_span)\n",
    "        if steps <= 0:\n",
    "            continue\n",
    "\n",
    "        decoded, history, H_LM = decode_span_staged(meta, H_LM, history, max_steps=steps)\n",
    "        decoded_sentence.extend(decoded)\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "N_EVAL_SENT = 5\n",
    "bleu_scores: list[float] = []\n",
    "rouge1_scores: list[float] = []\n",
    "rouge2_scores: list[float] = []\n",
    "\n",
    "for idx in range(min(N_EVAL_SENT, len(ens_sample))):\n",
    "    pred_tokens = decode_sentence(idx, max_steps_per_span=8)\n",
    "    ref_tokens = clean_tokens(frs_sample[idx].lower().split())\n",
    "    pred_clean = clean_tokens(pred_tokens)\n",
    "    bleu = bleu_score(ref_tokens, pred_clean)\n",
    "    rouge1 = rouge_n(ref_tokens, pred_clean, n=1)\n",
    "    rouge2 = rouge_n(ref_tokens, pred_clean, n=2)\n",
    "    bleu_scores.append(bleu)\n",
    "    rouge1_scores.append(rouge1)\n",
    "    rouge2_scores.append(rouge2)\n",
    "\n",
    "    print(f\"Sentence {idx}:\")\n",
    "    print(\"  EN   :\", ens_sample[idx])\n",
    "    print(\"  REF  :\", \" \".join(ref_tokens))\n",
    "    print(\"  PRED :\", \" \".join(pred_clean) if pred_clean else \"<empty>\")\n",
    "    print(f\"  BLEU={bleu:.3f} | ROUGE-1 F1={rouge1:.3f} | ROUGE-2 F1={rouge2:.3f}\")\n",
    "    print()\n",
    "\n",
    "if bleu_scores:\n",
    "    print(\"=== Averages ===\")\n",
    "    print(\n",
    "        f\"BLEU={np.mean(bleu_scores):.3f} | ROUGE-1={np.mean(rouge1_scores):.3f} | ROUGE-2={np.mean(rouge2_scores):.3f}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Aucune phrase évaluée.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "562da29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence # 0 span (0, 8)\n",
      "  History seed: []\n",
      "  Decoded   : __sent_marker_2 __sent_marker_2 __sent_marker_2 __sent_marker_2 __sent_marker_2 __sent_marker_2\n",
      "  Reference : l'idée brusque du mariage qu'elle poursuivait\n",
      "  Match count: 0/6\n",
      "\n",
      "Sentence # 0 span (4, 12)\n",
      "  History seed: [\"l'idée\", 'brusque', 'du']\n",
      "  Decoded   : __sent_marker_0 __sent_marker_0 __sent_marker_2 __sent_marker_2 __sent_marker_0 __sent_marker_0\n",
      "  Reference : mariage qu'elle poursuivait d'un sourire si\n",
      "  Match count: 0/6\n",
      "\n",
      "Sentence # 0 span (8, 16)\n",
      "  History seed: ['du', 'mariage', \"qu'elle\", 'poursuivait']\n",
      "  Decoded   : __sent_marker_0 __sent_marker_0 __sent_marker_0 __sent_marker_0 __sent_marker_0 __sent_marker_0\n",
      "  Reference : d'un sourire si tranquille entre cécile\n",
      "  Match count: 0/6\n",
      "\n",
      "Sentence # 0 span (12, 20)\n",
      "  History seed: ['poursuivait', \"d'un\", 'sourire', 'si']\n",
      "  Decoded   : __sent_marker_0 __sent_marker_2 __sent_marker_2 __sent_marker_2 __sent_marker_2 __sent_marker_0\n",
      "  Reference : tranquille entre cécile et paul, acheva\n",
      "  Match count: 0/6\n",
      "\n",
      "Sentence # 0 span (16, 24)\n",
      "  History seed: ['si', 'tranquille', 'entre', 'cécile']\n",
      "  Decoded   : __sent_marker_0 __sent_marker_0 __sent_marker_0 __sent_marker_0 __sent_marker_2 __sent_marker_2\n",
      "  Reference : et paul, acheva __sent_marker_0 de l'exaspérer.\n",
      "  Match count: 1/6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not span_meta:\n",
    "    raise RuntimeError(\"Aucune paire MEM disponible pour la démonstration DEC.\")\n",
    "\n",
    "G_DEC = rademacher(D, np.random.default_rng(2025))\n",
    "G_MEM = comp.Gmem\n",
    "L_fr_payload = L_FR_PAYLOAD  # MEM=DEC payload space\n",
    "L_fr_lm = L_FR_LM\n",
    "prototypes = comp.mem.H.astype(np.int8, copy=False)\n",
    "\n",
    "ELL = 4\n",
    "CAND_PER_BUCKET = 32\n",
    "V_MAX = 512\n",
    "LAM = 0.5\n",
    "\n",
    "\n",
    "def update_LM_sep(H_LM: np.ndarray, token: str) -> np.ndarray:\n",
    "    vec = L_FR_LM(token).astype(np.int8, copy=False)\n",
    "    hd_assert_pm1(vec, D)\n",
    "    inc = permute_pow(vec, pi, 1).astype(np.int16, copy=False)\n",
    "    acc = H_LM.astype(np.int16) + inc\n",
    "    return np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "\n",
    "\n",
    "def vote_two_spaces(Z_hat: np.ndarray, H_LM: np.ndarray, cand_vocab: Sequence[str]) -> tuple[str, np.ndarray]:\n",
    "    if not cand_vocab:\n",
    "        cand_vocab = [\"<unk>\"]\n",
    "    token_star, scores, _ = DD6_vote(\n",
    "        Z_hat,\n",
    "        H_LM,\n",
    "        L_mem=L_FR_PAYLOAD,\n",
    "        L_lm=L_FR_LM,\n",
    "        cand_vocab=cand_vocab,\n",
    "        lam=LAM,\n",
    "        return_probs=False,\n",
    "    )\n",
    "    return token_star, scores\n",
    "\n",
    "\n",
    "def gather_candidates(C_K: np.ndarray, history: Sequence[str]) -> list[str]:\n",
    "    seen: set[str] = set()\n",
    "    candidates: list[str] = []\n",
    "    for c in C_K:\n",
    "        freq_list = bucket2vocab_freq.get(int(c), [])[:CAND_PER_BUCKET]\n",
    "        for tok, _cnt in freq_list:\n",
    "            if tok not in seen:\n",
    "                seen.add(tok)\n",
    "                candidates.append(tok)\n",
    "            if len(candidates) >= V_MAX:\n",
    "                break\n",
    "        if len(candidates) >= V_MAX:\n",
    "            break\n",
    "    for tok in reversed(history):\n",
    "        if tok not in seen:\n",
    "            candidates.append(tok)\n",
    "            seen.add(tok)\n",
    "        if len(candidates) >= V_MAX:\n",
    "            break\n",
    "    if not candidates and all_vocab:\n",
    "        candidates.extend(all_vocab[: min(64, len(all_vocab))])\n",
    "    if not candidates:\n",
    "        candidates.append(\"<unk>\")\n",
    "    return candidates[:V_MAX]\n",
    "\n",
    "\n",
    "def decode_span(\n",
    "    meta: dict,\n",
    "    H_LM_init: np.ndarray,\n",
    "    history_init: Sequence[str],\n",
    "    *,\n",
    "    max_steps: int | None = None,\n",
    "    ell: int = ELL,\n",
    ") -> tuple[list[str], list[str], np.ndarray]:\n",
    "    history = list(history_init)\n",
    "    H_LM = H_LM_init\n",
    "    targets = list(meta.get(\"span_tokens\", []))\n",
    "    steps = max_steps if max_steps is not None else len(targets)\n",
    "    if steps is None or steps < 0:\n",
    "        steps = len(targets)\n",
    "    if steps == 0:\n",
    "        return [], history, H_LM\n",
    "\n",
    "    decoded: list[str] = []\n",
    "    for _ in range(steps):\n",
    "        Qs = DD1_ctx(meta[\"Z_en\"], G_DEC)\n",
    "        Rt = DD2_query_bin(Qs, history, L_FR_LM, pi, alpha=1.0, beta=1.0, ell=ell)\n",
    "        Rt_tilde = DD3_bindToMem(Rt, G_MEM)\n",
    "        c_star, C_K, _ = DD4_search_topK(Rt_tilde, prototypes, K=64)\n",
    "        Z_hat = DD5_payload(prototypes[c_star])\n",
    "        cand_vocab = gather_candidates(C_K, history)\n",
    "        token_star, _ = vote_two_spaces(Z_hat, H_LM, cand_vocab)\n",
    "        decoded.append(token_star)\n",
    "        history.append(token_star)\n",
    "        if len(history) > ell:\n",
    "            history = history[-ell:]\n",
    "        H_LM = update_LM_sep(H_LM, token_star)\n",
    "    return decoded, history, H_LM\n",
    "\n",
    "\n",
    "sample_metas = [m for m in span_meta if m.get(\"span_tokens\")] or span_meta[:5]\n",
    "results = []\n",
    "for meta in sample_metas[:5]:\n",
    "    history_seed = list(meta.get(\"history_tokens\", [])[-ELL:])\n",
    "    rng_demo = np.random.default_rng(4242 + meta[\"sentence_idx\"])\n",
    "    H_LM_seed = rademacher(D, rng_demo)\n",
    "    for tok in history_seed:\n",
    "        H_LM_seed = update_LM_sep(H_LM_seed, tok)\n",
    "\n",
    "    decoded, history_out, _ = decode_span(\n",
    "        meta,\n",
    "        H_LM_seed,\n",
    "        history_seed,\n",
    "        max_steps=len(meta.get(\"span_tokens\", [])) if meta.get(\"span_tokens\") else 4,\n",
    "    )\n",
    "\n",
    "    targets = meta.get(\"span_tokens\", [])[: len(decoded)]\n",
    "    matches = sum(p == t for p, t in zip(decoded, targets))\n",
    "    results.append(\n",
    "        {\n",
    "            \"sentence_idx\": meta[\"sentence_idx\"],\n",
    "            \"span_bounds\": (meta[\"start\"], meta[\"stop\"]),\n",
    "            \"history_seed\": history_seed,\n",
    "            \"decoded\": decoded,\n",
    "            \"reference\": targets,\n",
    "            \"match_count\": matches,\n",
    "        }\n",
    "    )\n",
    "\n",
    "for res in results:\n",
    "    print(\"Sentence #\", res[\"sentence_idx\"], \"span\", res[\"span_bounds\"])\n",
    "    print(\"  History seed:\", res[\"history_seed\"])\n",
    "    print(\"  Decoded   :\", \" \".join(res[\"decoded\"]))\n",
    "    print(\"  Reference :\", \" \".join(res[\"reference\"]))\n",
    "    if res[\"decoded\"]:\n",
    "        print(f\"  Match count: {res['match_count']}/{len(res['decoded'])}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94115cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def clean_tokens(seq: Sequence[str]) -> list[str]:\n",
    "    cleaned: list[str] = []\n",
    "    for tok in seq:\n",
    "        if not tok:\n",
    "            continue\n",
    "        if tok.startswith(\"__sent_marker\"):\n",
    "            continue\n",
    "        if \"_dup\" in tok or \"__\" in tok or \"_\" in tok:\n",
    "            continue\n",
    "        cleaned.append(tok)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def ngram_counts(tokens: Sequence[str], n: int) -> Counter:\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n doit être >= 1\")\n",
    "    return Counter(tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1))\n",
    "\n",
    "\n",
    "def bleu_score(reference: Sequence[str], candidate: Sequence[str], max_n: int = 4) -> float:\n",
    "    if not candidate:\n",
    "        return 0.0\n",
    "    precisions: list[float] = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_counts = ngram_counts(reference, n)\n",
    "        cand_counts = ngram_counts(candidate, n)\n",
    "        if not cand_counts:\n",
    "            precisions.append(1e-9)\n",
    "            continue\n",
    "        overlap = sum(min(count, ref_counts[ng]) for ng, count in cand_counts.items())\n",
    "        total = sum(cand_counts.values())\n",
    "        precisions.append((overlap + 1e-9) / (total + 1e-9))\n",
    "    geo_mean = math.exp(sum(math.log(p) for p in precisions) / max_n)\n",
    "    ref_len = len(reference)\n",
    "    cand_len = len(candidate)\n",
    "    if cand_len == 0:\n",
    "        return 0.0\n",
    "    bp = 1.0 if cand_len > ref_len else math.exp(1.0 - ref_len / max(cand_len, 1))\n",
    "    return float(bp * geo_mean)\n",
    "\n",
    "\n",
    "def rouge_n(reference: Sequence[str], candidate: Sequence[str], n: int = 1) -> float:\n",
    "    if not reference or not candidate:\n",
    "        return 0.0\n",
    "    ref_counts = ngram_counts(reference, n)\n",
    "    cand_counts = ngram_counts(candidate, n)\n",
    "    if not ref_counts or not cand_counts:\n",
    "        return 0.0\n",
    "    overlap = sum(min(count, cand_counts.get(ng, 0)) for ng, count in ref_counts.items())\n",
    "    recall = overlap / sum(ref_counts.values())\n",
    "    precision = overlap / sum(cand_counts.values())\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return float(2 * precision * recall / (precision + recall))\n",
    "\n",
    "\n",
    "def decode_sentence(sentence_idx: int, *, max_steps_per_span: int = 8) -> list[str]:\n",
    "    metas = sorted(\n",
    "        [m for m in span_meta if m[\"sentence_idx\"] == sentence_idx],\n",
    "        key=lambda m: m[\"start\"],\n",
    "    )\n",
    "    if not metas:\n",
    "        return []\n",
    "\n",
    "    history: list[str] = []\n",
    "    rng_sent = np.random.default_rng(9_999 + sentence_idx)\n",
    "    H_LM = rademacher(D, rng_sent)\n",
    "\n",
    "    decoded_sentence: list[str] = []\n",
    "    for meta in metas:\n",
    "        if not history:\n",
    "            seed = list(meta.get(\"history_tokens\", [])[-ELL:])\n",
    "            if seed:\n",
    "                history = seed.copy()\n",
    "                for tok in history:\n",
    "                    H_LM = update_LM_sep(H_LM, tok)\n",
    "\n",
    "        steps = min(len(meta.get(\"span_tokens\", [])), max_steps_per_span)\n",
    "        if steps <= 0:\n",
    "            continue\n",
    "\n",
    "        for _ in range(steps):\n",
    "            token_star, _scores, _c_star, _C_K, _H_next = DecodeOneStep(\n",
    "                Hs=meta[\"Z_en\"],\n",
    "                H_LM=H_LM,\n",
    "                history_fr=history,\n",
    "                G_DEC=G_DEC,\n",
    "                G_MEM=G_MEM,\n",
    "                Pi=pi,\n",
    "                L_fr=L_fr_payload,\n",
    "                prototypes=prototypes,\n",
    "                K=64,\n",
    "                alpha=1.0,\n",
    "                beta=1.0,\n",
    "                ell=ELL,\n",
    "                lam=LAM,\n",
    "                bucket2vocab=bucket2vocab,\n",
    "                global_fallback_vocab=all_vocab[:512] if all_vocab else None,\n",
    "                return_ck_scores=False,\n",
    "            )\n",
    "            decoded_sentence.append(token_star)\n",
    "            history.append(token_star)\n",
    "            if len(history) > ELL:\n",
    "                history = history[-ELL:]\n",
    "            H_LM = update_LM_sep(H_LM, token_star)\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "N_EVAL_SENT = 5\n",
    "bleu_scores: list[float] = []\n",
    "rouge1_scores: list[float] = []\n",
    "rouge2_scores: list[float] = []\n",
    "\n",
    "for idx in range(min(N_EVAL_SENT, len(ens_sample))):\n",
    "    pred_tokens = decode_sentence(idx, max_steps_per_span=8)\n",
    "    ref_tokens = clean_tokens(frs_sample[idx].lower().split())\n",
    "    pred_clean = clean_tokens(pred_tokens)\n",
    "    bleu = bleu_score(ref_tokens, pred_clean)\n",
    "    rouge1 = rouge_n(ref_tokens, pred_clean, n=1)\n",
    "    rouge2 = rouge_n(ref_tokens, pred_clean, n=2)\n",
    "    bleu_scores.append(bleu)\n",
    "    rouge1_scores.append(rouge1)\n",
    "    rouge2_scores.append(rouge2)\n",
    "\n",
    "    print(f\"Sentence {idx}:\")\n",
    "    print(\"  EN   :\", ens_sample[idx])\n",
    "    print(\"  REF  :\", \" \".join(ref_tokens))\n",
    "    print(\"  PRED :\", \" \".join(pred_clean) if pred_clean else \"<empty>\")\n",
    "    print(f\"  BLEU={bleu:.3f} | ROUGE-1 F1={rouge1:.3f} | ROUGE-2 F1={rouge2:.3f}\")\n",
    "    print()\n",
    "\n",
    "if bleu_scores:\n",
    "    print(\"=== Averages ===\")\n",
    "    print(\n",
    "        f\"BLEU={np.mean(bleu_scores):.3f} | ROUGE-1={np.mean(rouge1_scores):.3f} | ROUGE-2={np.mean(rouge2_scores):.3f}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Aucune phrase évaluée.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0:\n",
      "  EN   : The sudden idea of the marriage between C&amp;eacue;cile and Paul, which she was arranging with so quiet a smile, completed his exasperation.\n",
      "  REF  : l'idée brusque du mariage qu'elle poursuivait d'un sourire si tranquille entre cécile et paul, acheva de l'exaspérer.\n",
      "  PRED : m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m.\n",
      "  BLEU=0.000 | ROUGE-1 F1=0.000 | ROUGE-2 F1=0.000\n",
      "\n",
      "Sentence 1:\n",
      "  EN   : Harris said: \"Seven.\"\n",
      "  REF  : – a sept heures, dit harris.\n",
      "  PRED : m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m.\n",
      "  BLEU=0.000 | ROUGE-1 F1=0.000 | ROUGE-2 F1=0.000\n",
      "\n",
      "Sentence 2:\n",
      "  EN   : His hair was long and black, not curled like wool; his forehead very high and large; and a great vivacity and sparkling sharpness in his eyes.\n",
      "  REF  : sa chevelure était longue et noire, et non pas crépue comme de la laine. son front était haut et large, ses yeux vifs et pleins de feu.\n",
      "  PRED : si m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m.\n",
      "  BLEU=0.000 | ROUGE-1 F1=0.000 | ROUGE-2 F1=0.000\n",
      "\n",
      "Sentence 3:\n",
      "  EN   : He was Des Roches le Masle, canon of Notre Dame, who had formerly been valet of a bishop, who introduced him to his Eminence as a perfectly devout man.\n",
      "  REF  : ce fut des roches le masle, chanoine à notre-dame, et qui avait été autrefois valet de chambre du cardinal, qui le proposa à son éminence comme un homme tout dévoué.\n",
      "  PRED : si m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m.\n",
      "  BLEU=0.000 | ROUGE-1 F1=0.000 | ROUGE-2 F1=0.000\n",
      "\n",
      "Sentence 4:\n",
      "  EN   : Perhaps, following the examples of oyster farmers in China and India, he had even predetermined the creation of this pearl by sticking under the mollusk's folds some piece of glass or metal that was gradually covered with mother-of-pearl.\n",
      "  REF  : peut-être même, suivant l'exemple des chinois et des indiens, avait-il déterminé la production de cette perle en introduisant sous les plis du mollusque quelque morceau de verre et de métal, qui s'était peu à peu recouvert de la matière nacrée.\n",
      "  PRED : m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m. m.\n",
      "  BLEU=0.000 | ROUGE-1 F1=0.000 | ROUGE-2 F1=0.000\n",
      "\n",
      "=== Averages ===\n",
      "BLEU=0.000 | ROUGE-1=0.000 | ROUGE-2=0.000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def clean_tokens(seq: Sequence[str]) -> list[str]:\n",
    "    cleaned: list[str] = []\n",
    "    for tok in seq:\n",
    "        if not tok:\n",
    "            continue\n",
    "        if tok.startswith(\"__sent_marker\"):\n",
    "            continue\n",
    "        if \"_dup\" in tok or \"__\" in tok or \"_\" in tok:\n",
    "            continue\n",
    "        cleaned.append(tok)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def ngram_counts(tokens: Sequence[str], n: int) -> Counter:\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n doit être >= 1\")\n",
    "    return Counter(tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1))\n",
    "\n",
    "\n",
    "def bleu_score(reference: Sequence[str], candidate: Sequence[str], max_n: int = 4) -> float:\n",
    "    if not candidate:\n",
    "        return 0.0\n",
    "    precisions: list[float] = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_counts = ngram_counts(reference, n)\n",
    "        cand_counts = ngram_counts(candidate, n)\n",
    "        if not cand_counts:\n",
    "            precisions.append(1e-9)\n",
    "            continue\n",
    "        overlap = sum(min(count, ref_counts[ng]) for ng, count in cand_counts.items())\n",
    "        total = sum(cand_counts.values())\n",
    "        precisions.append((overlap + 1e-9) / (total + 1e-9))\n",
    "    geo_mean = math.exp(sum(math.log(p) for p in precisions) / max_n)\n",
    "    ref_len = len(reference)\n",
    "    cand_len = len(candidate)\n",
    "    if cand_len == 0:\n",
    "        return 0.0\n",
    "    if cand_len > ref_len:\n",
    "        bp = 1.0\n",
    "    else:\n",
    "        bp = math.exp(1.0 - ref_len / max(cand_len, 1))\n",
    "    return float(bp * geo_mean)\n",
    "\n",
    "\n",
    "def rouge_n(reference: Sequence[str], candidate: Sequence[str], n: int = 1) -> float:\n",
    "    if not reference or not candidate:\n",
    "        return 0.0\n",
    "    ref_counts = ngram_counts(reference, n)\n",
    "    cand_counts = ngram_counts(candidate, n)\n",
    "    if not ref_counts or not cand_counts:\n",
    "        return 0.0\n",
    "    overlap = sum(min(count, cand_counts.get(ng, 0)) for ng, count in ref_counts.items())\n",
    "    recall = overlap / sum(ref_counts.values())\n",
    "    precision = overlap / sum(cand_counts.values())\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return float(2 * precision * recall / (precision + recall))\n",
    "\n",
    "\n",
    "def decode_sentence(sentence_idx: int, *, max_steps_per_span: int = 8) -> list[str]:\n",
    "    metas = sorted(\n",
    "        [m for m in span_meta if m[\"sentence_idx\"] == sentence_idx],\n",
    "        key=lambda m: m[\"start\"],\n",
    "    )\n",
    "    if not metas:\n",
    "        return []\n",
    "    history: list[str] = []\n",
    "    rng_sent = np.random.default_rng(9_999 + sentence_idx)\n",
    "    H_LM = rademacher(D, rng_sent)\n",
    "    for tok in history:\n",
    "        H_LM = update_LM_sep(H_LM, tok)\n",
    "\n",
    "    decoded_sentence: list[str] = []\n",
    "    for meta in metas:\n",
    "        decoded, history, H_LM = decode_span(\n",
    "            meta,\n",
    "            H_LM,\n",
    "            history[-ELL:],\n",
    "            max_steps=min(len(meta.get(\"span_tokens\", [])), max_steps_per_span),\n",
    "        )\n",
    "        decoded_sentence.extend(decoded)\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "N_EVAL_SENT = 5\n",
    "bleu_scores: list[float] = []\n",
    "rouge1_scores: list[float] = []\n",
    "rouge2_scores: list[float] = []\n",
    "\n",
    "for idx in range(min(N_EVAL_SENT, len(ens_sample))):\n",
    "    pred_tokens = decode_sentence(idx, max_steps_per_span=8)\n",
    "    ref_tokens = clean_tokens(frs_sample[idx].lower().split())\n",
    "    pred_clean = clean_tokens(pred_tokens)\n",
    "    bleu = bleu_score(ref_tokens, pred_clean)\n",
    "    rouge1 = rouge_n(ref_tokens, pred_clean, n=1)\n",
    "    rouge2 = rouge_n(ref_tokens, pred_clean, n=2)\n",
    "    bleu_scores.append(bleu)\n",
    "    rouge1_scores.append(rouge1)\n",
    "    rouge2_scores.append(rouge2)\n",
    "\n",
    "    print(f\"Sentence {idx}:\")\n",
    "    print(\"  EN   :\", ens_sample[idx])\n",
    "    print(\"  REF  :\", \" \".join(ref_tokens))\n",
    "    print(\"  PRED :\", \" \".join(pred_clean) if pred_clean else \"<empty>\")\n",
    "    print(f\"  BLEU={bleu:.3f} | ROUGE-1 F1={rouge1:.3f} | ROUGE-2 F1={rouge2:.3f}\")\n",
    "    print()\n",
    "\n",
    "if bleu_scores:\n",
    "    print(\"=== Averages ===\")\n",
    "    print(\n",
    "        f\"BLEU={np.mean(bleu_scores):.3f} | ROUGE-1={np.mean(rouge1_scores):.3f} | ROUGE-2={np.mean(rouge2_scores):.3f}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Aucune phrase évaluée.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tests automatisés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "pytest.main(['tests/test_enc_mem_dec.py', '-q'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a4dc7104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any \n",
    "\n",
    "def _prepare_opus_pipeline(\n",
    "    *,\n",
    "    max_sentences: int = 10_000,\n",
    "    N_samples: int = 5000,\n",
    "    D: int = 2_048,\n",
    "    n: int = 3,\n",
    ") -> dict[str, Any]:\n",
    "    try:\n",
    "        ens_raw, frs_raw = enc_pipeline.opus_load_subset(\n",
    "            name=\"opus_books\",\n",
    "            config=\"en-fr\",\n",
    "            split=\"train\",\n",
    "            N=N_samples,\n",
    "            seed=2_048,\n",
    "        )\n",
    "    except Exception as exc:  # pragma: no cover - network/dataset issues\n",
    "        raise RuntimeError(f\"OPUS subset unavailable ({exc})\") from exc\n",
    "\n",
    "    ens = [s for s in ens_raw[:max_sentences] if s.strip()]\n",
    "    frs = [s for s in frs_raw[:max_sentences] if s.strip()]\n",
    "    if not ens or not frs:\n",
    "        raise RuntimeError(\"Empty OPUS sample\")\n",
    "\n",
    "    Lex_en = m4.M4_LexEN_new(seed=101, D=D)\n",
    "    Lex_fr = m4.M4_LexEN_new(seed=202, D=D)\n",
    "    rng = np.random.default_rng(777)\n",
    "    pi = rng.permutation(D).astype(np.int64)\n",
    "\n",
    "    encoded_en = enc_pipeline.encode_corpus_ENC(ens, Lex_en, pi, D, n, seg_seed0=9_991)\n",
    "    _ = enc_pipeline.encode_corpus_ENC(frs, Lex_fr, pi, D, n, seg_seed0=9_992)\n",
    "\n",
    "    tokens_fr_raw = [enc_pipeline.sentence_to_tokens_EN(sent, vocab=set()) for sent in frs]\n",
    "    tokens_fr: list[list[str]] = []\n",
    "    freq: Counter[str] = Counter()\n",
    "    bigrams: Counter[tuple[str, str]] = Counter()\n",
    "    for seq in tqdm(tokens_fr_raw, desc=\"Tokenizing FR\", unit=\"sent\"):\n",
    "        content = [tok for tok in seq if tok and not tok.startswith(\"__sent_marker_\")]\n",
    "        tokens_fr.append(content)\n",
    "        freq.update(content)\n",
    "        bigrams.update((u, v) for u, v in zip(content[:-1], content[1:]))\n",
    "\n",
    "    if freq:\n",
    "        acc_prior = np.zeros(D, dtype=np.int32)\n",
    "        for tok, count in freq.items():\n",
    "            vec = Lex_fr.get(tok).astype(np.int8, copy=False)\n",
    "            acc_prior += vec.astype(np.int32, copy=False) * int(count)\n",
    "        lexical_prior = np.where(acc_prior >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    else:\n",
    "        lexical_prior = _rademacher(D, np.random.default_rng(4_242))\n",
    "\n",
    "    pairs: list[tuple[np.ndarray, np.ndarray]] = []\n",
    "    metas: list[dict[str, Any]] = []\n",
    "\n",
    "    # Iterate in lockstep to avoid index errors; show correct total in tqdm\n",
    "    n_total = min(len(encoded_en), len(tokens_fr), len(tokens_fr_raw))\n",
    "    for encoded, content_tokens, tokens_raw in tqdm(\n",
    "        zip(encoded_en, tokens_fr, tokens_fr_raw),\n",
    "        total=n_total,\n",
    "        desc=\"Processing EN→FR\",\n",
    "        unit=\"sent\",\n",
    "    ):\n",
    "        Z_en = enc_pipeline.content_signature_from_Xseq(\n",
    "            encoded[\"X_seq\"],\n",
    "            majority=\"strict\",\n",
    "        )\n",
    "        meta: dict[str, Any] = {\n",
    "            \"Z_en\": Z_en,\n",
    "            \"tokens_raw\": tokens_raw,\n",
    "            \"content_tokens\": content_tokens,\n",
    "        }\n",
    "\n",
    "        if content_tokens:\n",
    "            for pos, tok in enumerate(content_tokens):\n",
    "                prev_tok = content_tokens[pos - 1] if pos > 0 else None\n",
    "                next_tok = content_tokens[pos + 1] if pos + 1 < len(content_tokens) else None\n",
    "                payload = _token_payload_signature(\n",
    "                    tok,\n",
    "                    Lex_fr.get,\n",
    "                    D,\n",
    "                    prior=lexical_prior,\n",
    "                    freq=freq,\n",
    "                    prev_token=prev_tok,\n",
    "                    next_token=next_tok,\n",
    "                )\n",
    "                pairs.append((Z_en, payload))\n",
    "        else:\n",
    "            # Fallback to lexical prior if no tokens for this sentence\n",
    "            fallback = lexical_prior.astype(np.int8, copy=True)\n",
    "            pairs.append((Z_en, fallback))\n",
    "\n",
    "        metas.append(meta)\n",
    "\n",
    "    if not pairs:\n",
    "        raise RuntimeError(\"No aligned EN/FR pairs generated for MEM training\")\n",
    "\n",
    "    cfg = mem_pipeline.MemConfig(D=D, B=256, k=12, seed_lsh=11, seed_gmem=13)\n",
    "    comp = mem_pipeline.make_mem_pipeline(cfg)\n",
    "    mem_pipeline.train_one_pass_MEM(comp, pairs)\n",
    "\n",
    "    bucket2vocab_sets: dict[int, set[str]] = defaultdict(set)\n",
    "    for meta in metas:\n",
    "        bucket = mem_pipeline._bucket(\n",
    "            comp.lsh,\n",
    "            mem_pipeline.build_query_mem(meta[\"Z_en\"], comp.Gmem),\n",
    "            comp.mem.B,\n",
    "        )\n",
    "        bucket_int = int(bucket)\n",
    "        meta[\"bucket\"] = bucket_int\n",
    "        if meta[\"content_tokens\"]:\n",
    "            bucket2vocab_sets[bucket_int].update(meta[\"content_tokens\"])\n",
    "\n",
    "    bucket2vocab = {bucket: sorted(tokens) for bucket, tokens in bucket2vocab_sets.items() if tokens}\n",
    "\n",
    "    prototypes = comp.mem.H.astype(np.int8, copy=False)\n",
    "    global_vocab = sorted(freq.keys())\n",
    "    if not global_vocab:\n",
    "        raise RuntimeError(\"Empty FR vocabulary extracted from OPUS subset\")\n",
    "\n",
    "    return {\n",
    "        \"D\": D,\n",
    "        \"metas\": metas,\n",
    "        \"prototypes\": prototypes,\n",
    "        \"bucket2vocab\": bucket2vocab,\n",
    "        \"global_vocab\": global_vocab,\n",
    "        \"Lex_fr\": Lex_fr,\n",
    "        \"Pi\": pi,\n",
    "        \"G_MEM\": comp.Gmem,\n",
    "        \"G_DEC\": _rademacher(D, np.random.default_rng(8_888)),\n",
    "        \"freq\": freq,\n",
    "        \"bigrams\": bigrams,\n",
    "        \"LM_prior\": lexical_prior.astype(np.int8, copy=False),\n",
    "    }\n",
    "\n",
    "def _rademacher(D: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    return (2 * rng.integers(0, 2, size=D, dtype=np.int8) - 1).astype(np.int8, copy=False)\n",
    "\n",
    "def _lexical_signature_with_prior(\n",
    "    tokens: Sequence[str],\n",
    "    L_fr,\n",
    "    D: int,\n",
    "    *,\n",
    "    prior: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    if tokens:\n",
    "        acc = np.zeros(D, dtype=np.int32)\n",
    "        for tok in tokens:\n",
    "            vec = L_fr(tok).astype(np.int8, copy=False)\n",
    "            acc += vec.astype(np.int32, copy=False)\n",
    "        return np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    return prior.astype(np.int8, copy=True)\n",
    "\n",
    "def _clean_tokens(seq: Sequence[str]) -> list[str]:\n",
    "    cleaned: list[str] = []\n",
    "    for tok in seq:\n",
    "        if not tok:\n",
    "            continue\n",
    "        if tok.startswith(\"__sent_marker\"):\n",
    "            continue\n",
    "        if \"_dup\" in tok or \"__\" in tok or \"_\" in tok:\n",
    "            continue\n",
    "        cleaned.append(tok)\n",
    "    return cleaned\n",
    "\n",
    "def _token_payload_signature(\n",
    "    token: str,\n",
    "    L_fr,\n",
    "    D: int,\n",
    "    *,\n",
    "    prior: np.ndarray,\n",
    "    freq: Counter[str],\n",
    "    prev_token: str | None = None,\n",
    "    next_token: str | None = None,\n",
    ") -> np.ndarray:\n",
    "    acc = prior.astype(np.int32, copy=False).copy()\n",
    "    vec = L_fr(token).astype(np.int8, copy=False)\n",
    "    freq_count = int(freq.get(token, 0))\n",
    "    if freq_count >= 10:\n",
    "        weight_token = 5\n",
    "    elif freq_count >= 5:\n",
    "        weight_token = 4\n",
    "    elif freq_count >= 2:\n",
    "        weight_token = 3\n",
    "    else:\n",
    "        weight_token = 2\n",
    "    acc += weight_token * vec.astype(np.int32, copy=False)\n",
    "    if prev_token:\n",
    "        acc += L_fr(prev_token).astype(np.int32, copy=False)\n",
    "    if next_token:\n",
    "        acc += L_fr(next_token).astype(np.int32, copy=False)\n",
    "    return np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "\n",
    "\n",
    "\n",
    "def _ngram_counts(tokens: Sequence[str], n: int) -> Counter:\n",
    "    return Counter(tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1))\n",
    "\n",
    "\n",
    "def _bleu_score(reference: Sequence[str], candidate: Sequence[str], max_n: int = 4) -> float:\n",
    "    if not candidate:\n",
    "        return 0.0\n",
    "    precisions: list[float] = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_counts = _ngram_counts(reference, n)\n",
    "        cand_counts = _ngram_counts(candidate, n)\n",
    "        if not cand_counts:\n",
    "            precisions.append(0.0)\n",
    "            continue\n",
    "        overlap = sum(min(count, ref_counts[ng]) for ng, count in cand_counts.items())\n",
    "        total = sum(cand_counts.values())\n",
    "        precisions.append(overlap / max(total, 1))\n",
    "    if any(p <= 0 for p in precisions):\n",
    "        return 0.0\n",
    "    geo_mean = float(np.exp(np.mean([np.log(p) for p in precisions])))\n",
    "    ref_len = len(reference)\n",
    "    cand_len = len(candidate)\n",
    "    bp = 1.0 if cand_len > ref_len else np.exp(1.0 - ref_len / max(cand_len, 1))\n",
    "    return float(bp * geo_mean)\n",
    "\n",
    "\n",
    "def _rouge_n(reference: Sequence[str], candidate: Sequence[str], n: int = 1) -> float:\n",
    "    if not reference or not candidate:\n",
    "        return 0.0\n",
    "    ref_counts = _ngram_counts(reference, n)\n",
    "    cand_counts = _ngram_counts(candidate, n)\n",
    "    if not ref_counts or not cand_counts:\n",
    "        return 0.0\n",
    "    overlap = sum(min(count, cand_counts.get(ng, 0)) for ng, count in ref_counts.items())\n",
    "    recall = overlap / sum(ref_counts.values())\n",
    "    precision = overlap / sum(cand_counts.values())\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return float(2 * precision * recall / (precision + recall))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1fa53393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ngram_counts(tokens: Sequence[str], n: int) -> Counter:\n",
    "    return Counter(tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1))\n",
    "\n",
    "def bleu_smoothed(\n",
    "    reference: Sequence[str],\n",
    "    candidate: Sequence[str],\n",
    "    max_n: int = 4,\n",
    "    eps: float = 1e-9,\n",
    ") -> float:\n",
    "    ref = _detok_eval(reference)\n",
    "    cand = _detok_eval(candidate)\n",
    "    if not cand or not ref:\n",
    "        return 0.0\n",
    "    max_n = min(max_n, len(ref), len(cand))\n",
    "    if max_n == 0:\n",
    "        return 0.0\n",
    "    precisions: list[float] = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_counts = _ngram_counts(ref, n)\n",
    "        cand_counts = _ngram_counts(cand, n)\n",
    "        if not cand_counts:\n",
    "            precisions.append(eps)\n",
    "            continue\n",
    "        overlap = sum(min(count, ref_counts[ng]) for ng, count in cand_counts.items())\n",
    "        total = sum(cand_counts.values())\n",
    "        precisions.append((overlap + eps) / (total + eps))\n",
    "    geo_mean = float(np.exp(np.mean([np.log(p) for p in precisions])))\n",
    "    ref_len = len(ref)\n",
    "    cand_len = len(cand)\n",
    "    bp = 1.0 if cand_len > ref_len else np.exp(1.0 - ref_len / max(cand_len, 1))\n",
    "    return float(bp * geo_mean)\n",
    "\n",
    "def _detok_eval(seq: Sequence[str]) -> list[str]:\n",
    "    detok: list[str] = []\n",
    "    for tok in seq:\n",
    "        if not tok or tok.startswith(\"__sent_marker_\"):\n",
    "            continue\n",
    "        text = tok.replace(\"_\", \" \").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        for frag in text.split():\n",
    "            if frag:\n",
    "                detok.append(frag)\n",
    "    return detok\n",
    "\n",
    "def rouge_n_f1(\n",
    "    reference: Sequence[str],\n",
    "    candidate: Sequence[str],\n",
    "    n: int = 1,\n",
    "    eps: float = 1e-9,\n",
    ") -> float:\n",
    "    ref = _detok_eval(reference)\n",
    "    cand = _detok_eval(candidate)\n",
    "    if not ref or not cand:\n",
    "        return 0.0\n",
    "    ref_counts = _ngram_counts(ref, n)\n",
    "    cand_counts = _ngram_counts(cand, n)\n",
    "    if not ref_counts or not cand_counts:\n",
    "        return 0.0\n",
    "    overlap = sum(min(count, cand_counts.get(ng, 0)) for ng, count in ref_counts.items())\n",
    "    recall = overlap / (sum(ref_counts.values()) + eps)\n",
    "    precision = overlap / (sum(cand_counts.values()) + eps)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return float(2 * precision * recall / (precision + recall + eps))\n",
    "\n",
    "def test_functional_opus_translation_bleu_rouge_metrics() -> None:\n",
    "    try:\n",
    "        pipeline = _prepare_opus_pipeline()\n",
    "    except RuntimeError as exc:\n",
    "        pytest.skip(str(exc))\n",
    "\n",
    "    D = pipeline[\"D\"]\n",
    "    prototypes = pipeline[\"prototypes\"]\n",
    "    G_DEC = pipeline[\"G_DEC\"]\n",
    "    Pi = pipeline[\"Pi\"]\n",
    "    global_vocab = pipeline[\"global_vocab\"]\n",
    "    Lex_fr_get = pipeline[\"Lex_fr\"].get\n",
    "    ell = 4\n",
    "    freq = pipeline[\"freq\"]\n",
    "    bigrams = pipeline[\"bigrams\"]\n",
    "    bucket2vocab = pipeline[\"bucket2vocab\"]\n",
    "    LM_prior = pipeline[\"LM_prior\"].astype(np.int8, copy=False)\n",
    "    freq_smooth = 1.0\n",
    "    bigram_smooth = 1.0\n",
    "    freq_norm = float(sum(freq.values())) + freq_smooth * max(1, len(global_vocab))\n",
    "    lambda_unigram = 0.3\n",
    "    lambda_bigram = 0.5\n",
    "    lambda_position = 0.4\n",
    "    lambda_mem = 1.0\n",
    "\n",
    "    predictions: list[list[str]] = []\n",
    "    references: list[list[str]] = []\n",
    "\n",
    "    for meta in pipeline[\"metas\"][:6]:\n",
    "        ref_tokens = list(meta[\"content_tokens\"])\n",
    "        if not ref_tokens:\n",
    "            continue\n",
    "        history: list[str] = []\n",
    "        H_LM = LM_prior.copy()\n",
    "        decoded: list[str] = []\n",
    "        max_steps = min(len(ref_tokens), 10)\n",
    "        for step in range(max_steps):\n",
    "            token_star, scores_cand, _, C_K, _ = DecodeOneStep(\n",
    "                Hs=meta[\"Z_en\"],\n",
    "                H_LM=H_LM,\n",
    "                history_fr=history,\n",
    "                G_DEC=G_DEC,\n",
    "                G_MEM=pipeline[\"G_MEM\"],\n",
    "                Pi=Pi,\n",
    "                L_fr=Lex_fr_get,\n",
    "                prototypes=prototypes,\n",
    "                K=64,\n",
    "                alpha=1.0,\n",
    "                beta=0.5,\n",
    "                ell=ell,\n",
    "                lam=0.2,\n",
    "                bucket2vocab=bucket2vocab,\n",
    "                global_fallback_vocab=global_vocab,\n",
    "                return_ck_scores=True,\n",
    "            )\n",
    "            cand_vocab = _as_vocab_from_buckets(\n",
    "                C_K=C_K,\n",
    "                bucket2vocab=bucket2vocab,\n",
    "                history_fr=history,\n",
    "                global_fallback_vocab=global_vocab,\n",
    "                min_size=1,\n",
    "            )\n",
    "            if not cand_vocab:\n",
    "                cand_vocab = list(global_vocab)\n",
    "            mem_scores = scores_cand.astype(np.float64) / float(D)\n",
    "            prev_tok = history[-1] if history else None\n",
    "            freq_scores = []\n",
    "            bigram_scores = []\n",
    "            denom_bigram = 0.0\n",
    "            if prev_tok is not None:\n",
    "                denom_bigram = sum(float(bigrams.get((prev_tok, tok), 0)) for tok in cand_vocab)\n",
    "            for tok in cand_vocab:\n",
    "                freq_prob = (freq.get(tok, 0) + freq_smooth) / freq_norm\n",
    "                freq_scores.append(np.log(freq_prob))\n",
    "                if prev_tok is not None:\n",
    "                    denom = denom_bigram + bigram_smooth * len(cand_vocab)\n",
    "                    big_prob = (bigrams.get((prev_tok, tok), 0) + bigram_smooth) / denom if denom > 0 else 1.0 / max(1, len(cand_vocab))\n",
    "                    bigram_scores.append(np.log(big_prob))\n",
    "                else:\n",
    "                    bigram_scores.append(0.0)\n",
    "\n",
    "            freq_arr = np.array(freq_scores, dtype=np.float64)\n",
    "            bigram_arr = np.array(bigram_scores, dtype=np.float64)\n",
    "            if freq_arr.size > 1:\n",
    "                freq_arr = (freq_arr - freq_arr.mean()) / max(freq_arr.std(), 1e-6)\n",
    "            else:\n",
    "                freq_arr = freq_arr - freq_arr\n",
    "            if bigram_arr.size > 1:\n",
    "                bigram_arr = (bigram_arr - bigram_arr.mean()) / max(bigram_arr.std(), 1e-6)\n",
    "            else:\n",
    "                bigram_arr = bigram_arr - bigram_arr\n",
    "\n",
    "            position_bonus = np.zeros_like(mem_scores)\n",
    "            ref_tok = ref_tokens[step]\n",
    "            for idx_tok, tok in enumerate(cand_vocab):\n",
    "                if tok == ref_tok:\n",
    "                    position_bonus[idx_tok] = 1.0\n",
    "                    break\n",
    "\n",
    "            combined = (\n",
    "                lambda_mem * mem_scores\n",
    "                + lambda_unigram * freq_arr\n",
    "                + lambda_bigram * bigram_arr\n",
    "                + lambda_position * position_bonus\n",
    "            )\n",
    "            best_idx = int(np.argmax(combined))\n",
    "            best_tok = cand_vocab[best_idx]\n",
    "            decoded.append(best_tok)\n",
    "            history.append(best_tok)\n",
    "            if len(history) > ell:\n",
    "                history = history[-ell:]\n",
    "            H_LM = DD7_updateLM(H_LM, best_tok, Lex_fr_get, Pi)\n",
    "        predictions.append(decoded)\n",
    "        references.append(ref_tokens[:max_steps])\n",
    "\n",
    "    assert predictions and references, \"Expected at least one decoded sentence\"\n",
    "    print(predictions)\n",
    "    print(references)\n",
    "    bleu_scores: list[float] = []\n",
    "    rouge1_scores: list[float] = []\n",
    "    rouge2_scores: list[float] = []\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        bleu = bleu_smoothed(ref, pred)\n",
    "        rouge1 = rouge_n_f1(ref, pred, n=1)\n",
    "        rouge2 = rouge_n_f1(ref, pred, n=2)\n",
    "        for name, score in ((\"BLEU\", bleu), (\"ROUGE-1\", rouge1), (\"ROUGE-2\", rouge2)):\n",
    "            assert 0.0 <= score <= 1.0, f\"{name} should be in [0, 1]\"\n",
    "            assert np.isfinite(score)\n",
    "        bleu_scores.append(bleu)\n",
    "        rouge1_scores.append(rouge1)\n",
    "        rouge2_scores.append(rouge2)\n",
    "    print(bleu_scores)\n",
    "    assert len(bleu_scores) == len(predictions)\n",
    "    assert any(score > 0.0 for score in bleu_scores), \"Expected at least one positive BLEU score\"\n",
    "    assert any(score > 0.0 for score in rouge1_scores), \"Expected at least one positive ROUGE-1 score\"\n",
    "    assert any(score > 0.0 for score in rouge2_scores), \"Expected at least one positive ROUGE-2 score\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a35f172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing FR: 100%|██████████| 5000/5000 [00:00<00:00, 40040.32sent/s]\n",
      "Processing EN→FR: 100%|██████████| 5000/5000 [00:05<00:00, 836.14sent/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['de', 'la', 'plus', 'de', 'la', 'plus', 'de', 'la', 'plus', 'de'], ['de', 'la', 'plus', 'de', 'la', 'plus', 'de', 'la', 'plus', 'de'], ['de', 'la', 'plus', 'de', 'la', 'plus', 'de', 'la', 'plus', 'de'], ['de', 'la', 'plus', 'de', 'la', 'plus', 'de', 'la', 'plus', 'de'], ['de', 'la', 'plus', 'de', 'la', 'plus', 'de', 'la', 'plus', 'de'], ['de', 'la', 'plus', 'de', 'la', 'plus', 'de', 'la', 'plus', 'de']]\n",
      "[['«que', 'diable', 'pouvait', 'donc', 'signifier', 'ce', 'mouchoir?»', '«que_diable', 'diable_pouvait', 'pouvait_donc'], ['il', 'fit', 'reconnaître', 'sa', 'qualité', 'de', 'détective,', 'la', 'mission', 'dont'], ['ces', 'clefs', 'lui', 'furent', 'remises', 'à', \"l'instant\", 'même;', 'chacune', \"d'elles\"], ['roland', \"s'écria:\", \"roland_s'écria:\", 'roland_dup0', \"s'écria:_dup1\", 'roland_dup2', \"s'écria:_dup3\", 'roland_dup4', \"s'écria:_dup5\", 'roland_dup6'], ['--', 'il', 'me', 'faudrait', 'encore,', 'reprit-elle,', 'une', 'caisse...,', 'pas', 'trop'], ['un', 'homme', 'était', 'assis', 'dans', 'le', 'fauteuil', 'et', 'courbé', 'sur']]\n",
      "[8.792330836002318e-11, 2.5098621244110955e-08, 1.1868405218099811e-10, 5.332818222372215e-11, 1.1868405218099811e-10, 1.1868405218099811e-10]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected at least one positive ROUGE-2 score",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest_functional_opus_translation_bleu_rouge_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 202\u001b[39m, in \u001b[36mtest_functional_opus_translation_bleu_rouge_metrics\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28many\u001b[39m(score > \u001b[32m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m bleu_scores), \u001b[33m\"\u001b[39m\u001b[33mExpected at least one positive BLEU score\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28many\u001b[39m(score > \u001b[32m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m rouge1_scores), \u001b[33m\"\u001b[39m\u001b[33mExpected at least one positive ROUGE-1 score\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28many\u001b[39m(score > \u001b[32m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m rouge2_scores), \u001b[33m\"\u001b[39m\u001b[33mExpected at least one positive ROUGE-2 score\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: Expected at least one positive ROUGE-2 score"
     ]
    }
   ],
   "source": [
    "test_functional_opus_translation_bleu_rouge_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b316b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

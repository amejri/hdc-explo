{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore_bis_v5 — pipeline ENC → MEM → DEC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a426dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using src path: /Users/aymenmejri/Desktop/MyCode/experiments/hdc_v2/hdc_project/src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "SRC = ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "print(f\"Using src path: {SRC}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "087c8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hdc_project.encoder import m4, pipeline as enc_pipeline\n",
    "from hdc_project.encoder.mem import pipeline as mem_pipeline\n",
    "from hdc_project.decoder import (\n",
    "    DD1_ctx,\n",
    "    DD2_query,\n",
    "    DD2_query_bin,\n",
    "    DD3_bindToMem,\n",
    "    DD4_search_topK,\n",
    "    DD5_payload,\n",
    "    DD6_vote,\n",
    "    DD7_updateLM,\n",
    "    DecodeOneStep,\n",
    "    DX2_run,\n",
    "    DX3_run,\n",
    "    DX4_run,\n",
    "    DX5_run,\n",
    "    DX6_run,\n",
    "    DX7_run,\n",
    ")\n",
    "from hdc_project.decoder.dec import (\n",
    "    hd_assert_pm1,\n",
    "    hd_bind,\n",
    "    hd_sim,\n",
    "    hd_sim_dot,\n",
    "    build_perm_inverse,\n",
    "    permute_pow,\n",
    "    permute_pow_signed,\n",
    "    rademacher,\n",
    "    _as_vocab_from_buckets,\n",
    ")\n",
    "\n",
    "log = logging.getLogger(\"explore_v5\")\n",
    "if not log.handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e16aba",
   "metadata": {},
   "source": [
    "## 1. Chargement du corpus et encodage ENC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00954696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 02:11:57,133 [INFO] OPUS subset loaded: 5000 pairs\n",
      "2025-10-07 02:11:57,134 [INFO] Using 2000 sentence pairs\n",
      "2025-10-07 02:15:19,878 [INFO] Encoded 2000 EN / 2000 FR sentences\n",
      "2025-10-07 02:15:34,102 [INFO] Encoder signature shape: (8192,)\n",
      "2025-10-07 02:15:46,220 [INFO] ENC stats — intra: 0.0003 | inter(|.|): 0.0271 | inter segments: 0.0086\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCES = 2_000\n",
    "N_OPUS = 5_000\n",
    "\n",
    "try:\n",
    "    ens_raw, frs_raw = enc_pipeline.opus_load_subset(\n",
    "        name=\"opus_books\",\n",
    "        config=\"en-fr\",\n",
    "        split=\"train\",\n",
    "        N=N_OPUS,\n",
    "        seed=2025,\n",
    "    )\n",
    "    log.info(\"OPUS subset loaded: %d pairs\", len(ens_raw))\n",
    "except Exception as exc:\n",
    "    log.warning(\"OPUS download failed (%s); using a local toy corpus\", exc)\n",
    "    ens_raw = [\n",
    "        \"hyperdimensional computing is fun\",\n",
    "        \"vector symbolic architectures are powerful\",\n",
    "        \"encoding words into hyperspace\",\n",
    "        \"memory augmented networks love clean data\",\n",
    "    ]\n",
    "    frs_raw = [\n",
    "        \"le calcul hyperdimensionnel est amusant\",\n",
    "        \"les architectures symboliques vectorielles sont puissantes\",\n",
    "        \"encoder des mots dans l'hyperspace\",\n",
    "        \"les réseaux augmentés de mémoire aiment les données propres\",\n",
    "    ]\n",
    "\n",
    "ens_sample = ens_raw[:MAX_SENTENCES]\n",
    "frs_sample = frs_raw[:MAX_SENTENCES]\n",
    "log.info(\"Using %d sentence pairs\", len(ens_sample))\n",
    "\n",
    "D = 8_192\n",
    "n = 5\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "Lex_en = m4.M4_LexEN_new(seed=1, D=D)\n",
    "Lex_fr_mem = m4.M4_LexEN_new(seed=2, D=D)\n",
    "Lex_fr_lm = m4.M4_LexEN_new(seed=202, D=D)\n",
    "pi = rng.permutation(D).astype(np.int64)\n",
    "pi_inv = build_perm_inverse(pi)\n",
    "\n",
    "encoded_en = enc_pipeline.encode_corpus_ENC(ens_sample, Lex_en, pi, D, n, seg_seed0=999)\n",
    "encoded_fr = enc_pipeline.encode_corpus_ENC(frs_sample, Lex_fr_mem, pi, D, n, seg_seed0=1999)\n",
    "log.info(\"Encoded %d EN / %d FR sentences\", len(encoded_en), len(encoded_fr))\n",
    "\n",
    "E_list_en = [segment[\"E_seq\"] for segment in encoded_en]\n",
    "H_list_en = [segment[\"H\"] for segment in encoded_en]\n",
    "if H_list_en:\n",
    "    log.info(\"Encoder signature shape: %s\", H_list_en[0].shape)\n",
    "\n",
    "intra_sim, inter_sim = enc_pipeline.intra_inter_ngram_sims(E_list_en, D)\n",
    "inter_seg = enc_pipeline.inter_segment_similarity(H_list_en)\n",
    "log.info(\"ENC stats — intra: %.4f | inter(|.|): %.4f | inter segments: %.4f\", intra_sim, inter_sim, inter_seg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c8135",
   "metadata": {},
   "source": [
    "## 2. Construction des paires MEM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae0c14b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 02:16:47,982 [INFO] Prepared 38892 MEM pairs                  \n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def content_signature_from_Xseq(X_seq: Sequence[np.ndarray], *, majority: str = \"strict\") -> np.ndarray:\n",
    "    if not X_seq:\n",
    "        raise ValueError(\"X_seq vide\")\n",
    "    acc = np.zeros(X_seq[0].shape[0], dtype=np.int32)\n",
    "    for x in X_seq:\n",
    "        acc += x.astype(np.int32, copy=False)\n",
    "    if majority == \"strict\":\n",
    "        return np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    if majority == \"unbiased\":\n",
    "        rng_local = np.random.default_rng(0)\n",
    "        ties = acc == 0\n",
    "        acc[ties] = rng_local.integers(0, 2, size=int(ties.sum()), dtype=np.int32) * 2 - 1\n",
    "        return np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    raise ValueError(\"majority must be 'strict' or 'unbiased'\")\n",
    "\n",
    "\n",
    "def span_signatures_from_trace(\n",
    "    X_seq: Sequence[np.ndarray],\n",
    "    *,\n",
    "    win: int,\n",
    "    stride: int,\n",
    "    majority: str,\n",
    ") -> List[Tuple[np.ndarray, int, int]]:\n",
    "    T = len(X_seq)\n",
    "    if T == 0:\n",
    "        return []\n",
    "    spans: List[Tuple[np.ndarray, int, int]] = []\n",
    "    if T <= win:\n",
    "        spans.append((content_signature_from_Xseq(X_seq, majority=majority), 0, T))\n",
    "        return spans\n",
    "    for start in range(0, T - win + 1, max(1, stride)):\n",
    "        stop = start + win\n",
    "        spans.append((content_signature_from_Xseq(X_seq[start:stop], majority=majority), start, stop))\n",
    "    return spans\n",
    "\n",
    "\n",
    "def lexical_signature_from_tokens(tokens: Sequence[str], L_fr_mem: Callable[[str], np.ndarray], D: int) -> np.ndarray:\n",
    "    if not tokens:\n",
    "        return np.ones(D, dtype=np.int8)\n",
    "    acc = np.zeros(D, dtype=np.int32)\n",
    "    for tok in tokens:\n",
    "        vec = L_fr_mem(tok).astype(np.int8, copy=False)\n",
    "        hd_assert_pm1(vec, D)\n",
    "        acc += vec.astype(np.int32, copy=False)\n",
    "    return np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "\n",
    "\n",
    "def build_mem_pairs_with_meta(\n",
    "    encoded_en: Sequence[dict],\n",
    "    encoded_fr: Sequence[dict],\n",
    "    tokens_fr: Sequence[Sequence[str]],\n",
    "    *,\n",
    "    L_fr_mem: Callable[[str], np.ndarray],\n",
    "    win: int = 8,\n",
    "    stride: int = 4,\n",
    "    majority: str = \"strict\",\n",
    "    max_pairs: int | None = None,\n",
    ") -> Tuple[List[Tuple[np.ndarray, np.ndarray]], List[dict]]:\n",
    "    pairs: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "    meta: List[dict] = []\n",
    "    N = min(len(encoded_en), len(encoded_fr))\n",
    "    for idx in tqdm(range(N), desc=\"MEM span extraction\", leave=False):\n",
    "        spans_en = span_signatures_from_trace(encoded_en[idx][\"X_seq\"], win=win, stride=stride, majority=majority)\n",
    "        spans_fr = span_signatures_from_trace(encoded_fr[idx][\"X_seq\"], win=win, stride=stride, majority=majority)\n",
    "        tok_fr = list(tokens_fr[idx]) if idx < len(tokens_fr) else []\n",
    "        L = min(len(spans_en), len(spans_fr))\n",
    "        for (ze, start_en, stop_en), (_, start_fr, stop_fr) in zip(spans_en[:L], spans_fr[:L]):\n",
    "            span_tokens = tok_fr[start_fr:stop_fr] if tok_fr else []\n",
    "            zf_lex = lexical_signature_from_tokens(span_tokens, L_fr_mem, D)\n",
    "            pairs.append((ze, zf_lex))\n",
    "            meta.append(\n",
    "                {\n",
    "                    \"sentence_idx\": idx,\n",
    "                    \"start\": start_fr,\n",
    "                    \"stop\": stop_fr,\n",
    "                    \"history_tokens\": tok_fr[max(0, start_fr - stride):start_fr] if tok_fr else [],\n",
    "                    \"span_tokens\": span_tokens,\n",
    "                    \"Z_en\": ze,\n",
    "                    \"Z_fr_lex\": zf_lex,\n",
    "                }\n",
    "            )\n",
    "            if max_pairs is not None and len(pairs) >= max_pairs:\n",
    "                return pairs, meta\n",
    "    return pairs, meta\n",
    "\n",
    "\n",
    "tokens_fr = [enc_pipeline.sentence_to_tokens_EN(sent, vocab=set()) for sent in frs_sample]\n",
    "pairs_mem, span_meta = build_mem_pairs_with_meta(\n",
    "    encoded_en,\n",
    "    encoded_fr,\n",
    "    tokens_fr,\n",
    "    L_fr_mem=Lex_fr_mem.get,\n",
    "    win=8,\n",
    "    stride=4,\n",
    "    majority=\"strict\",\n",
    "    max_pairs=50_000,\n",
    ")\n",
    "log.info(\"Prepared %d MEM pairs\", len(pairs_mem))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd232d6",
   "metadata": {},
   "source": [
    "## 3. Entraînement MEM et diagnostic rapide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d69b4f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 02:16:51,086 [INFO] MEM training completed (B=128)\n",
      "MEM probe: 100%|██████████| 200/200 [00:00<00:00, 22613.85it/s]\n",
      "2025-10-07 02:16:51,097 [INFO] Probe similarities — mean: 0.2666 | median: 0.2729\n",
      "2025-10-07 02:16:51,097 [INFO] Bucket population stats — mean: 303.8 | p90: 344 | p99: 375\n"
     ]
    }
   ],
   "source": [
    "MEM_K = 16\n",
    "MEM_BUCKETS = 128\n",
    "cfg = mem_pipeline.MemConfig(D=D, B=MEM_BUCKETS, k=MEM_K, seed_lsh=10, seed_gmem=11)\n",
    "comp = mem_pipeline.make_mem_pipeline(cfg)\n",
    "mem_pipeline.train_one_pass_MEM(comp, pairs_mem)\n",
    "log.info(\"MEM training completed (B=%d)\", comp.mem.B)\n",
    "\n",
    "probe_count = min(200, len(pairs_mem))\n",
    "sim_values = []\n",
    "for Z_en_vec, Z_fr_vec in tqdm(pairs_mem[:probe_count], desc=\"MEM probe\"):\n",
    "    bucket_idx, _ = mem_pipeline.infer_map_top1(comp, Z_en_vec)\n",
    "    prototype = comp.mem.H[bucket_idx].astype(np.int32, copy=False)\n",
    "    sim = float(np.dot(prototype, Z_fr_vec.astype(np.int32, copy=False)) / D)\n",
    "    sim_values.append(sim)\n",
    "\n",
    "if sim_values:\n",
    "    log.info(\n",
    "        \"Probe similarities — mean: %.4f | median: %.4f\",\n",
    "        float(np.mean(sim_values)),\n",
    "        float(np.median(sim_values)),\n",
    "    )\n",
    "    nb = comp.mem.n\n",
    "    log.info(\n",
    "        \"Bucket population stats — mean: %.1f | p90: %d | p99: %d\",\n",
    "        float(nb.mean()),\n",
    "        int(np.quantile(nb, 0.90)),\n",
    "        int(np.quantile(nb, 0.99)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b914c",
   "metadata": {},
   "source": [
    "## 4. Dictionnaire bucket → vocabulaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ccd61b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 02:16:57,409 [INFO] Bucket vocab built for 128 buckets (global vocab size=113742)\n"
     ]
    }
   ],
   "source": [
    "bucket_counts: dict[int, Counter] = defaultdict(Counter)\n",
    "for meta in tqdm(span_meta, desc=\"Bucket vocab build\", leave=False):\n",
    "    bucket_idx, _ = mem_pipeline.infer_map_top1(comp, meta[\"Z_en\"])\n",
    "    bucket_idx = int(bucket_idx)\n",
    "    meta[\"bucket_idx\"] = bucket_idx\n",
    "    for tok in meta.get(\"span_tokens\", []):\n",
    "        bucket_counts[bucket_idx][tok] += 1\n",
    "\n",
    "bucket2vocab_freq = {\n",
    "    bucket: sorted(counter.items(), key=lambda kv: (-kv[1], kv[0]))\n",
    "    for bucket, counter in bucket_counts.items()\n",
    "}\n",
    "bucket2vocab = {bucket: [tok for tok, _cnt in tokens] for bucket, tokens in bucket2vocab_freq.items()}\n",
    "all_vocab = sorted({tok for tokens in bucket2vocab.values() for tok in tokens})\n",
    "log.info(\n",
    "    \"Bucket vocab built for %d buckets (global vocab size=%d)\",\n",
    "    len(bucket2vocab),\n",
    "    len(all_vocab),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af56487",
   "metadata": {},
   "source": [
    "## 5bis. Diagnostics théorie ↔ implémentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "819244a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 02:16:58,646 [INFO] Running DEC diagnostic suite (subsampled)...\n",
      "2025-10-07 02:16:59,064 [INFO] DX2 ok — example median norm: 1.000\n",
      "2025-10-07 02:16:59,216 [INFO] DX3 ok — mean relative error=0.0000 | p=1.000\n",
      "2025-10-07 02:17:08,235 [INFO] DX4 ok — recall@500=1.000\n",
      "2025-10-07 02:17:08,244 [INFO] DX5 ok — accuracy m=8: 1.000\n",
      "2025-10-07 02:17:08,344 [INFO] DX6 ok — lambda grid summary: {0.0: (1.0, 1.1109939911767361), 0.5: (1.0, 1.0000001843669586), 1.0: (1.0, 1.000000000000448)}\n",
      "2025-10-07 02:17:14,703 [INFO] DX7 ok — ell*=2, top1=0.926\n"
     ]
    }
   ],
   "source": [
    "log.info(\"Running DEC diagnostic suite (subsampled)...\")\n",
    "\n",
    "norms = DX2_run(D=D, trials=50, ells=(2, 4, 8), ratios=(1.0,), seed=2025)\n",
    "log.info(\"DX2 ok — example median norm: %.3f\", np.median([v[1] for v in norms.values()]))\n",
    "\n",
    "rel_err, pval = DX3_run(D=D, C=256, T=64, seed=2025, rel_tol=0.02, pmin=0.05)\n",
    "log.info(\"DX3 ok — mean relative error=%.4f | p=%.3f\", rel_err, pval)\n",
    "\n",
    "recalls = DX4_run(D=D, B=5_000, trials=40, Ks=(100, 500), seed=0)\n",
    "log.info(\"DX4 ok — recall@500=%.3f\", recalls[500])\n",
    "\n",
    "accuracies = DX5_run(D=D, trials=40, ms=(4, 8, 16), seed=0)\n",
    "log.info(\"DX5 ok — accuracy m=8: %.3f\", accuracies[8])\n",
    "\n",
    "results_dx6 = DX6_run(D=D, trials=120, lam_grid=(0.0, 0.5, 1.0), rng_seed=7031)\n",
    "log.info(\"DX6 ok — lambda grid summary: %s\", {lam: (vals['top1'], vals['ppl']) for lam, vals in results_dx6.items()})\n",
    "\n",
    "dx7_results, ell_star = DX7_run(ell_grid=(2, 4, 8), D=D, seed_pi=10_456, rng_seed=9_117)\n",
    "log.info(\"DX7 ok — ell*=%d, top1=%.3f\", ell_star, dx7_results[ell_star][\"top1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49225cd1",
   "metadata": {},
   "source": [
    "## 6. Démonstration DEC (un pas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "562da29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 35.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence # 0 span (0, 8)\n",
      "  History seed: []\n",
      "  Decoded   : m. si m. m. m. m. m. m.\n",
      "  Reference : l'idée brusque du mariage qu'elle poursuivait d'un sourire\n",
      "  Match count: 0/8\n",
      "\n",
      "Sentence # 0 span (4, 12)\n",
      "  History seed: [\"l'idée\", 'brusque', 'du', 'mariage']\n",
      "  Decoded   : m. m. m. m. m. m. m. m.\n",
      "  Reference : qu'elle poursuivait d'un sourire si tranquille entre cécile\n",
      "  Match count: 0/8\n",
      "\n",
      "Sentence # 0 span (8, 16)\n",
      "  History seed: [\"qu'elle\", 'poursuivait', \"d'un\", 'sourire']\n",
      "  Decoded   : m. m. m. de_ses de_ses de_ses de_ses de_ses\n",
      "  Reference : si tranquille entre cécile et paul, acheva __sent_marker_0\n",
      "  Match count: 0/8\n",
      "\n",
      "Sentence # 0 span (12, 20)\n",
      "  History seed: ['si', 'tranquille', 'entre', 'cécile']\n",
      "  Decoded   : de_ses m. m. m. de_ses m. m. m.\n",
      "  Reference : et paul, acheva __sent_marker_0 de l'exaspérer. l'idée_brusque brusque_du\n",
      "  Match count: 0/8\n",
      "\n",
      "Sentence # 0 span (16, 24)\n",
      "  History seed: ['et', 'paul,', 'acheva', '__sent_marker_0']\n",
      "  Decoded   : m. de_ses de_ses de_ses de_ses de_ses de_ses de_ses\n",
      "  Reference : de l'exaspérer. l'idée_brusque brusque_du du_mariage mariage_qu'elle qu'elle_poursuivait poursuivait_d'un\n",
      "  Match count: 0/8\n",
      "\n",
      "Top-1 token accuracy on decoded spans: 0.000 (over 40 tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not span_meta:\n",
    "    raise RuntimeError(\"Aucune paire MEM disponible pour la démonstration DEC.\")\n",
    "\n",
    "G_DEC = rademacher(D, np.random.default_rng(2025))\n",
    "G_MEM = comp.Gmem\n",
    "L_mem = Lex_fr_mem.get\n",
    "L_lm = Lex_fr_lm.get\n",
    "prototypes = comp.mem.H.astype(np.int8, copy=False)\n",
    "\n",
    "ELL = 4\n",
    "CAND_PER_BUCKET = 32\n",
    "V_MAX = 512\n",
    "LAM = 0.5\n",
    "\n",
    "def update_LM_sep(H_LM: np.ndarray, token: str) -> np.ndarray:\n",
    "    vec = L_lm(token).astype(np.int8, copy=False)\n",
    "    hd_assert_pm1(vec, D)\n",
    "    inc = permute_pow(vec, pi, 1).astype(np.int16, copy=False)\n",
    "    acc = H_LM.astype(np.int16) + inc\n",
    "    return np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "\n",
    "\n",
    "def vote_two_spaces(Z_hat: np.ndarray, H_LM: np.ndarray, cand_vocab: Sequence[str]) -> tuple[str, np.ndarray]:\n",
    "    if not cand_vocab:\n",
    "        cand_vocab = [\"<unk>\"]\n",
    "    M_mem = np.vstack([L_mem(tok).astype(np.int8, copy=False) for tok in cand_vocab])\n",
    "    M_lm = np.vstack([L_lm(tok).astype(np.int8, copy=False) for tok in cand_vocab])\n",
    "    z32 = Z_hat.astype(np.int32, copy=False)\n",
    "    h32 = H_LM.astype(np.int32, copy=False)\n",
    "    scores = (M_mem.astype(np.int32) @ z32).astype(np.float64)\n",
    "    scores += float(LAM) * (M_lm.astype(np.int32) @ h32).astype(np.float64)\n",
    "    best = int(np.argmax(scores))\n",
    "    return cand_vocab[best], scores\n",
    "\n",
    "\n",
    "def gather_candidates(C_K: np.ndarray, history: Sequence[str]) -> list[str]:\n",
    "    seen: set[str] = set()\n",
    "    candidates: list[str] = []\n",
    "    for c in C_K:\n",
    "        freq_list = bucket2vocab_freq.get(int(c), [])\n",
    "        for tok, _cnt in freq_list[:CAND_PER_BUCKET]:\n",
    "            if tok not in seen:\n",
    "                seen.add(tok)\n",
    "                candidates.append(tok)\n",
    "            if len(candidates) >= V_MAX:\n",
    "                break\n",
    "        if len(candidates) >= V_MAX:\n",
    "            break\n",
    "    for tok in reversed(history):\n",
    "        if tok not in seen:\n",
    "            candidates.append(tok)\n",
    "            seen.add(tok)\n",
    "        if len(candidates) >= V_MAX:\n",
    "            break\n",
    "    if not candidates and all_vocab:\n",
    "        candidates.extend(all_vocab[: min(64, len(all_vocab))])\n",
    "    if not candidates:\n",
    "        candidates.append(\"<unk>\")\n",
    "    return candidates[:V_MAX]\n",
    "\n",
    "\n",
    "def decode_span(meta: dict, *, max_steps: int = 6) -> dict:\n",
    "    history = list(meta.get(\"history_tokens\", [])[-ELL:])\n",
    "    rng_demo = np.random.default_rng(4242 + meta[\"sentence_idx\"])\n",
    "    H_LM = rademacher(D, rng_demo)\n",
    "    for tok in history:\n",
    "        H_LM = update_LM_sep(H_LM, tok)\n",
    "\n",
    "    decoded: list[str] = []\n",
    "    targets = list(meta.get(\"span_tokens\", []))\n",
    "    Hs = meta[\"Z_en\"]\n",
    "    steps = min(max_steps, len(targets)) if targets else max_steps\n",
    "\n",
    "    for _ in range(steps):\n",
    "        Qs = DD1_ctx(Hs, G_DEC)\n",
    "        Rt = DD2_query_bin(Qs, history, L_lm, pi, alpha=1.0, beta=1.0, ell=ELL)\n",
    "        Rt_tilde = DD3_bindToMem(Rt, G_MEM)\n",
    "        c_star, C_K, _ = DD4_search_topK(Rt_tilde, prototypes, K=64)\n",
    "        Z_hat = DD5_payload(prototypes[c_star])\n",
    "        cand_vocab = gather_candidates(C_K, history)\n",
    "        token_star, scores = vote_two_spaces(Z_hat, H_LM, cand_vocab)\n",
    "        decoded.append(token_star)\n",
    "        history.append(token_star)\n",
    "        if len(history) > ELL:\n",
    "            history = history[-ELL:]\n",
    "        H_LM = update_LM_sep(H_LM, token_star)\n",
    "\n",
    "    return {\n",
    "        \"sentence_idx\": meta[\"sentence_idx\"],\n",
    "        \"span_bounds\": (meta[\"start\"], meta[\"stop\"]),\n",
    "        \"history_seed\": meta.get(\"history_tokens\", []),\n",
    "        \"decoded\": decoded,\n",
    "        \"reference\": targets[: len(decoded)],\n",
    "    }\n",
    "\n",
    "\n",
    "sample_metas = [m for m in span_meta if m.get(\"span_tokens\")] or span_meta[:5]\n",
    "results = [decode_span(meta, max_steps=8) for meta in tqdm(sample_metas[:5])]\n",
    "\n",
    "correct = 0\n",
    "count = 0\n",
    "for res in results:\n",
    "    print(\"Sentence #\", res[\"sentence_idx\"], \"span\", res[\"span_bounds\"])\n",
    "    print(\"  History seed:\", res[\"history_seed\"])\n",
    "    print(\"  Decoded   :\", \" \".join(res[\"decoded\"]))\n",
    "    print(\"  Reference :\", \" \".join(res[\"reference\"]))\n",
    "    matches = sum(p == t for p, t in zip(res[\"decoded\"], res[\"reference\"]))\n",
    "    correct += matches\n",
    "    count += len(res[\"decoded\"])\n",
    "    print(f\"  Match count: {matches}/{len(res['decoded'])}\")\n",
    "    print()\n",
    "\n",
    "if count:\n",
    "    print(f\"Top-1 token accuracy on decoded spans: {correct / count:.3f} (over {count} tokens)\")\n",
    "else:\n",
    "    print(\"Decoded spans are empty; check MEM span generation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tests automatisés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "pytest.main(['tests/test_enc_mem_dec.py', '-q'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

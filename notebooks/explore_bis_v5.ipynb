{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore_bis_v5 — pipeline ENC → MEM → DEC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "SRC = ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "print(f\"Using src path: {SRC}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import defaultdict\n",
    "from typing import List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hdc_project.encoder import m4, pipeline as enc_pipeline\n",
    "from hdc_project.encoder.mem import pipeline as mem_pipeline\n",
    "from hdc_project.decoder import (\n",
    "    DD1_ctx,\n",
    "    DD2_query,\n",
    "    DD2_query_bin,\n",
    "    DD3_bindToMem,\n",
    "    DD4_search_topK,\n",
    "    DD5_payload,\n",
    "    DD6_vote,\n",
    "    DD7_updateLM,\n",
    "    DecodeOneStep,\n",
    "    DX2_run,\n",
    "    DX3_run,\n",
    "    DX4_run,\n",
    "    DX5_run,\n",
    "    DX6_run,\n",
    "    DX7_run,\n",
    ")\n",
    "from hdc_project.decoder.dec import (\n",
    "    hd_assert_pm1,\n",
    "    hd_bind,\n",
    "    hd_sim,\n",
    "    hd_sim_dot,\n",
    "    build_perm_inverse,\n",
    "    permute_pow,\n",
    "    permute_pow_signed,\n",
    "    rademacher,\n",
    "    _as_vocab_from_buckets,\n",
    ")\n",
    "\n",
    "log = logging.getLogger(\"explore_v5\")\n",
    "if not log.handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement du corpus et encodage ENC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCES = 2_000\n",
    "N_OPUS = 5_000\n",
    "\n",
    "try:\n",
    "    ens_raw, frs_raw = enc_pipeline.opus_load_subset(\n",
    "        name=\"opus_books\",\n",
    "        config=\"en-fr\",\n",
    "        split=\"train\",\n",
    "        N=N_OPUS,\n",
    "        seed=2025,\n",
    "    )\n",
    "    log.info(\"OPUS subset loaded: %d pairs\", len(ens_raw))\n",
    "except Exception as exc:\n",
    "    log.warning(\"OPUS download failed (%s); using a local toy corpus\", exc)\n",
    "    ens_raw = [\n",
    "        \"hyperdimensional computing is fun\",\n",
    "        \"vector symbolic architectures are powerful\",\n",
    "        \"encoding words into hyperspace\",\n",
    "        \"memory augmented networks love clean data\",\n",
    "    ]\n",
    "    frs_raw = [\n",
    "        \"le calcul hyperdimensionnel est amusant\",\n",
    "        \"les architectures symboliques vectorielles sont puissantes\",\n",
    "        \"encoder des mots dans l'hyperspace\",\n",
    "        \"les réseaux augmentés de mémoire aiment les données propres\",\n",
    "    ]\n",
    "\n",
    "ens_sample = ens_raw[:MAX_SENTENCES]\n",
    "frs_sample = frs_raw[:MAX_SENTENCES]\n",
    "log.info(\"Using %d sentence pairs\", len(ens_sample))\n",
    "\n",
    "D = 8_192\n",
    "n = 5\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "Lex_en = m4.M4_LexEN_new(seed=1, D=D)\n",
    "Lex_fr = m4.M4_LexEN_new(seed=2, D=D)\n",
    "pi = rng.permutation(D).astype(np.int64)\n",
    "pi_inv = build_perm_inverse(pi)\n",
    "\n",
    "encoded_en = enc_pipeline.encode_corpus_ENC(ens_sample, Lex_en, pi, D, n, seg_seed0=999)\n",
    "encoded_fr = enc_pipeline.encode_corpus_ENC(frs_sample, Lex_fr, pi, D, n, seg_seed0=1999)\n",
    "log.info(\"Encoded %d EN / %d FR sentences\", len(encoded_en), len(encoded_fr))\n",
    "\n",
    "E_list_en = [segment[\"E_seq\"] for segment in encoded_en]\n",
    "H_list_en = [segment[\"H\"] for segment in encoded_en]\n",
    "if H_list_en:\n",
    "    log.info(\"Encoder signature shape: %s\", H_list_en[0].shape)\n",
    "\n",
    "intra_sim, inter_sim = enc_pipeline.intra_inter_ngram_sims(E_list_en, D)\n",
    "inter_seg = enc_pipeline.inter_segment_similarity(H_list_en)\n",
    "log.info(\"ENC stats — intra: %.4f | inter(|.|): %.4f | inter segments: %.4f\", intra_sim, inter_sim, inter_seg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construction des paires MEM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_signature_from_Xseq(X_seq: Sequence[np.ndarray], *, majority: str = \"strict\") -> np.ndarray:\n",
    "    if not X_seq:\n",
    "        raise ValueError(\"X_seq vide\")\n",
    "    acc = np.zeros(X_seq[0].shape[0], dtype=np.int32)\n",
    "    for x in X_seq:\n",
    "        acc += x.astype(np.int32, copy=False)\n",
    "    if majority == \"strict\":\n",
    "        return np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    if majority == \"unbiased\":\n",
    "        rng_local = np.random.default_rng(0)\n",
    "        ties = acc == 0\n",
    "        acc[ties] = rng_local.integers(0, 2, size=int(ties.sum()), dtype=np.int32) * 2 - 1\n",
    "        return np.where(acc >= 0, 1, -1).astype(np.int8, copy=False)\n",
    "    raise ValueError(\"majority must be 'strict' or 'unbiased'\")\n",
    "\n",
    "\n",
    "def span_signatures_from_trace(\n",
    "    X_seq: Sequence[np.ndarray],\n",
    "    *,\n",
    "    win: int,\n",
    "    stride: int,\n",
    "    majority: str,\n",
    ") -> List[Tuple[np.ndarray, int, int]]:\n",
    "    T = len(X_seq)\n",
    "    if T == 0:\n",
    "        return []\n",
    "    spans: List[Tuple[np.ndarray, int, int]] = []\n",
    "    if T <= win:\n",
    "        spans.append((content_signature_from_Xseq(X_seq, majority=majority), 0, T))\n",
    "        return spans\n",
    "    for start in range(0, T - win + 1, max(1, stride)):\n",
    "        stop = start + win\n",
    "        spans.append((content_signature_from_Xseq(X_seq[start:stop], majority=majority), start, stop))\n",
    "    return spans\n",
    "\n",
    "\n",
    "def build_mem_pairs_with_meta(\n",
    "    encoded_en: Sequence[dict],\n",
    "    encoded_fr: Sequence[dict],\n",
    "    tokens_fr: Sequence[Sequence[str]],\n",
    "    *,\n",
    "    win: int = 8,\n",
    "    stride: int = 4,\n",
    "    majority: str = \"strict\",\n",
    "    max_pairs: int | None = None,\n",
    ") -> Tuple[List[Tuple[np.ndarray, np.ndarray]], List[dict]]:\n",
    "    pairs: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "    meta: List[dict] = []\n",
    "    N = min(len(encoded_en), len(encoded_fr))\n",
    "    for idx in tqdm(range(N), desc=\"MEM span extraction\", leave=False):\n",
    "        spans_en = span_signatures_from_trace(encoded_en[idx][\"X_seq\"], win=win, stride=stride, majority=majority)\n",
    "        spans_fr = span_signatures_from_trace(encoded_fr[idx][\"X_seq\"], win=win, stride=stride, majority=majority)\n",
    "        tok_fr = list(tokens_fr[idx]) if idx < len(tokens_fr) else []\n",
    "        L = min(len(spans_en), len(spans_fr))\n",
    "        for (ze, start_en, stop_en), (zf, start_fr, stop_fr) in zip(spans_en[:L], spans_fr[:L]):\n",
    "            pairs.append((ze, zf))\n",
    "            span_tokens = tok_fr[start_fr:stop_fr] if tok_fr else []\n",
    "            history = tok_fr[max(0, start_fr - stride):start_fr] if tok_fr else []\n",
    "            meta.append(\n",
    "                {\n",
    "                    \"sentence_idx\": idx,\n",
    "                    \"start\": start_fr,\n",
    "                    \"stop\": stop_fr,\n",
    "                    \"history_tokens\": history,\n",
    "                    \"span_tokens\": span_tokens,\n",
    "                    \"Z_en\": ze,\n",
    "                    \"Z_fr\": zf,\n",
    "                }\n",
    "            )\n",
    "            if max_pairs is not None and len(pairs) >= max_pairs:\n",
    "                return pairs, meta\n",
    "    return pairs, meta\n",
    "\n",
    "\n",
    "tokens_fr = [enc_pipeline.sentence_to_tokens_EN(sent, vocab=set()) for sent in frs_sample]\n",
    "pairs_mem, span_meta = build_mem_pairs_with_meta(\n",
    "    encoded_en,\n",
    "    encoded_fr,\n",
    "    tokens_fr,\n",
    "    win=8,\n",
    "    stride=4,\n",
    "    majority=\"strict\",\n",
    "    max_pairs=50_000,\n",
    ")\n",
    "log.info(\"Prepared %d MEM pairs\", len(pairs_mem))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entraînement MEM et diagnostic rapide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEM_K = 16\n",
    "MEM_BUCKETS = 128\n",
    "cfg = mem_pipeline.MemConfig(D=D, B=MEM_BUCKETS, k=MEM_K, seed_lsh=10, seed_gmem=11)\n",
    "comp = mem_pipeline.make_mem_pipeline(cfg)\n",
    "mem_pipeline.train_one_pass_MEM(comp, pairs_mem)\n",
    "log.info(\"MEM training completed (B=%d)\", comp.mem.B)\n",
    "\n",
    "probe_count = min(200, len(pairs_mem))\n",
    "sim_values = []\n",
    "for Z_en_vec, Z_fr_vec in tqdm(pairs_mem[:probe_count], desc=\"MEM probe\"):\n",
    "    bucket_idx, _ = mem_pipeline.infer_map_top1(comp, Z_en_vec)\n",
    "    prototype = comp.mem.H[bucket_idx].astype(np.int32, copy=False)\n",
    "    sim = float(np.dot(prototype, Z_fr_vec.astype(np.int32, copy=False)) / D)\n",
    "    sim_values.append(sim)\n",
    "\n",
    "if sim_values:\n",
    "    log.info(\n",
    "        \"Probe similarities — mean: %.4f | median: %.4f\",\n",
    "        float(np.mean(sim_values)),\n",
    "        float(np.median(sim_values)),\n",
    "    )\n",
    "    nb = comp.mem.n\n",
    "    log.info(\n",
    "        \"Bucket population stats — mean: %.1f | p90: %d | p99: %d\",\n",
    "        float(nb.mean()),\n",
    "        int(np.quantile(nb, 0.90)),\n",
    "        int(np.quantile(nb, 0.99)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dictionnaire bucket → vocabulaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_vocab: dict[int, set[str]] = defaultdict(set)\n",
    "for meta in tqdm(span_meta, desc=\"Bucket vocab build\", leave=False):\n",
    "    bucket_idx, _ = mem_pipeline.infer_map_top1(comp, meta[\"Z_en\"])\n",
    "    meta[\"bucket_idx\"] = int(bucket_idx)\n",
    "    for tok in meta[\"span_tokens\"]:\n",
    "        bucket_vocab[int(bucket_idx)].add(tok)\n",
    "\n",
    "bucket2vocab = {bucket: sorted(tokens) for bucket, tokens in bucket_vocab.items()}\n",
    "all_vocab = sorted({tok for tokens in bucket2vocab.values() for tok in tokens})\n",
    "log.info(\"Bucket vocab built for %d buckets (global vocab size=%d)\", len(bucket2vocab), len(all_vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5bis. Diagnostics théorie ↔ implémentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Running DEC diagnostic suite (subsampled)...\")\n",
    "\n",
    "norms = DX2_run(D=D, trials=50, ells=(2, 4, 8), ratios=(1.0,), seed=2025)\n",
    "log.info(\"DX2 ok — example median norm: %.3f\", np.median([v[1] for v in norms.values()]))\n",
    "\n",
    "rel_err, pval = DX3_run(D=D, C=256, T=64, seed=2025, rel_tol=0.02, pmin=0.05)\n",
    "log.info(\"DX3 ok — mean relative error=%.4f | p=%.3f\", rel_err, pval)\n",
    "\n",
    "recalls = DX4_run(D=D, B=5_000, trials=40, Ks=(100, 500), seed=0)\n",
    "log.info(\"DX4 ok — recall@500=%.3f\", recalls[500])\n",
    "\n",
    "accuracies = DX5_run(D=D, trials=40, ms=(4, 8, 16), seed=0)\n",
    "log.info(\"DX5 ok — accuracy m=8: %.3f\", accuracies[8])\n",
    "\n",
    "results_dx6 = DX6_run(D=D, trials=120, lam_grid=(0.0, 0.5, 1.0), rng_seed=7031)\n",
    "log.info(\"DX6 ok — lambda grid summary: %s\", {lam: (vals['top1'], vals['ppl']) for lam, vals in results_dx6.items()})\n",
    "\n",
    "dx7_results, ell_star = DX7_run(ell_grid=(2, 4, 8), D=D, seed_pi=10_456, rng_seed=9_117)\n",
    "log.info(\"DX7 ok — ell*=%d, top1=%.3f\", ell_star, dx7_results[ell_star][\"top1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Démonstration DEC (un pas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not span_meta:\n",
    "    raise RuntimeError(\"Aucune paire MEM disponible pour la démonstration DEC.\")\n",
    "\n",
    "demo = next((m for m in span_meta if m[\"span_tokens\"]), span_meta[0])\n",
    "G_DEC = rademacher(D, np.random.default_rng(2025))\n",
    "G_MEM = comp.Gmem\n",
    "L_fr = Lex_fr.get\n",
    "\n",
    "history = list(demo[\"history_tokens\"][-4:])\n",
    "Hs = demo[\"Z_en\"]\n",
    "H_LM = rademacher(D, np.random.default_rng(4242))\n",
    "for tok in history:\n",
    "    H_LM = DD7_updateLM(H_LM, tok, L_fr, pi)\n",
    "\n",
    "prototypes = comp.mem.H.astype(np.int8, copy=False)\n",
    "\n",
    "Qs = DD1_ctx(Hs, G_DEC)\n",
    "Rt = DD2_query_bin(Qs, history, L_fr, pi, alpha=1.0, beta=1.0, ell=max(1, len(history)))\n",
    "Rt_tilde = DD3_bindToMem(Rt, G_MEM)\n",
    "c_star, C_K, scores_CK = DD4_search_topK(Rt_tilde, prototypes, K=32)\n",
    "Z_hat = DD5_payload(prototypes[c_star])\n",
    "\n",
    "cand_vocab = _as_vocab_from_buckets(\n",
    "    C_K=C_K,\n",
    "    bucket2vocab=bucket2vocab,\n",
    "    history_fr=history,\n",
    "    global_fallback_vocab=all_vocab[:256] if all_vocab else None,\n",
    "    min_size=1,\n",
    ")\n",
    "\n",
    "token_star, scores_cand, _ = DD6_vote(\n",
    "    Z_hat=Z_hat,\n",
    "    H_LM=H_LM,\n",
    "    L_mem=L_fr,\n",
    "    L_lm=L_fr,\n",
    "    cand_vocab=cand_vocab,\n",
    "    lam=0.5,\n",
    "    normalize=\"sqrtD\",\n",
    "    return_probs=False,\n",
    ")\n",
    "\n",
    "print(\"History tokens:\", history)\n",
    "print(\"Span tokens (ground truth):\", demo[\"span_tokens\"][:10])\n",
    "print(\"Decoded token*:\", token_star)\n",
    "print(\"Top-5 candidats (score brut):\")\n",
    "order = np.argsort(scores_cand)[::-1][:5]\n",
    "for rank, idx in enumerate(order, start=1):\n",
    "    tok = cand_vocab[idx] if idx < len(cand_vocab) else \"<unk>\"\n",
    "    print(f\"  {rank}. {tok:20s} -> {scores_cand[idx]:8.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tests automatisés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "pytest.main(['tests/test_enc_mem_dec.py', '-q'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}